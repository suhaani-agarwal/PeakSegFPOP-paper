% -*- compile-command: "make jss-slides.pdf" -*-
\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\z}{$z = 2, 4, 3, 5, 1$} 

\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\definecolor{PDPA}{HTML}{66C2A5}
\definecolor{CDPA}{HTML}{FC8D62}
\definecolor{GPDPA}{HTML}{4D4D4D}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{
A Generalized Functional Pruning Optimal Partitioning Algorithm
for Peak Detection in Large Genomic Data
}

\author{
  Toby Dylan Hocking\\
  toby.hocking@nau.edu\\
  joint work with Guillem Rigaill}

%\date{13 Nov 2017}

\maketitle

\section{Problem: optimizing ChIP-seq peak detection}

\begin{frame}
  \frametitle{Chromatin immunoprecipitation sequencing (ChIP-seq)}
  Analysis of DNA-protein interactions.

  \includegraphics[width=\textwidth]{Chromatin_immunoprecipitation_sequencing_wide.png}

  Source: ``ChIP-sequencing,'' Wikipedia.
\end{frame}

\begin{frame}
  \frametitle{Problem: find peaks in each of several samples}
  \includegraphics[width=\textwidth]{screenshot-ucsc-edited}

  \begin{itemize}
  \item Grey profiles are noisy aligned read count signals -- \\peaks
    are genomic locations with protein binding sites.
  \item Black bars are peaks called by MACS2 (Zhang et al, 2008) -- many
    false positives! (black bars where there is only noise)
  \item From a machine learning perspective, this is binary
    classification (positive=peaks, negative=noise).
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Previous work in genomic peak detection}
%   \begin{itemize}
%   \item Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
%   \item SICER, Zang et al, 2009.
%   \item HOMER, Heinz et al, 2010.
%   \item CCAT, Xu et al, 2010.
%   \item RSEG, Song et al, 2011.
%   \item Triform, Kornacker et al, 2012.
%   \item Histone modifications in cancer (HMCan), Ashoor et al, 2013.
%   \item PeakSeg, Hocking, Rigaill, Bourque, ICML 2015.
%   %\item PeakSegJoint Hocking and Bourque, arXiv:1506.01286.
%   \item ... dozens of others.
%   \end{itemize}
%   Two big questions: how to choose the best...
%   \begin{itemize}
%   \item ...algorithm? (testing)
%   \item \alert<1>{...parameters? (training)}
%   \end{itemize}
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{How to choose parameters of unsupervised peak
%     detectors?}
% \scriptsize
% 19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
% \begin{verbatim}
%   [-g GSIZE]
%   [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
%   [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
%   [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
%   [--down-sample] [--seed SEED] [--nolambda]
%   [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
%   [--shift-control] [--half-ext] [--broad]
%   [--broad-cutoff BROADCUTOFF] [--call-summits]
% \end{verbatim}
% 10 parameters for Histone modifications in cancer (HMCan),
% Ashoor et al, 2013.
% \begin{verbatim}
% minLength 145
% medLength 150
% maxLength 155
% smallBinLength 50
% largeBinLength 100000
% pvalueThreshold 0.01
% mergeDistance 200
% iterationThreshold 5
% finalThreshold 0
% maxIter 20
% \end{verbatim}
% \end{frame}
 
\begin{frame}
  \frametitle{Which macs parameter is best for these data?}
  \includegraphics[width=1\textwidth]{figure-macs-problem.png}
\end{frame}

\begin{frame}
  \frametitle{Compute likelihood/loss of piecewise constant model}
  \includegraphics[width=1\textwidth]{figure-macs-problem-7-5.png}
  % $\PoissonLoss(\mathbf z, \mathbf m) = \sum_{i=1}^n m_i - z_i \log(m_i)$
  % for count data $\mathbf z\in\ZZ_+^n$ 
  % and segment mean model $\mathbf m\in\RR^n$.
\end{frame}

\begin{frame}
  \frametitle{Idea: choose the parameter with a lower loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-15.png}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}

  Simple model with only one parameter (number of peaks).

  %Choose the number of peaks via standard penalties (AIC, BIC,
  %  ...)\\or learned penalties based on visual labels (more on this later).
\end{frame}

% \begin{frame}
%   \frametitle{Maximum likelihood Poisson segmentation models}
%   \includegraphics[width=1\textwidth]{figure-Segmentor-PeakSeg}

%   \begin{itemize}
%   \item Previous work: unconstrained maximum likelihood mean\\
%     for $s$ segments ($s-1$ changes), Cleynen et al 2014.
%   \item Hocking et al, ICML 2015: PeakSeg constraint enforces up, down, up,
%     down changes (and not up, up, down). 
%   \item Odd-numbered segments are background noise,\\
%     even-numbered segments are peaks.
%   \item Constrained Dynamic Programming Algorithm, $O(n^2)$ time for $n$ data points.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{But quadratic time is not fast enough for genomic data!}
%   \includegraphics[width=\textwidth]{figure-PDPA-timings-dp}
%   \begin{itemize}
%   \item Genomic data is large, $n \geq 10^6$.
%   \item Split into subsets? What if we split a peak in half?
%   \item Need linear time algorithm for analyzing whole data set.
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{Statistical model is a piecewise constant Poisson mean}
  H {\it et al.}, {\it ICML} 2015. 

  \input{figure-PeakSeg}
  \vskip -0.8cm    
  \begin{itemize}
  \item We have $n$ count data $z_1, \dots, z_n\in\ZZ_+$.
  \item Fix the number of segments $S\in\{1, 2, \dots, n\}$.
  \item Optimization variables: $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment means $u_1,\dots,u_S\in\RR_+$.
  \item Let $0=t_0<t_1 < \cdots < t_{S-1}<t_S=n$ be the segment
    limits.
  \item Statistical model: for every segment $s\in\{1,\dots,S\}$,
    $z_i \stackrel{\text{iid}}{\sim} \text{Poisson}(u_s)$ for every data
    point $i\in(t_{s-1},t_s]$.
  \item PeakSeg up-down constraint: $u_1\leq u_2 \geq u_3 \leq u_4 \geq \cdots$
  \item Want to find means $u_s$ which maximize the Poisson likelihood:
    $P(Z = z_i|u_s) = u_s^{z_i} e^{-u_s} / (z_i!)$.
  \item Equivalent to finding means $u_s$ which minimize the Poisson
    loss: $\ell(u_s, z_i) = u_s - z_i\log u_s$.
  % \item Comparison to Hidden Markov Model:
  %   \begin{description}
  %   \item[Likelihood] Same emission terms, no transition terms.
  %   \item[Constraint] Number of changes rather than values.
  %   \end{description}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Maximum likelihood changepoint detection with up-down constraints on adjacent segment means (PeakSeg)}
    
\only<1>{\input{figure-PeakSeg}}      
\only<2>{\input{figure-PeakSeg-unconstrained}}
\only<3>{\input{figure-PeakSeg-constrained}}
\vskip -1.4cm
\begin{align*}
    \minimize_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=N
  }} &\ \ 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
\\
      \text{subject to \hskip 0.75cm} &\ \ \alert<3>{u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\},}
  \nonumber\\
  &\ \ \alert<3>{u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\}.}
  \nonumber 
\end{align*}
\vskip -0.4cm
\begin{itemize}  
\item Simple: 1 parameter = number of segments $S\in\{1,3,\dots\}$.
\item Hard optimization problem, naively $O(N^S)$ time.
\item \alert<2>{Previous unconstrained model: not always up-down changes.}
\item \alert<3>{Interpretable: $P=(S-1)/2$ peaks (segments 2, 4, ...).}
\item H {\it et al.}, {\it ICML} 2015: $O(SN^2)$ time approximate
  algorithm. 
\item H {\it et al.}, arXiv 2017: $O(SN\log N)$ time optimal algorithm.
\end{itemize}
\end{frame} 

\section{New functional pruning algorithm}


\begin{frame}
  \frametitle{Previous work on the Segment Neighborhood problem}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Dynamic Prog. Algo. & Pruned DPA \\
     & exact $O(SN^2)$ & exact $O(SN\log N)$\\
    %R pkgs: & \alert<1>{changepoint} & \alert<2>{cghseg, Segmentor}\\
    & Auger and L. 1989 & Rigaill 2010\\
    \hline
    up-down constrained & Constrained DPA & Generalized PDPA \\
     & inexact $O(SN^2)$ & exact $O(SN\log N)$\\
    %R pkgs: & \alert<3>{PeakSegDP} & \alert<4>{PeakSegOptimal}\\
    & H et al 2015 & H et al 2017\\
    \hline
  \end{tabular}
  \begin{itemize}
  % \item \alert<1>{Auger and Lawrence 1989, Jackson et al 2005}.
  % \item \alert<2>{Rigaill 2010, Johnson 2013, Cleynen et al 2014}.
  % \item \alert<3>{Hocking, Rigaill, Bourque 2015}.
  % \item \alert<4>{\textbf{Contribution:} Generalized Pruned Dynamic
  %       Programming Algorithm (GPDPA) that \textbf{exactly} computes the
  %     \textbf{constrained} model for $n$ data points and up to $S$ segments in
  %      $O(Sn\log n)$ time}.
  \item All algorithms solve the \textbf{Segment Neighborhood}
    ``constrained'' problem: most likely mean $m_i$ for data $z_i$,
    subject to the constraint of $S$ segments ($S-1$ changes).
  \end{itemize} 
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{N}
% \\
%    0=t_0<t_1<\cdots<t_{S-1}<t_S=N
  }} &\ \ 
    \sum_{i=1}^N \ell( m_i,  z_i) 
\\
      \text{subject to} &\ \ {\alert{\sum_{i=1}^{N-1} I[m_i\neq m_{i+1}]}=S-1,}
  \nonumber\\
  &\ \ \text{...up-down constraints on $m$.}
  \nonumber 
\end{align*}
\end{frame}

\begin{frame}
  \frametitle{Previous work on the Optimal Partitioning problem}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Opt. Part. Algo. & FPOP \\
     & exact $O(N^2)$ & exact $O(N\log N)$\\
    %R pkgs: & \alert<1>{changepoint} & \alert<2>{cghseg, Segmentor}\\
    & Jackson et al 2005 & Maidstone et al 2016\\
    \hline
    up-down constrained &  & Generalized FPOP \\
     &  & exact $O(N\log N)$\\
    %R pkgs: & \alert<3>{PeakSegDP} & \alert<4>{PeakSegOptimal}\\
    &  & \textbf{This work}\\
    \hline
  \end{tabular}
  \begin{itemize}
  % \item \alert<1>{Auger and Lawrence 1989, Jackson et al 2005}.
  % \item \alert<2>{Rigaill 2010, Johnson 2013, Cleynen et al 2014}.
  % \item \alert<3>{Hocking, Rigaill, Bourque 2015}.
  % \item \alert<4>{\textbf{Contribution:} Generalized Pruned Dynamic
  %       Programming Algorithm (GPDPA) that \textbf{exactly} computes the
  %     \textbf{constrained} model for $n$ data points and up to $S$ segments in
  %      $O(Sn\log n)$ time}.
  \item All algorithms solve the \textbf{Optimal Partitioning}
    ``penalized'' problem: most likely mean $m_i$ for data $z_i$,
    penalized by a non-negative $\lambda\in\RR_+$ for each change:
  \end{itemize}
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{N}
% \\
%    0=t_0<t_1<\cdots<t_{S-1}<t_S=N
  }} &\ \ 
    \sum_{i=1}^N \ell( m_i,  z_i)  + \lambda\alert{\sum_{i=1}^{N-1}I[m_{i}\neq m_{i+1}]}
\\
      \text{subject to} &\ \ \text{...up-down constraints on $m$.}
  \nonumber 
\end{align*}
\end{frame}


\begin{frame}
  \frametitle{Dynamic programming and functional pruning}
  \textbf{Classical dynamic programming for optimal partitioning}
  (Jackson et al 2005) computes the vector of optimal loss values up
  to $N$ data points, $O(N^2)$ time because each DP iteration needs to
  consider all $O(N)$ possible changepoints and cost values.
$$
\begin{array}{ccccc}
  \mathcal C_{1} & \mathcal C_2 & \cdots & \mathcal C_{N-1} &  \mathcal C_N
\end{array}
$$
\textbf{Functional pruning optimal partitioning} (Maidstone 2016)
computes a vector of loss \textbf{functions}, $O(N\log N)$ because
each DP iteration only considers $O(\log N)$ candidate changepoints
\\(the others --- which will never be optimal --- are pruned).
$$
\begin{array}{ccccc}
   C_{1}(m_1) & C_2(m_2) & \cdots & C_{N-1}(m_{N-1}) &   C_N(m_{N})
\end{array}
$$
\textbf{Contribution of this work}: a new algorithm that applies the
functional pruning technique to the up-down constrained model.
\end{frame}

\begin{frame}
  \frametitle{Constrained optimal partitioning problem}
  \begin{align*}
  \label{eq:penalized_peakseg}
  \minimize_{
    \substack{
    \mathbf m\in\RR^N,\ \mathbf s\in\{0, 1\}^N\\
\mathbf c\in\{-1, 0,1\}^{N-1}\\
}
    } &\ \ 
  \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(c_i \neq 0) \\
  \text{subject to\ \ } &\ \text{no change: }c_t = 0 \Rightarrow m_t = m_{t+1}\text{ and }s_t=s_{t+1}
  \nonumber\\
&\ \text{go up: }c_t = 1 \Rightarrow m_t \leq m_{t+1}\text{ and }(s_t,s_{t+1})=(0,1),
  \nonumber\\
&\ \text{go down: } c_t = -1 \Rightarrow m_t \geq m_{t+1}\text{ and }(s_t,s_{t+1})=(1,0).
\nonumber
\end{align*}
\begin{minipage}{0.5\linewidth}
    \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2cm,
                    thick,main node/.style={circle,draw}]

  \node[main node] (1) {$s=1$};
  \node[main node] (2) [below of=1] {$s=0$};

  \path[every node/.style={font=\sffamily\small}]
    (2) edge [bend left] node {$c=1, \leq, \lambda$} (1)
    (1) edge [bend left] node {$c=-1, \geq, \lambda$} (2);
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.45\linewidth}
  Nodes=states $s$,\\
  Edges=changes $c$ (constraint, penalty).
\end{minipage}
\end{frame}

\begin{frame}
  \frametitle{Dynamic programming update rules}
Recursively compute two vectors of real-valued cost functions:
$$
\begin{array}{cccl}
  \overline C_{1}(m_1) & \cdots & \overline C_N(m_{N})& \text{ optimal cost in peak state $s=1$}\\
  \underline C_{1}(m_1) & \cdots & \underline C_N(m_{N})& \text{ optimal cost in background state $s=0$}\\
\end{array}
$$
\begin{minipage}{0.65\linewidth}
  $$\overline C_{t+1}(\mu) = \ell(\mu, z_i) + \min\{
  \overline C_t(\mu),\, 
  \underline C_t^\leq(\mu)+\lambda
\},$$
  $$\underline C_{t+1}(\mu) = \ell(\mu, z_i) + \min\{
  \underline C_t(\mu),\, 
  \overline C_t^\geq(\mu)+\lambda
\},$$
where $f^\leq(\mu) = \min_{x\leq\mu} f(x)$,\\
$f^\geq(\mu) = \min_{x\geq\mu} f(x)$.
\end{minipage}
\begin{minipage}{0.25\linewidth}
  \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=3cm,
  thick,main node/.style={circle,draw}]
  \node[main node] (peak_t) {$\overline C_t$};
  \node[main node] (bkg_t) [below of=peak_t] {$\underline C_t$};
  \node[main node] (peak_t1) [right of=peak_t] {$\overline C_{t+1}$};
  \node[main node] (bkg_t1) [right of=bkg_t] {$\underline C_{t+1}$};
  \path[every node/.style={font=\small}]
    (peak_t) edge [dotted] node {$\overline C_{t}$} (peak_t1)
    (peak_t) edge [black, bend right] node [right] {$\overline C_{t}^{\geq}+\lambda$} (bkg_t1)
    (bkg_t) edge [dotted] node[midway, below] {$\underline C_{t}$} (bkg_t1)
    (bkg_t) edge [black, bend left] node[right] {$\underline C_{ t}^{\leq}+\lambda$} (peak_t1)
;
\end{tikzpicture}
\end{minipage}
\end{frame}

\section{Results on ChIP-seq benchmark}

\begin{frame}
  \frametitle{Functional pruning algorithms faster for larger data sets}

Benchmark data set: \url{http://github.com/tdhock/feature-learning-benchmark}

\begin{itemize}
\item 4951 data sets ranging from $N=10^3$ to $N=10^7$.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Labels used to determine optimal number of peaks}
  \includegraphics[width=\textwidth]{jss-figure-label-error-data-labels}
\end{frame}

\begin{frame}
  \frametitle{Labels used to determine optimal number of peaks}
  \includegraphics[width=\textwidth]{jss-figure-label-error-data-labels}
\end{frame}

\begin{frame}
  \frametitle{Labels used to determine optimal number of peaks}
  \includegraphics[width=\textwidth]{jss-figure-label-error-too-few}
\end{frame}

\begin{frame}
  \frametitle{Labels used to determine optimal number of peaks}
  \includegraphics[width=\textwidth]{jss-figure-label-error-too-many}
\end{frame}

\begin{frame}
  \frametitle{Labels used to determine optimal number of peaks}
  \includegraphics[width=\textwidth]{jss-figure-label-error}
\end{frame}

\begin{frame}
  \frametitle{Number of peaks/segments increases with $N$ }
  \input{jss-figure-data-peaks-slide}
  \vskip -0.4cm
  \begin{itemize} 
  \item %H {\it et al.}, arXiv 2017: $O(SN\log N)$ time/space
    Optimal algorithm requires $O(S)$ dynamic programming iterations,
    each is $O(N\log N)$ time/space.
  \item If we want $S=O(\sqrt{N})$ segments then the algorithm is
    $O(N \sqrt N \log N)$ time/space -- too much for large data.
  \item For example $N=10^7$ data. Each $O(N\log N)$ DP iteration
    takes 1 hour, 80 GB. Overall if we want $S=O(\sqrt N) = 2828$
    segments we need means 220 TB of storage space and 17 weeks of
    computation time!
  \end{itemize}

  
\end{frame}


\begin{frame}
  \frametitle{New labeling method for peak detection in ChIP-seq data}

  H {\it et al.}, {\it Bioinformatics} 2017: choose peak model/parameters
  which minimize the number of incorrectly predicted labels.

  \includegraphics[width=\textwidth]{figure-PeakError.pdf}
  \begin{itemize}
  \item \textbf{peakStart}: exactly one peak start (0=FN, more=FP).
  \item \textbf{peakEnd}: exactly one peak end (0=FN, more=FP).
  \item \textbf{noPeaks}: no overlapping peaks (otherwise FP).
  \item \textbf{peaks}: at least one overlapping peak (otherwise FN).
  \item R package PeakError.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Contribution: new fast and exact constrained algorithm}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Dynamic Programming & Pruned DP \\
     & exact $O(S n^2)$ & exact $O(Sn\log n)$\\
    R pkgs: & changepoint & cghseg, Segmentor\\
    \hline
    up-down constrained & constrained DP & \textbf{GPDPA} \\
     & inexact $O(Sn^2)$ & exact $O(Sn\log n)$\\
    R pkgs: & PeakSegDP & \textbf{PeakSegOptimal}\\
    \hline 
  \end{tabular}
  \begin{itemize}
  \item New GPDPA that computes optimal changepoints 
    using the Poisson loss and the up-down constraint.
  \item State-of-the-art peak detection accuracy in ChIP-seq data.
  \item Empirically $O(S n \log n)$ time and memory for $n$ data
    points and $S$ segments.
  \item C++ code in PeakSegOptimal R package, 
    \url{https://cran.r-project.org/package=PeakSegOptimal}
  \end{itemize}
  Future work: other constraints and loss functions.
\end{frame}

\begin{frame}
  \frametitle{Cost function computation}
  Computation of each update rule is linear in number of
  intervals/pieces/candidates -- empirically $O(\log n)$.
  \begin{minipage}[t]{\linewidth}
  \hskip -1cm
  \input{figure-2-min-envelope} 
  \end{minipage}
\end{frame}

\end{document}
