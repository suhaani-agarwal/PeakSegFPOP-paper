% -*- compile-command: "make jss-paper.pdf" -*-
\documentclass[article]{jss}

\newcommand{\R}{\proglang{R}}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{Ckt}{HTML}{E41A1C}
\definecolor{Min}{HTML}{4D4D4D}%grey30
%{B3B3B3}%grey70
\definecolor{MinMore}{HTML}{377EB8}
\definecolor{Data}{HTML}{984EA3}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2016} 
%\usepackage{fullpage}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}


%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking\\Northern Arizona University
   \And Guillem Rigaill\\INRA
   \And Paul Fearnhead\\Lancaster University
   \And Guillaume Bourque\\McGill University}
\Plainauthor{Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque} 

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Generalized Functional Pruning Optimal Partitioning (GFPOP)
  for Constrained Changepoint Detection in Large Genomic Data}

\Plaintitle{Generalized Functional Pruning Optimal Partitioning (GFPOP) for
  Constrained Changepoint Detection in Large Genomic Data}

%\Shorttitle{A Short Demo Article in \proglang{R}}
\Shorttitle{Constrained Optimal Changepoint Detection in Large Data}

%% - \Abstract{} almost as usual
\Abstract{We describe a new algorithm and \proglang{R} package for
  peak detection in large genomic data sets using constrained
  changepoint algorithms. These detect changes from background to peak
  regions by imposing the constraint that the mean should alternately
  increase then decrease. An existing algorithm for this problem
  exists, and gives state-of-the-art accuracy results, but it is
  computationally expensive when the number of changes is large. We
  propose the GFPOP algorithm that jointly estimates the number of
  peaks and their locations by minimizing a cost function which
  consists of a data fitting term and a penalty for each
  changepoint. Empirically this algorithm has a cost that is
  $O(N \log(N))$ for analysing data of length $N$. We also propose a
  sequential search algorithm that finds the best solution with $K$
  segments in $O(\log(K) N\log(N))$ time, which is much faster than
  the previous $O(K N\log(N))$ algorithm. We show that our disk-based
  implementation in the \pkg{PeakSegPipeline} \R\ package can be used
  quickly compute constrained optimal models with many changepoints in
  genomic data sets of up to ten million observations.}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, \proglang{R}} 

\Plainkeywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, R} 

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  Northern Arizona University\\
  School of Informatics, Computing, and Cyber Systems\\
  Flagstaff, AZ, USA\\
  E-mail: \email{Toby.Hocking@nau.edu}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
\\ \\
  Guillem Rigaill\\
  Laboratoire de Math\'ematiques at Mod\'elisation d'Evry (LaMME), Universit\'e
d'Evry Val d'Essonne, UMR CNRS 8071, ENSIIE, USC INRA,
Institute of Plant Sciences Paris-Saclay (IPS2), UMR 9213/UMR1403, CNRS, INRA,
Universit\'e Paris-Sud, Universit\'e d'Evry, Universit\'e Paris-Diderot, Sorbonne
Paris-Cit\'e,\\
  Evry, France\\
  E-mail: \email{Guillem.Rigaill@inra.fr}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
  \\ \\
  Paul Fearnhead\\
  Lancaster University\\
  Lancaster, UK\\
  E-mail: \email{p.fearnhead@lancaster.ac.uk}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
  \\ \\
  Guillaume Bourque\\
  McGill University\\
  Montr\'eal, Qu\'ebec, Canada\\
  E-mail: \email{guil.bourque@mcgill.ca}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}

\newpage

\section[Introduction]{Introduction} \label{sec:intro}

\subsection{Peak detection via changepoint methods}

There are many applications, particularly within genomics, that
involve detecting regions that deviate from a usual/background
behaviour, and where qualitatively these deviations lead to an
increased mean of some measured signal. For example, ChIP-seq data
measure transcription factor binding or histone modification
\citep{chip-seq}; ATAC-seq data measure open chromatin \citep{ATACseq}. In these data
we have counts of aligned reads at different positions along a
chromosome, and we would like to detect regions for which the count data are larger than the usual background level.

One approach to detecting these regions is through algorithms that detect changes in the mean of the data. This paper
builds on recent work of \citet{Hocking-constrained-changepoint-detection} and presents a new changepoint algorithm, and its implementation in
\R. This algorithm is based on modeling count data using a Poisson distribution, and using the knowledge that we have
background regions with small values and peak regions with large values. This imposes constraints
on the directions of changes, with the mean of the data alternately increasing then decreasing in value. A particular
challenge with genomic data is that for an algorithm to be widely used, it must scale well  to large data in terms of 
both time and memory costs.

There are other algorithms for tackling this type of problem, for
example based on hidden Markov models \citep{chipseq-hmm}.  One drawback of such
methods is that they assume the background/peak means do not change across
large genomic regions, whereas such long-range changes are observed in
many real data sets. For a detailed comparison of other algorithms with
changepoint approaches we refer the reader to
\citep{HOCKING2016-chipseq}; we focus the remainder of the paper on
optimal changepoint models.

\subsection{Optimal changepoint models with no constraints between adjacent segment means}

Denote the data by $z_1,\dots,z_N$. We assume the data is ordered: for genomic applications the ordering will 
be due to position along a chromosome, for time-series data the ordering is commonly by time. The aim of 
changepoint analysis is to partition the data in to $K$ segments that each contain consecutive data points, 
such that features of the data are common within a segment but differ between segments. The feature of the data 
that changes will depend on the application, but could be, for example, the mean of the data, the variance, or 
the distribution. Detecting changes of different features requires different statistical algorithms.

Throughout we will let $K$ be the number of segments, with the changepoints being $0=t_0<t_1<\cdots<t_{K-1}<t_K=N$. 
This means that the $k$th segment will contain data points $z_{t_{k-1}+1},\ldots,z_{t_k}$. We denote the 
segment-specific parameter for the segment by $m_k$. For the problem of detecting changes in ChIP-seq count data,
the simplest statistical model uses Poisson random variables with segment-specific mean  
parameters for that segment. 
%it is natural to model each data point as a realisation of a Poisson random variable, with the segment-specific parameter being the mean for that segment. 
Change detection is then an attempt to detect the points along the 
chromosome where the mean of the data changes. 

The algorithm we present is based on detecting changes via minimizing
a measure of fit to the data, with this measure of fit being the
negative log-likelihood under our Poisson model. This corresponds to
using the loss function $\ell(m,z)=m-z\log m$ for fitting a non-negative count data point
$z\in\ZZ_+$ with a mean parameter $m\in\RR_+$. If we know the number of
segments $K$ we can estimate the location of the segments by solving the
following minimization problem,
 \begin{align}
    \minimize_{\substack{
  \mathbf m\in\RR^{K}
\\
   0=t_0<t_1<\cdots<t_{K-1}<t_K=N
  }} &\ \ 
    \sum_{k=1}^K\  \sum_{i=t_{k-1}+1}^{t_k} \ell(m_k,z_i).
\label{min:ut}
\end{align}
Optimizing by naively searching over all possible arrangements of changepoints is an expensive $O(N^K)$ time
operation. However, solving (\ref{min:ut}) can be achieved efficiently using dynamic programming. The first such
algorithm was the Segment Neighborhood algorithm, which computes the series of 
optimal segmentations with  1 to $K$ segments in $O(KN^2)$ time
\citep{segment-neighborhood}.  The classical algorithm for solving the
Segment Neighborhood problem is available in \R\ as
\code{changepoint::cpt.mean}. Recent research has led to faster algorithms, based on pruning the search space of 
the Segment Neighborhood algorithm
\citep{pruned-dp-new,johnson}, and these algorithms empirically take $O(K N \log N)$ time. 
%%proposed similar functional
%pruning techniques for solving the Segment Neighborhood problem in
%faster $O(K N\log N)$ time. 
The novelty of these techniques is a functional
representation of the optimal cost, which allows pruning of the $O(N)$
possible changepoints to only $O(\log N)$ candidates (while
maintaining optimality). The original implementation of the PDPA was
available in \R\ as \verb|cghseg:::segmeanCO| for the Normal
homoscedastic model, but \pkg{cghseg} has been removed from CRAN as of
18 December 2017. The PDPA for the Normal homoscedastic model is now
available as \verb|jointseg::Fpsn| on Bioconductor
\citep{jointseg}. \citet{cleynen2013segmentation} described a
generalization of the PDPA for other likelihood/loss functions
(Poisson, negative binomial, Normal heteroscedastic). These are
available in \R\ as \verb|Segmentor3IsBack::Segmentor|.

In practice it is unusual to know how many segments there are present in the data. 
To estimate $K$ it is common to use some information criteria that takes account 
both of the measure of fit to the data and the complexity of the segmentation
model being fitted. The most natural measures of complexity are linear in the 
number of changepoints. Whilst it is possible to estimate $K$ by solving the 
Segment Neighborhood problem for an appropriate set of changes, and calculating 
the value of the information criteria for each value of the number of segments, it 
is faster to jointly estimate both $K$ and the changepoint locations that minimise 
the information criteria. The first algorithm to do so was the Optimal Partioning 
algorithm introduced by \citet{optimal-partitioning}. Optimal partitioning is an  
$O(N^2)$ algorithm, and can be significantly faster than Segment Neighborhood for 
large $K$. 

There has also been substantial research into speeding up the optimal partitioning algorithm, 
using various ideas to prune the search space. In particular the Pruned Exact Linear Time (PELT)
algorithm of \citet{pelt}, which is implemented within \code{changepoint::cpt.mean}, and Functional Pruning
Optimal Partitioning (FPOP) of \citet{Maidstone2016} which is available in \R\ as
\verb|fpop::Fpop| \citep{fpop}. These algorithms have a computational cost of 
$O(N)$ if $K$ increases linearly with $N$. The FPOP algorithm has a computational cost 
that is empirically $O(N \log N)$ in situations where $K$ increases sub-linearly with $N$. 
See Table \ref{tab:unconstrained-algos} for a summary of the different dynamic programming algorithms and implementations.

Whilst solving the optimal partitioning problem is faster than solving (\ref{min:ut}) for a range of $K$,
the drawback is that you only get a single segmentation for a single value $K$ of the number of segments. 
Furthermore the choice of penalty that you impose with the information criteria -- which corresponds the 
improvement in fit to the data needed to add an additional changepoint -- can be hard to tune and have an 
important impact on the accuracy of the estimate of the number of changepoints. One way to ameliorate this 
concern is to find segmentations for a range of penalties, which can be done efficiently \citep{crops}.


\begin{table*}[t!]
  \centering
  \begin{tabular}{r|c|c}
    Problem & No changepoint pruning & Functional pruning \\
    \hline
    Segment  & Dynamic Prog. Algo. (DPA) & Pruned DPA (PDPA) \\
                  Neighborhood & Optimal, $O(KN^2)$ time & Optimal, $O(KN\log N)$ time\\
            $K$ segments& \citet{segment-neighborhood} & \citet{pruned-dp-new}\\
            & \pkg{changepoint} & \pkg{jointseg}\\
    \hline
    Optimal  & Optimal Partitioning Algorithm & FPOP \\
    Partitioning & Optimal, $O(N^2)$ time & Optimal, $O(N\log N)$ time\\
            Penalty $\lambda$& \citet{optimal-partitioning} & \citet{Maidstone2016}  \\
    &  & \pkg{fpop}\\
    \hline
  \end{tabular}
  \caption{Previous work on algorithms for optimal changepoint detection with 
    no constraints between adjacent segment means.}
\label{tab:unconstrained-algos} 
\end{table*}

There are alternative approaches to fitting changepoint models, the most common of which are based on 
specifying a test for a single change and then repeatedly applying this test to identify multiple 
changepoints. Such approaches can be applied more
widely than the dynamic programming based approaches described above, 
and often have strong computational performance with algorithms that are 
$O(N \log N)$ for the Segment Neighborhood problem. In situations where both procedures 
can be used, these methods are often identical if we wish to identify at most one changepoint. 
The advantage that the dynamic programming approaches have is that they jointly detect multiple 
changepoints which can lead to more accurate estimates \cite[see e.g.][]{Maidstone2016}. Several of 
these alternative algorithms are available in \R.   For example, the \pkg{wbs} package implements
the wild binary segmentation method of \citet{wbs}. An efficient
implementation of the classical binary segmentation heuristic is
available as \code{fpop::multiBinSeg}. The
\pkg{stepR} package implements the SMUCE algorithm for multiscale
changepoint inference \citep{stepR}.

\subsection{Models with inequality constraints between adjacent segment means}

The models discussed above are unconstrained in the sense that there
are no constraints between mean parameters $m_k$ on different
segments. However, as described above, constraints can be useful when
data need to be interpreted in terms of pre-defined domain-specific
states. In the ChIP-seq application the changepoint model needs to be
interpreted in terms of peaks (large values which represent protein
binding/modification) and background (small values which represent noise).

In this context, \citet{HOCKING-PeakSeg} introduced a $O(KN^2)$
Constrained Dynamic Programming Algorithm (CDPA) for fitting a model
where up changes are followed by down changes, and vice versa
(Table~\ref{tab:constrained-algos}). These constraints ensure that
odd-numbered segments can be interpreted as background, and
even-numbered segments can be interpreted as peaks. Although the CDPA
provides a sub-optimal solution to the Segment Neighborhood problem in
$O(KN^2)$ time, \citet{HOCKING2016-chipseq} showed that it achieves
state-of-the-art peak detection accuracy in a benchmark of
ChIP-seq data sets. 

Because the quadratic time complexity of the CDPA limits its
application to relatively small data sets,
\citet{Hocking-constrained-changepoint-detection} proposed to generalize
the functional pruning method for changepoint models with
constraints between adjacent segment means. The resulting Generalized
Pruned Dynamic Programming Algorithm (GPDPA) reduces the number of
candidate changepoints from $O(N)$ to $O(\log N)$ while enforcing the constraints and maintaining
optimality. The GPDPA computes the optimal solution to the up-down
constrained Segment Neighborhood problem in $O(KN\log N)$ time.  The
\pkg{PeakSegOptimal} \R\ package provides an in-memory solver for the up-down constrained Segment Neighborhood model
\citep{Hocking-constrained-changepoint-detection}.

\begin{table}
  \centering
  \begin{tabular}{r|c|c}
    & No changepoint pruning & Functional pruning \\
    \hline
    Segment Neighborhood & Constrained DPA & Generalized PDPA \\
$K$ segments    & Sub-optimal,  $O(KN^2)$ & Optimal, $O(KN\log N)$\\
    & \citet{HOCKING-PeakSeg} & 
% dont know why this isn't abbreviated to etal!!!
\citet{Hocking-constrained-changepoint-detection}
%Hocking \emph{et al.} (2017)
 \\
& \pkg{PeakSegDP} & \pkg{PeakSegOptimal}\\
    \hline
    Optimal Partitioning &  & Generalized FPOP \\
    Penalty $\lambda$&  & Optimal, $O(N\log N)$\\
    &  & \textbf{This work}\\
    & & \pkg{PeakSegPipeline}\\
    \hline
  \end{tabular}
  \caption{Algorithms for optimal changepoint detection with up-down 
constraints on adjacent segment means. Previous work is limited to solvers for the Segment Neighborhood problem; this paper presents Generalized Functional Pruning Optimal Partitioning (GFPOP), Algorithm~\ref{algo:GFPOP}.}
  \label{tab:constrained-algos}
\end{table}

%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\subsection{Contributions}

This paper presents two new algorithms for constrained optimal
changepoint detection (Section~\ref{sec:algorithms}), along with an
analysis of their empirical time/space complexity in a benchmark of
large genomic data (Section~\ref{sec:results}). The algorithms are
implemented in the \R\ package \pkg{PeakSegPipeline} on
GitHub.\footnote{ \url{https://github.com/tdhock/PeakSegPipeline} }

First, we present a new algorithm for solving the Optimal Partitioning
problem with up-down constraints between adjacent segment means
(GFPOP, Algorithm~\ref{algo:GFPOP}). The fastest existing algorithm
for the up-down constrained changepoint model was the
$O(KN\log N)$ solver for the Segment Neighborhood problem
(Table~\ref{tab:constrained-algos}). In large genomic data sets, we
are only interested in models with many segments/changepoints, so it
is a waste of time and space to compute all models from 1 to $K$
segments using Segment Neighborhood algorithms. Our proposed GFPOP
algorithm solves the Optimal Partitioning problem, so yields one
optimal model with $K$ segments (without having to compute the models
from 1 to $K-1$ segments). We show that the empirical complexity of
our GFPOP implementation is $O(N\log N)$ time, $O(N\log N)$ space, and
$O(\log N)$ memory, which makes it possible to compute optimal models with many peaks
for large genomic data sets on common laptop computers.

Although solving the Optimal Partitioning problem is faster by a
factor of $O(K)$, the user is unable to directly choose the number of
segments $K$. The user inputs a penalty $\lambda$, and gets one of the
optimal changepoint models as output. Thus, we also propose a
sequential search (Algorithm~\ref{algo:seq-search}) which computes the
optimal model for a specified number of segments $K$. It repeatedly
calls GFPOP to solve Optimal Partitioning with different penalties
$\lambda$, until it finds the maximum likelihood model with at most $K$
segments. We empirically show that the sequential search only requires
$O(\log K)$ evaluations of GFPOP. Overall the proposed algorithm is
thus $O( N \log(N)\log(K))$ time, $O(N\log N)$ disk, $O(\log N)$
memory. In an analysis of benchmark genomic data sets, we show that
this algorithm can compute an optimal model with $O(\sqrt N)>1000$ peaks
for $N=10^7$ data using only hours of compute time and gigabytes of
storage (which is much less than weeks/terabytes which would be
required for the Segment Neighborhood solver). 

\section{Statistical models and optimization problems} \label{sec:models}


\subsection{Unconstrained Optimal Partitioning problem}

Define our loss function to be the Poisson loss, $\ell(m,z)=m-z\log m$, and let $\lambda>0$ be a penalty for adding a changepoint. Then we can infer the number of segments and the location of the changes by solving
the Optimal Partitioning problem
\begin{align}
  \label{min:op}
  \minimize_{
  \substack{
  \mathbf m\in\RR^N
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(m_i\neq m_{i+1}).
\end{align}
The first term measures fit to the data, and the second term measures model complexity, which is proportional to the number of
changepoints. 
The non-negative penalty $\lambda\in\RR_+$ controls the tradeoff
between the two objectives (it is a tuning parameter that must be
fixed before solving the problem). Larger penalty $\lambda$ values
result in models with fewer changepoints/segments. The extreme penalty
values are $\lambda=0$ which yields $N$ segments ($N-1$ changepoints),
and $\lambda=\infty$ which yields 1 segment (0 changepoints).

Below we write an equivalent version of the Optimal Partitioning
problem, in terms of changepoint variables $c_i$ and state variables
$s_i$:
\begin{align}
  \label{min:op-c}
  \minimize_{
  \substack{
  \mathbf m\in\RR^N,\, \mathbf s\in\{0\}^N\\
  \mathbf c\in\{0,1\}^{N-1}\\
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(c_i=1) \\
  \text{subject to\ \ } &\ \text{no change: }c_i = 0 \Rightarrow m_i = m_{i+1}\text{ and } s_i=s_{i+1}, \nonumber\\
    &\ \text{change: }c_i = 1 \Rightarrow m_i \neq m_{i+1}\text{ and }(s_i,s_{i+1})=(0,0).\label{eq:change-constraint}
\end{align}
Note that the state $s_i$ and changepoint $c_i$ variables could be
eliminated from the optimization problem --- $s_i=0$ and 
$c_i=I(m_i\neq m_{i+1})$ for all $i$. We include them in problem~(\ref{min:op-c})
in order to show the relationship with the problem in the next
section, with constraints between adjacent segment means.

\citet{Hocking-constrained-changepoint-detection} proposed to use a
graph to represent a constrained changepoint model. The graph that
corresponds to problem~(\ref{min:op-c}) is shown in
Figure~\ref{fig:state-graph}, left. In such graphs, nodes represent
possible values of state variables $s_i$ and edges represent possible
changepoints $c_i\neq 0$. Each edge/changepoint corresponds to a
constraint such as (\ref{eq:change-constraint}).


\subsection{Optimal Partitioning problem with up-down constraints between
  adjacent segment means}

\begin{figure}
  \centering
  \begin{minipage}{3in}
    \centering
    \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2cm,
      thick,main node/.style={circle,draw}]

      \node[main node] (0) {$s=0$};

      \path[every node/.style={font=\sffamily\small}]
      (0) edge [loop above] node {$c=1,\lambda$} (0);
    \end{tikzpicture}
  \end{minipage}
  \begin{minipage}{3in}
    \centering
    \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2cm,
      thick,main node/.style={circle,draw}]

      \node[main node] (1) {$s=1$};
      \node[main node] (0) [below of=1] {$s=0$};
      \node (start) [left of=0] {start};
      \node (end) [right of=0] {end};

      \path[every node/.style={font=\sffamily\small}]
      (0) edge [bend left] node {$c=1, \lambda, \leq$} (1)
      (start) edge [dashed] (0)
      (0) edge [dashed] (end)
      (1) edge [bend left] node {$c=-1, 0, \geq$} (0);
    \end{tikzpicture}
  \end{minipage}
  \caption{State graphs for two changepoint models. Nodes represent
    states and solid edges represent changepoints. \textbf{Left:}
    one-state model with no constraints between adjacent segment
    means, problem~(\ref{min:op-c}). \textbf{Right}: two-state model with
    up-down constraints between adjacent segment means,
    problem~(\ref{min:op-up-down}). State $s=0$ represents background
    noise (small values) whereas state $s=1$ represents peaks (large
    values). Constraint $c=1$ enforces a non-decreasing change via the
    min-less operator ($\leq$) with a penalty of $\lambda$; $c=-1$
    enforces a non-increasing change via the min-more operator
    ($\geq$) with a penalty of $0$. The model is additionally
    constrained to start and end in the background noise $s=0$ state
    ($s_1=s_N=0$).}
  \label{fig:state-graph}
\end{figure}

For genomic data such as ChIP-seq \citep{chip-seq}, it is desirable to
have a changepoint model which is interpretable in terms of peaks
(large values) and background noise (small values). We therefore
propose a model based on the graph shown in
Figure~\ref{fig:state-graph}, right. It has two nodes/states: $s=0$
for background, and $s=1$ for peaks. It has two edges/changes: $c=1$
for a non-decreasing change from background $s=0$ to a peak $s=1$, and
$c=-1$ for a non-increasing change from a peak $s=1$ to background
$s=0$. Furthermore, the model is constrained to start and end in the
background state (because peaks are not present at the boundaries of
genomic data sequences). Maximum likelihood inference in
this model corresponds to the following minimization problem:
\begin{align}
  \label{min:op-up-down}
  F(\lambda) = \min_{
  \substack{
  \mathbf m\in\RR^N,\ \mathbf s\in\{0, 1\}^N\\
  \mathbf c\in\{-1, 0,1\}^{N-1}\\
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(c_i = 1) \\
  \text{subject to\ \ } &\ \text{no change: }c_i = 0 \Rightarrow m_i = m_{i+1}\text{ and }s_i=s_{i+1}, \nonumber\\
    &\ \text{non-decreasing change: }c_i = 1 \Rightarrow m_i \leq m_{i+1}\text{ and }(s_i,s_{i+1})=(0,1),\nonumber\\
    &\ \text{non-increasing change: } c_i = -1 \Rightarrow m_i \geq m_{i+1}\text{ and }(s_i,s_{i+1})=(1,0),\nonumber\\
  & \ \text{start and end down: } s_1=s_N=0.\nonumber
\end{align}
Note how the problem~(\ref{min:op-up-down}) with up-down constraints
is of the same form as the previous unconstrained
problem~(\ref{min:op-c}). Again there is one constraint for every
edge/changepoint in the state graph
(Figure~\ref{fig:state-graph}). The difference is that in
problem~(\ref{min:op-up-down}), we have inequality constraints between
adjacent segment means (e.g. when $c_i=1$, we must have a
non-decreasing change in the mean $m_i\leq m_{i+1}$). Another
difference is the model complexity in problem~(\ref{min:op-up-down})
is the total number of $c_i=1$ non-decreasing changes, which is
equivalent to the number of peak segments $P$, and is linear in the
total number of segments $K=2P+1$ and changes $K-1=2P$.


The solution to the Optimal Partitioning
problem~(\ref{min:op-up-down}) can be computed by first solving the
Segment Neighborhood version of the problem \citep{Maidstone2016}. In
\R\ the \pkg{PeakSegDP} package provides a sub-optimal solution in
$O(K N^2)$ time, and the \pkg{PeakSegOptimal} package provides an
optimal solution in $O(KN\log N)$ time. However in large genomic data
the number of peaks/segments $K$ increases with $N$, so it is
intractable to solve the Segment Neighborhood problem because both $N$
and $K$ are large. Therefore in the next section we propose a new
algorithm for directly solving the constrained Optimal Partitioning
problem~(\ref{min:op-up-down}), which can yield a large number of
peaks in $O(N\log N)$ time.

\section{Algorithms and Software}
\label{sec:algorithms}
\subsection{Generalized Functional Pruning Optimal Partitioning (GFPOP)}

In this section we propose a generalization of the FPOP algorithm
\citep{Maidstone2016} which allows optimal inference in models with
inequality constraints between adjacent means, such as
problem~(\ref{min:op-up-down}). In particular we implemented the
optimal changepoint model using the Poisson loss and the up-down
constraints. The state graph
(Figure~\ref{fig:state-graph}, right) can be converted into a directed
acyclic graph (Figure~\ref{fig:computation-graph}) that represents the
dynamic programming updates required to solve
problem~(\ref{min:op-up-down}). Each node in the computation graph
represents an optimal cost function, and each edge represents an input
to the $\min\{\}$ operation in the dynamic programming
equations~(\ref{eq:dp-over}) and (\ref{eq:dp-under}) below.

\begin{figure}
  \centering
  \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.5cm,
    thick,main node/.style={circle,draw}]
    \node[main node] (peak_t1) {$ C_{1,i-1}$};
    \node[main node] (bkg_t1) [below of=peak_t1] {$ C_{0,i-1}$};
    \node[main node] (peak_t) [right of=peak_t1] {$ C_{1,i}$};
    \node[main node] (bkg_t) [right of=bkg_t1] {$ C_{0,i}$};
    \node[main node] (peak_2) [left of=peak_t1] {$ C_{1,2}$};
    \node[main node] (bkg_2) [left of=bkg_t1] {$ C_{0,2}$};
    \node[main node] (peak_N1) [right of=peak_t] {$ C_{1,N-1}$};
    \node[main node] (bkg_N1) [right of=bkg_t] {$ C_{0,N-1}$};
    \node[main node] (bkg_N) [right of=bkg_N1] {$ C_{0,N}$};
    \node[main node] (bkg_1) [left of=bkg_2] {$ C_{0,1}$};
    \path[every node/.style={font=\small}]
    (peak_t1) edge [dotted] node {$ C_{1,i-1}$} (peak_t)
    (peak_t1) edge [black, bend right] node [right] {$ C_{1,i-1}^{\geq}$} (bkg_t)
    (bkg_t1) edge [dotted] node[midway, below] {$ C_{0,i-1}$} (bkg_t)
    (bkg_t1) edge [black, bend left] node[right] {$ C_{0,i-1}^{\leq}+\lambda$} (peak_t)
    (bkg_1) edge [black, bend left] node[right] {$ C_{0,1}^{\leq}+\lambda$} (peak_2)
    (bkg_1) edge [dotted] node[midway, below] {$ C_{0,1}$} (bkg_2)
    (peak_N1) edge [black, bend right] node [right] {$ C_{1,N-1}^{\geq}$} (bkg_N)
    (bkg_N1) edge [dotted] node[midway, below] {$ C_{0,N-1}$} (bkg_N)
    (bkg_2) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (bkg_t1)
    (peak_2) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (peak_t1)
    (bkg_N1) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (bkg_t)
    (peak_N1) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (peak_t)
    ;
  \end{tikzpicture}
  \caption{Directed acyclic graph (DAG) representing dynamic
    programming computations (Algorithm~\ref{algo:GFPOP}) for
    changepoint model with up-down constraints between adjacent
    segment means. Nodes in the graph repesent cost functions, and
    edges represent inputs to the the MinOfTwo sub-routine
    (solid=changepoint, dotted=no change). There is one column for
    each data point and one row for each state: the optimal cost of
    the peak state $s=1$ at data point $i$ is $ C_{1,i}$ (top
    row); the optimal cost of the background noise state $s=0$ is
    $ C_{0,i}$ (bottom row). There is only one edge going to
    $ C_{0,2}$ and $ C_{1,2}$ because the model is
    constrained to start in the background noise state ($s_1=0$).}
  \label{fig:computation-graph}
\end{figure}

More precisely, we define the optimal cost of mean $\mu$ in state
$\sigma$ at any data point $\tau\in\{1,\dots,N\}$ to be
\begin{align}
  \label{eq:C_sigma_tau}
   C_{\sigma,\tau}(\mu)=\min_{
  \substack{
  \mathbf m\in\RR^\tau,\ \mathbf s\in\{0, 1\}^\tau\\
  \mathbf c\in\{-1, 0,1\}^{\tau-1}\\
  }
  } &\ \ 
      \sum_{i=1}^\tau \ell(m_i, z_i) + \lambda \sum_{i=1}^{\tau-1} I(c_i = 1) \\
  \text{subject to\ \ } &\ c_i = 0 \Rightarrow m_i = m_{i+1}\text{ and }s_i=s_{i+1}, \nonumber\\
    &\ c_i = 1 \Rightarrow m_i \leq m_{i+1}\text{ and }(s_i,s_{i+1})=(0,1),\nonumber\\
    &\ c_i = -1 \Rightarrow m_i \geq m_{i+1}\text{ and }(s_i,s_{i+1})=(1,0),\nonumber\\
  & \  s_1=s_N=0,\nonumber\\
& \ m_\tau=\mu,\, s_\tau=\sigma.\label{tau-constraints}
\end{align}
Note how the objective and constraints above are identical to the
up-down constrained Optimal Partitioning
problem~(\ref{min:op-up-down}) up to $\tau-1$ data points, but with
two added constraints at data point $\tau$ (\ref{tau-constraints}). At
data point $\tau$ the mean is constrained to be $m_\tau=\mu$ and the
state is constrained to be $s_\tau=\sigma$. The optimal cost
$C_{\sigma,\tau}(\mu)$ is a real-valued function that must be computed
by minimizing over all previous means $m_1,\dots,m_{\tau-1}$, states
$s_1,\dots,s_{\tau-1}$, and changes $c_1,\dots,c_{\tau-1}$.
It can be computed recursively using the dynamic programming
updates that we propose below.

The algorithm begins by initializing the optimal cost of the
background state at the first data point,
\begin{equation}
  \label{eq:init_1}
  C_{0,1}(\mu)=\ell(\mu, z_1).
\end{equation}
The computations for the second data point are also special, because
the model is constrained to start in the background state $s_1=0$. To
get to the background state $s_2=0$ at the second data point requires no
change ($c_1=0$), with a cost of
\begin{eqnarray}
  C_{0,2}(\mu)=C_{0,1}(\mu)+\ell(\mu, z_2).
\end{eqnarray}
Similarly, to get to the peak state $s_2=1$ at the second data point
requires a non-decreasing change ($c_1=1$), with a cost of
\begin{equation}
  \label{eq:C_12}
  C_{1,2}(\mu)=\min_{m_1\leq\mu} C_{0,1}(m_1)+\lambda + \ell(\mu, z_2)=C_{0,1}^\leq(\mu)+\lambda+\ell(\mu, z_2).
\end{equation}
Note that we were able to re-write the optimal cost function in terms
of a single variable $\mu$ by using the min-less operator,
\begin{equation}
  \label{eq:min-less}
  f^\leq(\mu) = \min_{x\leq\mu} f(x).
\end{equation}
The min-less operator was introduced by \citet{Hocking-constrained-changepoint-detection} in order to compute the optimal cost in the
functional pruning algorithm that solves the Segment Neighborhood
version of this problem. 

More generally, the dynamic programming update rules can be derived
from the computation graph (Figure~\ref{fig:computation-graph}). The
optimal cost of the peak state $s=1$ at data $i>2$ is
\begin{equation}
  \label{eq:dp-over}
   C_{1,i}(\mu) = \ell(\mu, z_i) + \min\{
   C_{1,i-1}(\mu),\, 
   C_{0,i-1}^\leq(\mu)+\lambda
\}.
\end{equation}
Note how the inputs to the $\min\{\}$ operation are the same as the
edges leading to the $C_{1,i}$ node in the computation graph
(Figure~\ref{fig:computation-graph}). 

Similarly, the optimal cost of
the background state $s=0$ is
\begin{equation}
  \label{eq:dp-under}
     C_{0,i}(\mu) = \ell(\mu, z_i) + \min\{
   C_{0,i-1}(\mu),\, 
   C_{1,i-1}^\geq(\mu)+\lambda
\},
\end{equation}
where the min-more operator is defined as
\begin{equation}
  \label{eq:min-more}
  f^\geq(\mu) = \min_{x\geq\mu} f(x).
\end{equation}
These dynamic programming computations are summarized in
Algorithm~\ref{algo:GFPOP}, Generalized Functional Pruning Optimal
Partitioning. The key to implementing the algorithm is to use a
PiecewiseFunction data structure that can exactly represent an
optimal cost function $C_{s,i}$. In the case of the Poisson loss,
each $C_{s,i}(\mu)$ is a piecewise function where each piece is of the
form $\alpha\mu + \beta\log\mu +
\gamma$. Therefore the optimal cost can be stored as a list
of intervals of $\mu\in[\text{MIN},\text{MAX}]$, each with
coefficients $\alpha,\beta,\gamma$.

\begin{algorithm}
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf z\in\RR^N$, penalty constant $\lambda\geq 0$.
\STATE Output: vectors of optimal segment means $U\in\RR^{N}$ and ends $T\in\{1,\dots,N\}^{N}$
\STATE Initialize $2\times N$ empty PiecewiseFunction objects $C_{s,i}$ either in memory or on disk.
\label{line:initialize}
\STATE Compute min $\underline z$ and max $\overline z$ of $\mathbf z$.
\label{line:op-min-max}
\STATE
$ C_{0,1}\gets
 \text{OnePiece}(z_1, \underline z, \overline z)$
\label{line:first-cost}
\STATE for data point $i$ from 2 to $N$: // dynamic programming
\label{line:for-dp-t}
\begin{ALC@g}
  \STATE $ M_1\gets \lambda + \text{MinLess}(i-1,  C_{0,i-1})$ //cost of non-decreasing change
  \label{line:op-MinLess}
  \STATE $ C_{1,i}\gets \text{MinOfTwo}( M_1,  C_{1,i-1})+\text{OnePiece}(z_i, \underline z, \overline z)$
  \label{line:op-MinOfTwo}
  \STATE $ M_0\gets \text{MinMore}(i-1,  C_{1,i-1})$ //cost of non-increasing change
  \label{line:op-MinMore}
  \STATE $ C_{0,i}\gets \text{MinOfTwo}( M_0,  C_{0,i-1})+\text{OnePiece}(z_i, \underline z, \overline z)$
  \label{line:op-MinOfTwo-under}
\end{ALC@g}
\STATE $\text{mean},\text{prevEnd},\text{prevMean}\gets \text{ArgMin}( C_{0,n})$ // begin decoding
\label{line:op-ArgMin}
\STATE $\text{seg}\gets 1;\, U_{\text{seg}}\gets \text{mean};\, T_{\text{seg}}\gets \text{prevEnd}$
\label{line:op-store-i}
\STATE while $\text{prevEnd} > 0$:
\begin{ALC@g}
  \STATE if $\text{prevMean} < \infty$: $\text{mean}\gets \text{prevMean}$
  \STATE if $\text{seg}$ is odd: $\text{cost}\gets  C_{1,\text{prevEnd}}$ else $ C_{0,\text{prevEnd}}$
  \STATE $\text{prevEnd},\text{prevMean}\gets\text{FindMean}(\text{mean}, \text{cost})$
  \STATE $\text{seg}\gets \text{seg}+1;\, U_{\text{seg}}\gets \text{mean};\, T_{\text{seg}}\gets \text{prevEnd}$
\label{line:op-i+1}
\end{ALC@g}
\caption{\label{algo:GFPOP}Generalized Functional Pruning Optimal
  Partitioning (GFPOP) for changepoint model with up-down constraints
  between adjacent segment means.}
\end{algorithmic}
\end{algorithm}

\paragraph{Discussion of pseudocode.} Algorithm~\ref{algo:GFPOP} begins on line~\ref{line:initialize} by
initializing the array $C_{s, i}$ of optimal cost functions (either in
memory or on disk). It then computes the min $\underline z$ and max
$\overline z$ of the data (line~\ref{line:op-min-max}) and uses the
OnePiece sub-routine to initialize the optimal cost at the first data
point (line~\ref{line:first-cost}). Since the Poisson loss is
$\ell(\mu, z_1)=\mu-z_1\log \mu$, this first optimal cost function is
represented as the single function piece with interval/coefficients
$(\alpha=1,\beta=-z_1,\gamma=0,\text{MIN}=\underline
z,\text{MAX}=\overline z)$. 

The dynamic programming recursion in this algorithm is a loop over
data points $i$ (line~\ref{line:for-dp-t}). To compute $C_{1,i}$, the
penalty constant $\lambda$ is added to all of the result of MinLess
(line~\ref{line:op-MinLess}), before computing MinOfTwo and adding the
cost of the new data point (line~\ref{line:op-MinOfTwo}). The
computation for $C_{0,i}$ is similar, but uses MinMore and does not
add the penalty $\lambda$
(lines~\ref{line:op-MinMore}--\ref{line:op-MinOfTwo-under}). The
details about how the MinLess/MinMore/MinOfTwo sub-routines process
the PiecewiseFunction objects have been described previously
\citep{Hocking-constrained-changepoint-detection}.

After computing the optimal cost functions, the decoding of optimal
parameters occurs on
lines~\ref{line:op-ArgMin}--\ref{line:op-i+1}. The last segment mean
and second to last segment end are first stored on
line~\ref{line:op-store-i} in $(U_1,T_1)$. For each other segment $i$,
the mean and previous segment end are stored on line~\ref{line:op-i+1}
in $(U_i,T_i)$. Note that there should be space to store $(U_i,T_i)$
parameters for up to $N$ segments. In practice our implementation
writes these parameters to a text output file on disk.

\paragraph{Computational complexity.} The complexity of Algorithm~\ref{algo:GFPOP} is $O(N I)$, where $I$ is
the mean number of intervals (function pieces) that are used to
represent the $C_{s, i}$ cost functions. Theoretically there are some
pathological data sets for which the algorithm computes $I=O(N)$
intervals, which results in the worst-case complexity of $O(N^2)$.
Since the number of intervals in real data is empirically
$I=O(\log N)$ (see Figure~\ref{fig:intervals}), the overall complexity
of Algorithm~\ref{algo:GFPOP} is on average $O(N \log N)$. Using
disk-based storage its complexity is $O(N\log N)$ time, $O(N\log N)$
disk, $O(\log N)$ memory. 

\paragraph{Usage in \R.} We implemented the disk-based version of Algorithm~\ref{algo:GFPOP} in
\proglang{C++} code with an interface in the \R\ package
\pkg{PeakSegPipeline}. To illustrate its usage we first load a set of
genomic data,

<<loadData>>=

library(PeakSegPipeline)
data(Mono27ac, package="PeakSegPipeline")
Mono27ac$coverage

@ 

Note that the 4 column bedGraph format shown above must be used to
represent a data set. Futhermore a run-length encoding should be used
for data sets that have runs of the same values. Each row represents a
sequence of identical data values. For example the first row means
that the value 0 occurs on the 72601 positions in [60000,132601), the
second row means a value of 1 for the 42 positions in [132601,132643),
etc. This run-length encoding results in significant savings in disk
space and time 
%while maintaining optimality
\citep{Segmentor}; for
example in the data set above there are only 6921 lines used to
represent 520000 data values.

In order to handle very large data sets while using only $O(\log N)$
memory, the algorithm reads input data from a text file on disk (and
never actually stores the entire data set in memory). So before using
the algorithm we must save the data set to disk, in bedGraph format
(the four columns shown above, separated by tabs). Note that the file
name must be \verb|coverage.bedGraph|:

<<saveData>>=

data.dir <- file.path("Mono27ac", "chr11:60000-580000")
dir.create(data.dir, showWarnings=FALSE, recursive=TRUE)
write.table(
  Mono27ac$coverage, file.path(data.dir, "coverage.bedGraph"),
  col.names=FALSE, row.names=FALSE, quote=FALSE, sep="\t")

@ 

After saving the file to disk, we can run the algorithm using the code
below: 

<<problemPeakSegFPOP>>=

## Compute one model with penalty=10000
fit <- PeakSegPipeline::problem.PeakSegFPOP(data.dir, "10000")

@ 

For the first argument you must give the folder name (not the
\verb|coverage.bedGraph| file name) to the problem.PeakSegFPOP
function.  Note that the second argument must be a character string
that represents a penalty value (non-negative real number, larger
penalties yield fewer peaks). The smallest value is \code{"0"} which
yields max peaks, and the largest value is \code{"Inf"} which yields
no peaks. It must be an \R\ character string (not a real number)
because that string is used to create files 
%such as \verb|Mono27ac/chr11:60000-580000/coverage.bedGraph_penalty=10000_loss.tsv|
which are used to store/cache the results. If the files already exist
(and are consistent) then problem.PeakSegFPOP just reads them;
otherwise it runs the dynamic programming C++ code in order to create
those files.

The returned \code{fit} object is a named list of data.tables. The
\code{fit\$loss} component shown below is one row that contains general
information about the computed model: 

<<fitLoss>>=

fit$loss

@ 

Above we can see that the optimal model for $\lambda=10^4$ had $P=7$
peaks ($K=15$ segments/$K-1=14$ changepoints). The
\code{fit\$segments} component is used to visualize three of those
peaks in a subset of the data below.

<<plotModel, fig=TRUE, height=3>>=

library(ggplot2)
gg <- ggplot()+theme_bw()+
  geom_step(aes(chromStart, count), color="grey50", data=Mono27ac$coverage)+
  geom_segment(aes(chromStart, mean, xend=chromEnd, yend=mean),
    color="green", size=1, data=fit$segments)+
  coord_cartesian(xlim=c(2e5, 3e5))
print(gg)

@ 

\subsection{Sequential search algorithm for $P^*$ peaks}

Note that in GFPOP~(Algorithm~\ref{algo:GFPOP}), the user inputs a
penalty $\lambda$, and is unable to directly choose the number of
segments/peaks. In this section, we propose an algorithm that allows
the user to specify the number of peaks. The algorithm then repeatedly
calls GFPOP until it finds the most likely model with at most the
specified number of peaks.

To understand how the algorithm works, we must review the relationship
between the Optimal Partitioning and Segment Neighborhood problems
\citep{Maidstone2016}. We define the optimal loss for a given
number of peaks $P$ as
\begin{align}
  \label{eq:L_P}
  L_P=\min_{
  \substack{
  \mathbf m\in\RR^N,\ \mathbf s\in\{0, 1\}^N\\
  \mathbf c\in\{-1, 0,1\}^{N-1}\\
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) \\
  \text{subject to\ \ } &\ c_i = 0 \Rightarrow m_i = m_{i+1}\text{ and }s_i=s_{i+1}, \nonumber\\
    &\ c_i = 1 \Rightarrow m_i \leq m_{i+1}\text{ and }(s_i,s_{i+1})=(0,1),\nonumber\\
    &\ c_i = -1 \Rightarrow m_i \geq m_{i+1}\text{ and }(s_i,s_{i+1})=(1,0),\nonumber\\
  & \  s_1=s_N=0,\nonumber\\
& P = \sum_{i=1}^{N-1} I(c_i = 1).\label{P-constraint}
\end{align}
The problem above is the Segment Neighborhood version of the Optimal
Partitioning problem that GFPOP solves (\ref{min:op-up-down}). The
penalty $\lambda$ is absent, and the model complexity (the number of
peaks) has moved to a constraint (\ref{P-constraint}). Recall that
$F(\lambda)$ is the minimum value of the Optimal Partitioning
problem~(\ref{min:op-up-down}). It can be written in terms of the
solution to the Segment Neighborhood problem~(\ref{eq:L_P}),
\begin{equation}
  \label{eq:F(lambda)}
  F(\lambda) = \min_{
    P\in\{0,1,\dots,P_{\text{max}}\} 
  } L_P + \lambda P.
\end{equation}
The equation above makes it clear that there are only a finite number
of optimal changepoint models (from 0 to $P_{\text{max}}$
peaks). $F(\lambda)$ is a concave, non-decreasing function that can be
computed as the minimum of a finite number of affine functions
$f_P(\lambda)=L_P+\lambda P$.

We now assume the user wants to compute the optimal model with a fixed
number of peaks $P^*$. To compute that model we will maximize the function
\begin{equation}
  \label{eq:G}
  G(\lambda)=F(\lambda) - P^*\lambda = \min_{
    P\in\{0,1,\dots,P_{\text{max}}\}
    } \underbrace{L_P + \lambda(P-P^*)}_{g_P(\lambda)}.
\end{equation}
From the equation above it is clear that $G(\lambda)$ is a concave
function that can be computed as the minimum of a finite number of
affine functions $g_P(\lambda)=L_P+\lambda(P-P^*)$. 
%The maximum of $G$ is characterized by the subdifferential equation $0\in \partial G(\lambda)$. 
For an example $G$
function see Figure~\ref{fig:evaluations-concave}.

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \input{jss-figure-evaluations-concave}
\end{minipage} 
\begin{minipage}{3in} 
  \input{jss-figure-evaluations-concave-zoom} 
\end{minipage} 
\vskip -0.5cm
\caption{\label{fig:evaluations-concave} Example of a $G(\lambda)$
  function which is maximized in order to find the most likely model
  with at most $P^*=75$ peaks. Red dots show $G(\lambda)$ values
  evaluated by the algorithm; grey lines show affine functions
  $g_P(\lambda)=L_P+(P-P^*)\lambda$ used to determine the next
  $\lambda$ value (line~\ref{line:lambda} of
  Algorithm~\ref{algo:seq-search}). \textbf{Left:} iteration 1 runs
  GFPOP with $\lambda\in\{0,\infty\}$, resulting in initial lower
  bound of $\underline p=0$ peaks and upper bound of
  $\overline p=29681$ peaks. In iteration 2 the algorithm finds the
  intersection of the upper/lower bound lines
  $g_0(\lambda)=g_{29681}(\lambda)$ at $\lambda=90.9$; running GFPOP
  with that penalty reduces the upper bound to $\underline
  p=3445$. \textbf{Right:} In the last iteration (13), we run GFPOP
  with $\lambda=2522.1$ (which is where $g_{74}$
  intersects $g_{76}$), resulting in 76 peaks when we already have
  $\overline p=76$ as an upper bound (computed in iteration 12). The
  maximum of $G$ is thus
  $G(2522.1)=g_{74}(2522.1)=g_{76}(2522.1)$; the algorithm returns the
  model with $P=74$ peaks.}
\end{figure}  
   
\paragraph{Discussion of pseudocode.} Algorithm~\ref{algo:seq-search}
summarizes the sequential search. The main idea of the sequential
search algorithm is to keep track of a lower bound $\underline p<P^*$
and upper bound $\overline p>P^*$ on the number of peaks computed thus
far.  The algorithm starts with $\lambda=0$, $\overline p=P_{\text{max}}$
(line~\ref{line:lambda0}) and $\lambda=\infty,\underline p=0$
(line~\ref{line:lambdaInf}).  At each iteration of the algorithm, we
find the intersection of the affine functions
$g_{\underline p}(\lambda)=g_{\overline p}(\lambda)$, which leads to a
new candidate penalty
$\lambda_{\text{new}}=(L_{\overline p}-L_{\underline
  p})/(\underline p-\overline p)$ (line~\ref{line:lambda}).
As previously described
\citep{crops}, there are two possibilities for the solution to the
Optimal Partitioning problem:
\begin{itemize}
\item GFPOP($\lambda_{\text{new}}$) yields
  $\underline p$ or $\overline p$ peaks (line~\ref{line:returnFewer}). In that case
  $\max_\lambda G(\lambda)=G(\lambda_{\text{new}})=g_{\underline p}(\lambda_{\text{new}})=g_{\overline p}(\lambda_{\text{new}})$ and there
  is no Optimal Partitioning model with $P^*$ peaks. We terminate the
  algorithm by returning the model with $\underline p$ peaks.
\item GFPOP($\lambda_{\text{new}}$) yields a new model
  with $p_{\text{new}}$ peaks. If $p_{\text{new}}=P^*$ then
  $\max_\lambda G(\lambda)=L_{P^*}$ and we return this
  model (line~\ref{line:returnNew}). Otherwise it must be true that
  $\underline p < p_{\text{new}} < \overline p$. If $\underline p<p_{\text{new}} < P^*$
  then we use $p_{\text{new}}$ for a new lower bound $\underline p$ (line~\ref{line:new-under}); otherwise
  we use it for a new upper bound $\overline p$ (line~\ref{line:new-over}).
  %% implying
  %% $G(\lambda_{\text{new}})=g_{p_{\text{new}}}(\lambda_{\underline
  %%   p,\overline p})$. In that case,
  %% $G(\lambda_{\text{new}})<g_{\underline
  %%   p}(\lambda_{\text{new}})$ and
  %% $G(\lambda_{\text{new}})<g_{\overline
  %%   p}(\lambda_{\text{new}})$. We cont
\end{itemize}
%% Because $G$ is defined as the
%% minimum of the $g_P$ (\ref{eq:G}), we know that
%% $G(\lambda_{\text{new}})\leq g_{\underline p}(\lambda)$
%% and
%% $G(\lambda_{\text{new}})\leq g_{\overline p}(\lambda)$.



\begin{algorithm}
    \begin{algorithmic}[1]
  \STATE Input: data $\mathbf z\in\RR^N$, target peaks $P^*$.
  \STATE $\overline L,\overline p \gets \text{GFPOP}(\mathbf z, \lambda=0)$ // initialize upper bound to max peak model
  \label{line:lambda0}
  \STATE $\underline L,\underline p \gets \text{GFPOP}(\mathbf z, \lambda=\infty)$ // initialize lower bound to 0 peak model
  \label{line:lambdaInf}
  \STATE While $P^*\not\in \{\underline p,\, \overline p\}$:
  \label{line:while}
  \begin{ALC@g}
    \STATE $\lambda_{\text{new}}=(\overline L-\underline L)/(\underline p-\overline p)$
    \label{line:lambda}
    \STATE $L_{\text{new}},p_{\text{new}}\gets\text{GFPOP}(\mathbf z, \lambda_{\text{new}})$
    \STATE If $p_{\text{new}}\in\{\underline p, \overline p\}$: return model with $\underline p$ peaks.
    \label{line:returnFewer}
    \STATE If $p_{\text{new}}=P^*$: return model with $p_{\text{new}}$ peaks.
    \label{line:returnNew}
    \STATE If $p_{\text{new}} < P^*$: $\underline L,\underline p\gets L_{\text{new}},p_{\text{new}}$ // new lower bound
    \label{line:new-under}
    \STATE Else: $\overline L,\overline p\gets L_{\text{new}},p_{\text{new}}$ // new upper bound
    \label{line:new-over}
% > prob.i <- 3
% > fit.list$others[order(iteration)][, list(target.peaks=prob$peaks, iteration, under, over, penalty, peaks, total.cost)]
%    target.peaks iteration under over    penalty peaks total.cost
% 1:           33         1    NA   NA     0.0000  7487 -201361.96
% 2:           33         1    NA   NA        Inf     0  920923.98
% 3:           33         2     0 7487   149.8979   753  -24385.02
% 4:           33         3     0  753  1255.3904    47  153676.28
% 5:           33         4     0   47 16324.4191    10  310043.81
% 6:           33         5    10   47  4226.1495    21  214200.02
% 7:           33         6    21   47  2327.8360    33  177484.99
% > 
  \end{ALC@g}
  \end{algorithmic}
\caption{\label{algo:seq-search}Sequential search for $P^*$ peaks using GFPOP.}
\end{algorithm}

\paragraph{Computational complexity.} The space complexity is the same as
GFPOP: $O(N\log N)$ disk and $O(\log N)$ memory. Its time complexity
is linear in the number of iterations of the while loop
(line~\ref{line:while}). Empirically we see $O(\log P^*)$ iterations
(Figure~\ref{fig:variable-peaks}), which implies an overall time
complexity of $O(N\log(N)\log(P^*))$.

\paragraph{Usage in \R.} The \R\ code below computes the optimal model with 17 peaks:

<<seq-search>>=

## Compute the optimal model with 17 peaks.
fit <- PeakSegPipeline::problem.sequentialSearch(data.dir, 17L)

@ 

If you want to see how many iterations/penalties the algorithm
required in order to compute the optimal model with 17 peaks, you can
look at the \code{fit$others} component:

<<>>=

fit$others[, list(iteration, under, over, penalty, peaks, total.loss)]

@ 

The output above shows that the algorithm only used three iterations
to compute the optimal model with 17 peaks. The \code{under} and
\code{over} columns show the current values of $\underline p$ and
$\overline p$, respectively. The \code{peaks} and \code{total.loss}
are $p_{\text{new}},L_{\text{new}}$ from the model that resulted from running GFPOP
with $\lambda=\text{\code{penalty}}$. Note that iteration 1 evaluates both
extreme penalties $\lambda\in\{0,\infty\}$ in parallel (and
$\lambda=\infty$ is the trivial model with 0 peaks that can be
computed without dynamic programming), so these two models are
considered a single iteration.

% \begin{leftbar}
% Note that around the \verb|{equation}| above there should be no spaces (avoided
% in the {\LaTeX} code by \verb|%| lines) so that ``normal'' spacing is used and
% not a new paragraph started.
% \end{leftbar}

% \proglang{R} provides a very flexible implementation of the general GLM
% framework in the function \fct{glm} \citep{Chambers+Hastie:1992} in the
% \pkg{stats} package. Its most important arguments are
% \begin{Code}
% glm(formula, data, subset, na.action, weights, offset,
%   family = gaussian, start = NULL, control = glm.control(...),
%   model = TRUE, y = TRUE, x = FALSE, ...)
% \end{Code}
% where \code{formula} plus \code{data} is the now standard way of specifying
% regression relationships in \proglang{R}/\proglang{S} introduced in
% \cite{Chambers+Hastie:1992}. The remaining arguments in the first line
% (\code{subset}, \code{na.action}, \code{weights}, and \code{offset}) are also
% standard  for setting up formula-based regression models in
% \proglang{R}/\proglang{S}. The arguments in the second line control aspects
% specific to GLMs while the arguments in the last line specify which components
% are returned in the fitted model object (of class \class{glm} which inherits
% from \class{lm}). For further arguments to \fct{glm} (including alternative
% specifications of starting values) see \code{?glm}. For estimating a Poisson
% model \code{family = poisson} has to be specified.

% \begin{leftbar}
% As the synopsis above is a code listing that is not meant to be executed,
% one can use either the dedicated \verb|{Code}| environment or a simple
% \verb|{verbatim}| environment for this. Again, spaces before and after should be
% avoided.

% Finally, there might be a reference to a \verb|{table}| such as
% Table~\ref{tab:overview}. Usually, these are placed at the top of the page
% (\verb|[t!]|), centered (\verb|\centering|), with a caption below the table,
% column headers and captions in sentence style, and if possible avoiding vertical
% lines.
% \end{leftbar}

% \begin{table}[t!]
% \centering
% \begin{tabular}{lllp{7.4cm}}
% \hline
% Type           & Distribution & Method   & Description \\ \hline
% GLM            & Poisson      & ML       & Poisson regression: classical GLM,
%                                            estimated by maximum likelihood (ML) \\
%                &              & Quasi    & ``Quasi-Poisson regression'':
%                                            same mean function, estimated by
%                                            quasi-ML (QML) or equivalently
%                                            generalized estimating equations (GEE),
%                                            inference adjustment via estimated
%                                            dispersion parameter \\
%                &              & Adjusted & ``Adjusted Poisson regression'':
%                                            same mean function, estimated by
%                                            QML/GEE, inference adjustment via
%                                            sandwich covariances\\
%                & NB           & ML       & NB regression: extended GLM,
%                                            estimated by ML including additional
%                                            shape parameter \\ \hline
% Zero-augmented & Poisson      & ML       & Zero-inflated Poisson (ZIP),
%                                            hurdle Poisson \\
%                & NB           & ML       & Zero-inflated NB (ZINB),
%                                            hurdle NB \\ \hline
% \end{tabular}
% \caption{\label{tab:overview} Overview of various count regression models. The
% table is usually placed at the top of the page (\texttt{[t!]}), centered
% (\texttt{centering}), has a caption below the table, column headers and captions
% are in sentence style, and if possible vertical lines should be avoided.}
% \end{table}


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Results on large genomic data} \label{sec:results}

In this section we discuss several applications of our algorithms in
some large genomic data sets. We downloaded the \verb|chipseq| data set from
the UCI machine learning repository \citep{uci-ml-repo}. We considered
4951 data sets ranging in size from $N=10^3$ to $N=10^7$ data points
to segment (lines in the bedGraph file). 

\begin{figure}[t!]
\centering
\begin{minipage}{3.5in}
  \includegraphics[width=\textwidth]{jss-figure-more-likely-models-three-peaks}
\end{minipage}
\begin{minipage}{2.3in}
  \includegraphics[width=\textwidth]{jss-figure-more-likely-models-three-peaks-zoom}
\end{minipage}
\caption{\label{fig:three-peaks} One ChIP-seq data set with three peak
  models. \textbf{Top:} the MACS2 algorithm (a heuristic from the
  bioinformatics literature) computed a sub-optimal model with five
  peaks for these data. \textbf{Middle:} the most likely model with
  five peaks contains one equality constraint between segment means,
  which suggests that there are less than five easily interpretable peaks. \textbf{Bottom:} the most likely model with three peaks is
  also more likely than the MACS2 model, and more interpretable (fewer
  small peaks).}
\end{figure}
 
\subsection{Application: Computing the maximum likelihood model with a
  given number of peaks}

In this section we show that our algorithms can be used to compute the
most likely model for a given number of peaks, which is in particular
more likely and interpretable than the peak model from a heuristic
algorithm. A subset of one data set is shown in
Figure~\ref{fig:three-peaks}, along with three segmentation/peak
models. In the top panel, we show the peak model that results from
running MACS2, a heuristic algorithm from the bioinformatics
literature \citep{MACS}. It detects five peaks, so we ran
Algorithm~\ref{algo:seq-search} with $P^*=5$ on these data in order to
compute the most likely model with at most 5 peaks (shown in middle
panel). It is clear that the optimal 5 peak
model is a better fit in terms of likelihood (as expected); it is also a better fit visually, especially for the peak on the left. Furthermore the optimal 5 peak model actually has one equality
constraint between adjacent segment means, suggesting that there are
less than five easily interpretable peaks. Therefore we also computed
the optimal 3 peak model (bottom panel), which also has a higher
log-likelihood than the 5 peak MACS2 model. Overall it is clear that
our algorithms can be used to compute models which are both more
likely and more interpretable (with fewer peaks) than heuristics such
as MACS2.



% \begin{leftbar}
% For code input and output, the style files provide dedicated environments.
% Either the ``agnostic'' \verb|{CodeInput}| and \verb|{CodeOutput}| can be used
% or, equivalently, the environments \verb|{Sinput}| and \verb|{Soutput}| as
% produced by \fct{Sweave} or \pkg{knitr} when using the \code{render_sweave()}
% hook. Please make sure that all code is properly spaced, e.g., using
% \code{y = a + b * x} and \emph{not} \code{y=a+b*x}. Moreover, code input should
% use ``the usual'' command prompt in the respective software system. For
% \proglang{R} code, the prompt \code{"R> "} should be used with \code{"+  "} as
% the continuation prompt. Generally, comments within the code chunks should be
% avoided -- and made in the regular {\LaTeX} text instead. Finally, empty lines
% before and after code input/output should be avoided (see above).
% \end{leftbar}

\subsection{GFPOP is empirically log-linear}

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models-computation}
\end{minipage}
\caption{\label{fig:intervals} In our empirical
  tests, the computational requirements of the GFPOP algorithm
  were log-linear $O(N \log N)$ in the number of data points $N$ to
  segment. \textbf{Left:} we analyzed the number of intervals $I$
  (candidate changepoints) stored in the $C_t(\mu)$ cost functions,
  because the total time/space complexity is $O(NI)$. We observed
  empirically that the mean number of intervals $I=O(\log N)$ (red
  curve). Even the maximum number of intervals (blue curve) is much
  less than $N$. \textbf{Right:} storage on disk (top panel) and
  computation time (bottom panel) are empirically $O(N \log N)$. Error
  bands show median and 5\%/95\% quantiles over several data sets of a
  given size $N$; black dots and text show computational requirements
  for the most extreme data sets.}
\end{figure}

To measure the empirical time complexity of GFPOP
(Algorithm~\ref{algo:GFPOP}), we ran it on all 4951 genomic data sets,
with a grid of penalty values $\lambda\in(\log N, N)$ for each data
set of size $N$. The overall theoretical time/space complexity is
$O(NI)$, where $I$ is the number of intervals (candidate changepoints)
stored in the $C_{s,t}$ optimal cost functions. During each run we
therefore recorded the mean and max number of intervals over all
$s,t$. We observed that the empirical mean/max number of intervals
increases logarithmically with data set size, $I=O(\log N)$
(Figure~\ref{fig:intervals}, left). Remarkably, for the largest data
set ($N=11,499,958$) the algorithm only computed a mean of $I=19$
intervals. The most intervals computed to represent any single
$C_{s,t}$ function was 512 intervals for one data set with
$N=4,583,432$. 

Since empirically $I=O(\log N)$ in these large genomic data sets, we
expected an overall time/space complexity of $O(N\log N)$. The
empirical measurements of time and space requirements are consistent
with this expectation (Figure~\ref{fig:intervals}, right). For the
largest data sets ($N=10^7$), the algorithm takes only about 80
gigabytes of storage and 1 hour of computation time. Overall these
results suggest that GFPOP can be used to compute optimal peak models
for large genomic data in $O(N\log N)$ space/time.
 
\subsection{Disk storage is slower than memory by a constant
  factor}

In the previous section, we discussed how tens of gigabytes of
storage are required to run GFPOP on large data. Since typical
computers may not have enough memory, our implementation uses
disk-based storage. We compared our disk-based implementation to
another memory-based implementation, in terms of computation time on
small data sets for which GFPOP uses $<1$GB of storage. We observed
that disk storage is slower than memory storage by a constant factor
(1.7--2.3$\times$, Figure~\ref{fig:disk-memory-compare-speed}), which was expected. 

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed-penalty}
\end{minipage}
\caption{\label{fig:disk-memory-compare-speed} The disk-based storage
  method is only a constant factor slower than the memory-based
  method. We benchmarked both methods on several small data sets
  ($N\leq 462,890$) for which optimal models could be computed using
  1GB of storage. \textbf{Left:} computation time is empirically
  $O(N\log N)$ for both storage methods, but the disk-based method is
  slower by a constant factor. Median line and quartile band computed
  over several penalty values for a given data set. \textbf{Right:}
  fixing one data set data set with $N=106,569$, the computation time
  increases with penalty value $\lambda$ for both storage methods.}
\end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-all}
% \caption{\label{fig:target-intervals-models-all} The PeakSegFPOP algorithm
%   was used on the large data sets in the
%   benchmark. Data sizes range from $N=10^2$ to $10^7$ weighted data to
%   segment (x-axis); disk usage (top panel) and computation time
%   (bottom panel) are log-linear $O(N \log N)$.}
% \end{figure}

\subsection{Sequential search is faster than Segment Neighborhood}

In this section we compare the number of $O(N\log N)$ dynamic
programming iterations required for the propsed sequential search
(Algorithm~\ref{algo:seq-search}) and the previous Generalized Pruned
Dynamic Programming Algorithm (GPDPA) of
\citet{Hocking-constrained-changepoint-detection}. Both algorithms
compute the solution to the Segment Neighborhood problem (optimal
model with at most $P$ peaks). The GPDPA requires exactly $2P$ iterations of
dynamic programming, each of which is an $O(N\log N)$ operation. In
contrast, the proposed sequential search
(Algorithm~\ref{algo:seq-search}) needs to solve for a sequence of
penalties, each of which is done via GFPOP in $O(N\log N)$ time. 

For two data sets with $N\approx 10^6$ we therefore recorded the
empirical number of times GFPOP was called by the sequential search
algorithm. We observed that the number of GFPOP calls grows
logarithmically with $P$ (Figure~\ref{fig:variable-peaks}, left). For
a large number of peaks ($P>5$), it is clearly faster to use the
sequential search algorithm (Figure~\ref{fig:variable-peaks},
right). Overall these experiments indicate that the time complexity of
the sequential search in large genomic data is $O(N\log(N)\log(P))$
for $N$ data and $P$ peaks.


\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-variable-peaks}
\begin{minipage}{3in}
  \input{jss-figure-variable-peaks}
\end{minipage}
\begin{minipage}{3in} 
  \input{jss-figure-variable-peaks-zoom}
\end{minipage}
\vskip -0.5cm
\caption{\label{fig:variable-peaks} Comparison of time to compute optimal model with at most $P$ peaks using
  Segment Neighborhood (grey lines) and Optimal Partitioning with proposed sequential search (green
  dots). GFPOP with sequential search
  (Algorithm~\ref{algo:seq-search}) was used to compute optimal models
  with different numbers of peaks $P$, for two data sets with
  $N\approx 10^6$. \textbf{Left:} the number of iterations is
  linear $O(P)$ for Segment Neighborhood (grey line) but empirically
  $O(\log P)$ for Optimal Partitioning with sequential search (green
  dots). \textbf{Right:} Optimal Partitioning is empirically faster
  for computing models with $P>5$ peaks (10 segments); Segment
  Neighborhood is faster for smaller models.}
\end{figure}

\subsection{Application: computing a zero-error peak model}

In this section we study the perfomance of the proposed algorithms in
a typical application. In the UCI \verb|chipseq| data set, there are
labels that indicate subsets of the data with or without peaks. In
this context the labels can be used to compute false positive and
false negative rates for any peak model. For example
Figure~\ref{fig:label-error} shows one data set with six labels and
four peak models computed via GFPOP. Small penalties result in too
many peaks, and large false positive rates. Large penalties result in
too few peaks, and large false negative rates. A range of intermediate
penalties/peaks achieves zero label errors. 
The labels can thus be used to determine an appropriate number of
peaks (with zero errors) for each data set.

\begin{figure}[t!]
\centering
\includegraphics{jss-figure-label-error}
\caption{\label{fig:label-error} Labels are used to compute an error
  rate for each peak model (blue bars), defined as the sum of false
  positive and false negative labels (rectangles with black
  outline). This H3K36me3 ChIP-seq data set has $N=1,254,751$ data to
  segment on a subset of chr12, but in the plot above we show only the
  82,233 data (grey signal) in the region around the labels (colored
  rectangles). The model with penalty=6682 results in 320 peaks, which
  is too many (three false positive labels with more than one peak
  start/end). Conversely, the model with penalty=278653 results in 33
  peaks, which is too few (only two peaks in the plotted region,
  resulting in two false negative labels on the right where there
  should be exactly one peak start/end). The range of penalties between 9586 and
  267277 results in models with between 34 and 236 peaks, and achieves
  zero label errors. }
\end{figure}

More generally, after computing GFPOP models for a range of penalties
for each data set, we computed the label error of each model. For each
data set we computed the min/max peaks that achieves zero label errors
(34/236 in Figure~\ref{fig:label-error}), along with the mean of those
two values, $(34+236)/2=135$. We plot the mean number of peaks that
achieves zero label errors as a function of data set size in
Figure~\ref{fig:data-peaks}. In these data it is clear that models
with $O(\sqrt N)$ peaks achieve zero label errors.

\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-data-peaks}
\input{jss-figure-data-peaks}
\caption{\label{fig:data-peaks} The model with minimal label
  errors has $O(\sqrt{N})$ peaks in a data set of size $N$. For each data set we
  computed peak models with minimal label errors (see
  Figure~\ref{fig:label-error}); we then plot the number of peaks in
  minimal error models as a function of data set size $N$. Black median line
  and grey quartile band computed over several data sets of a given size
  $N$; asymptotic reference lines shown in red.}
\end{figure}
  
Computing the optimal model with $O(\sqrt N)$ peaks is computationally
expensive using Segment Neighborhood algorithms, because the overall
complexity would be $O(N\sqrt N\log N)$. For example in $N=10^7$ data,
$P=O(\sqrt N)=1414$ peaks achieves zero label errors. Computing the
optimal model with Segment Neighborhood would thus require 2828
$O(N\log N)$ DP iterations. If we assume that each iteration would
have similar computational requirements as one $O(N\log N)$ run of
GFPOP, each would require about 1 hour and 80 gigabytes
(Figure~\ref{fig:intervals}). Overall that would mean 220 terabytes of
storage and 17 weeks of computation time, which is much too expensive
in practice.

Instead, we can use the proposed sequential search
(Algorithm~\ref{algo:seq-search}) to compute a zero-error model with
$O(\sqrt N)$ peaks. In our empirical tests, we observed that only
$O(\log N)$ GFPOP calls are required to compute $O(\sqrt N)$ peaks
(Figure~\ref{fig:evaluations}, left). In particular for $N=10^7$ data
only 10--15 GFPOP calls are required, which is significantly fewer
than the 2828 DP iterations that would be required for Segment
Neighborhood solvers. 

We also observed that the empirical timings of the sequential search
are only a log-factor slower than solving for one penalty
(Figure~\ref{fig:evaluations}, right). In particular for $N=10^7$ data
the sequential search takes on the order of hours, which is much less
than the weeks that would be required to solve the Segment
Neighborhood problem. Overall these empirical results indicate that
the sequential search algorithm can be used to compute a zero-error
model with $O(\sqrt N)$ peaks in $O(N(\log N)^2)$ time.

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  %\includegraphics{jss-figure-evaluations}
  \input{jss-figure-evaluations}
\end{minipage} 
\begin{minipage}{3in} 
  %\includegraphics{jss-figure-evaluations-computation}
  \input{jss-figure-evaluations-computation} 
\end{minipage} 
\vskip -0.5cm
\caption{\label{fig:evaluations} Computing a zero-error model with
  $O(\sqrt{N})$ peaks is possible in $O(N(\log N)^2)$ time using our
  proposed Optimal Partitioning Search algorithm. \textbf{Left:}
  Segment Neighborhood requires $O(\sqrt{N})$ dynamic programming
  iterations to compute a model with $O(\sqrt{N})$ peaks; our proposed
  Optimal Partitioning search algorithm requires only $O(\log N)$
  iterations. \textbf{Right:} Optimal Partitioning solves for one
  penalty in $O(N\log N)$ space/time (median line and 5\%/95\% quantile band
  over data sets and penalties); finding the zero-error model with
  $O(\sqrt{N})$ peaks takes $O(N (\log N)^2)$ time/space -- only a log
  factor more (points).}
\end{figure} 

 

% \begin{CodeChunk}
% \begin{CodeInput}
% R> m_pois <- glm(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine,
% +    family = poisson)
% \end{CodeInput}
% \end{CodeChunk}
% %
% To account for potential overdispersion we also consider a negative binomial
% GLM.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> library("MASS")
% R> m_nbin <- glm.nb(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine)
% \end{CodeInput}
% \end{CodeChunk}
% %
% In a comparison with the BIC the latter model is clearly preferred.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> BIC(m_pois, m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
%        df      BIC
% m_pois 18 2046.851
% m_nbin 19 1157.235
% \end{CodeOutput}
% \end{CodeChunk}
% %
% Hence, the full summary of that model is shown below.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> summary(m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
% Call:
% glm.nb(formula = Days ~ (Eth + Sex + Age + Lrn)^2, data = quine, 
%     init.theta = 1.60364105, link = log)

% Deviance Residuals: 
%     Min       1Q   Median       3Q      Max  
% -3.0857  -0.8306  -0.2620   0.4282   2.0898  

% Coefficients: (1 not defined because of singularities)
%             Estimate Std. Error z value Pr(>|z|)    
% (Intercept)  3.00155    0.33709   8.904  < 2e-16 ***
% EthN        -0.24591    0.39135  -0.628  0.52977    
% SexM        -0.77181    0.38021  -2.030  0.04236 *  
% AgeF1       -0.02546    0.41615  -0.061  0.95121    
% AgeF2       -0.54884    0.54393  -1.009  0.31296    
% AgeF3       -0.25735    0.40558  -0.635  0.52574    
% LrnSL        0.38919    0.48421   0.804  0.42153    
% EthN:SexM    0.36240    0.29430   1.231  0.21818    
% EthN:AgeF1  -0.70000    0.43646  -1.604  0.10876    
% EthN:AgeF2  -1.23283    0.42962  -2.870  0.00411 ** 
% EthN:AgeF3   0.04721    0.44883   0.105  0.91622    
% EthN:LrnSL   0.06847    0.34040   0.201  0.84059    
% SexM:AgeF1   0.02257    0.47360   0.048  0.96198    
% SexM:AgeF2   1.55330    0.51325   3.026  0.00247 ** 
% SexM:AgeF3   1.25227    0.45539   2.750  0.00596 ** 
% SexM:LrnSL   0.07187    0.40805   0.176  0.86019    
% AgeF1:LrnSL -0.43101    0.47948  -0.899  0.36870    
% AgeF2:LrnSL  0.52074    0.48567   1.072  0.28363    
% AgeF3:LrnSL       NA         NA      NA       NA    
% ---
% Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

% (Dispersion parameter for Negative Binomial(1.6036) family taken to be 1)

%     Null deviance: 235.23  on 145  degrees of freedom
% Residual deviance: 167.53  on 128  degrees of freedom
% AIC: 1100.5

% Number of Fisher Scoring iterations: 1


%               Theta:  1.604 
%           Std. Err.:  0.214 

%  2 x log-likelihood:  -1062.546 
% \end{CodeOutput}
% \end{CodeChunk}


% \begin{figure}[t!] 
% \centering
% \includegraphics{jss-figure-more-likely-models-one-peak}
% \caption{\label{fig:one-peak} Default peak model from the baseline
%   MACS2 algorithm (top) and the optimal peak model computed using the
%   proposed PeakSegFPOP algorithm (bottom).}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-penalty}
% \caption{\label{fig:target-intervals-models-penalty} The PeakSegFPOP
%   algorithm was used to compute optimal models for six of the largest
%   problems (panels from left to right) for a variety of penalty
%   parameters (x-axis). Y-axes show empirical measurements of cost
%   function pieces (intervals, top panel), timings (minutes, middle
%   panel), and disk usage (gigabytes, bottom panel).}
% \end{figure}


%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and discussion} \label{sec:summary} 

This paper presented two new algorithms for constrained optimal
changepoint detection. We presented Generalized Functional
Partitioning Optimal Partitioning (GFPOP) which computes the optimal
model for one penalty $\lambda$. We also proposed a sequential search
algorithm which repeated calls GFPOP in order to compute the most
likely model with at most $P$ peaks. 

We analyzed the proposed algorithms by running them on a set of
genomic data sets ranging from $N=10^3$ to $N=10^7$. First, we showed
that the algorithms can be used to compute models which are more
likely than existing heuristics, and often more interpretable (fewer
peaks). 

Second, we studied the empirical complexity of GFPOP as the a function
of the number of data $N$. We showed that GFPOP requires $O(N\log N)$
time, $O(N \log N)$ space, and $O(\log N)$ memory. We furthermore
showed that using disk-based storage is only a constant factor slower
than memory-based storage. Overall we showed that GFPOP can be used to
compute optimal peak models for up to $N=10^7$ data in reasonable
amounts of time (minutes). 

Third, we studied the empirical complexity of sequential search as a
function of data size $N$ and number of peaks $P$. We showed that it
requires $O(N\log(N)\log(P))$ time, $O(N\log N)$ disk, $O(\log N)$
memory. In particular we showed that it is always faster than Segment
Neighborhood solvers for models with $P>5$ peaks. 

Finally we analyzed the labels in our benchmark of large genomic data,
which indicated that an appropriate number of peaks is $P=O(\sqrt
N)$. We showed that sequential search computes the model with
$O(\sqrt N)$ peaks in $O(N(\log N)^2)$ time, whereas existing Segment
Neighborhood algorithms would be $O(N\sqrt N\log N)$. We showed that
for $N=10^7$ data our approach only requires hours/gigabytes of
time/space to compute optimal models. Our algorithms thus make it
practical for the first time to compute optimal models with many peaks
for large genomic data sets.




%% -- Optional special unnumbered sections -------------------------------------




% \begin{leftbar} 
% If necessary or useful, information about certain computational details
% such as version numbers, operating systems, or compilers could be included
% in an unnumbered section. Also, auxiliary packages (say, for visualizations,
% maps, tables, \dots) that are not cited in the main text can be credited here.
% \end{leftbar}

% The results in this paper were obtained using
% \proglang{R}~3.4.1 with the
% \pkg{MASS}~7.3.47 package. \proglang{R} itself
% and all packages used are available from the Comprehensive
% \proglang{R} Archive Network (CRAN) at
% \url{https://CRAN.R-project.org/}.


%\section*{Acknowledgments}

% \begin{leftbar}
% All acknowledgments (note the AE spelling) should be collected in this
% unnumbered section before the references. It may contain the usual information
% about funding and feedback from colleagues/reviewers/etc. Furthermore,
% information such as relative contributions of the authors may be added here
% (if any).
% \end{leftbar}


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{jss-refs}
 

% %% -- Appendix (if any) --------------------------------------------------------
% %% - After the bibliography with page break.
% %% - With proper section titles and _not_ just "Appendix".

% \newpage

% \begin{appendix}

% \section{More technical details} \label{app:technical}

% \begin{leftbar}
% Appendices can be included after the bibliography (with a page break). Each
% section within the appendix should have a proper section title (rather than
% just \emph{Appendix}).

% For more technical style details, please check out JSS's style FAQ at
% \url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
% which includes the following topics:
% \begin{itemize}
%   \item Title vs.\ sentence case.
%   \item Graphics formatting.
%   \item Naming conventions.
%   \item Turning JSS manuscripts into \proglang{R} package vignettes.
%   \item Trouble shooting.
%   \item Many other potentially helpful details\dots
% \end{itemize}
% \end{leftbar}


% \section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

% \begin{leftbar}
% References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
% references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
% \verb|\citealp| etc.\ (and never hard-coded). This commands yield different
% formats of author-year citations and allow to include additional details (e.g.,
% pages, chapters, \dots) in brackets. In case you are not familiar with these
% commands see the JSS style FAQ for details.

% Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
% when acquiring the entries automatically from mixed online sources. However,
% it is important that informations are complete and presented in a consistent
% style to avoid confusions. JSS requires the following format.
% \begin{itemize}
%   \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
%     be used in the references.
%   \item Titles should be in title case.
%   \item Journal titles should not be abbreviated and in title case.
%   \item DOIs should be included where available.
%   \item Software should be properly cited as well. For \proglang{R} packages
%     \code{citation("pkgname")} typically provides a good starting point.
% \end{itemize}
% \end{leftbar}

% \end{appendix}

% %% -----------------------------------------------------------------------------


\end{document}
 
