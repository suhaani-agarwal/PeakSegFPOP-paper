\documentclass{article}
\usepackage{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amssymb,amsmath}
%\usepackage{natbib}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}

\begin{document}

\title{A linear time algorithm for peak detection using constrained
  optimal segmentation}

\author{
  Toby Dylan Hocking\\
  Department of Human Genetics\\
  McGill University\\
  Montreal, QC H2R-2G9 Canada \\
  \texttt{toby.hocking@mail.mcgill.ca} \\
  %% examples of more authors
  \And
  Guillem Rigaill \\
  University of Evry \\
  Evry, France \\
  \texttt{guillem.rigaill@evry.fr} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\maketitle

\begin{abstract}
  Change-point detection is a central problem in time series and
  genomic data sets. In several kinds of data it is desirable to
  constrain the possible change-points to obtain a more interpretable
  model. We propose a new constrained Pruned Dynamic Programming
  Algorithm (cPDPA) which recovers the optimal change-points subject
  to affine constraints on adjacent segment means. We use this
  algorithm for isotonic regression and peak detection.
\end{abstract}

\section{Introduction}

Change-point detection is a central problem in many fields. TODO ref
isotonic regression and PAVA.

An algorithm with quadratic time complexity was proposed to solve the
peak detection problem \citep{PeakSeg}. 

\citet{pruned-dp} proposed a Pruned Dynamic Programming Algorithm
(PDPA) to exactly solve the unconstrained segmentation problem.

\subsection{Contributions}

The main contribution of this paper is a new Constrained Pruned
Dynamic Programming Algorithm (cPDPA) which is guaranteed to recover
the exact solution for a class of constrained optimal segmentation
problems. 

\section{Related work}
\label{sec:related}

\subsection{Maximum likelihood segmentation models}

The model we consider in this paper is a constrained version of the
optimal segmentation model \citep{TODO_MORE_REFS,Segmentor}. The
unconstrained model can be computed using a dynamic programming
algorithm (DPA) \citep{bellman}, or a pruned dynamic programming
algorithm (pDPA) \citep{pruned-dp}. Both algorithms are guaranteed to
recover the exact solution to the unconstrained model, but there are
two important differences. The pDPA is more complicated to implement,
but is also computationally faster than the DPA. For segmenting a
sequence of $d$ data points, the pDPA takes on average $O(d\log d)$
time whereas the DPA takes $O(d^2)$ time.

The constraints that we consider in this paper are a generalization of
the peak detection model \citep{PeakSeg} and the isotonic regression
model \citep{isotonic}. Rather than searching all possible
change-points to find the most likely model with $s$ segments, we
propose to constrain the possible change-points so that the segment
means may be more easily interpreted.

\section{From unconstrained to constrained maximum likelihoood
  segmentation}
\label{sec:model}

In this section we first discuss the existing unconstrained maximum
likelihood model, and then we discuss a more general framework for
constrained maximum likle

\subsection{Unconstrained maximum likelihood segmentation}

For a sequence of $d$ data points $\mathbf y\in\ZZ_+^d$ to segment, we
fix a maximum number of segments $ s_{\text{\text{max}}}\leq d$. The
unconstrained maximum likelihood segmentation model is defined as the
most likely mean vector $\mathbf m\in\RR^d$ with $s\in\{1, 2, \dots,
s_{\max}\}$ piecewise constant segments:
\begin{align}
  \label{unconstrained}
  \mathbf{\hat m}^s(\mathbf y)  =\ 
  &\argmin_{\mathbf m\in\RR^{d}} && 
  \ell
  %\tag{\textbf{Unconstrained}}
  (\mathbf m, \mathbf y) \\
  &\text{such that} && \Segments(\mathbf m)=s,
  \nonumber
\end{align}
where the loss function is
\begin{equation}\label{eq:loss}
  \ell(\mathbf m, \mathbf y)= \sum_{j=1}^d m_j - y_j \log m_j.
\end{equation} 
The model complexity is the number of piecewise constant segments
\begin{equation}
  \Segments(\mathbf m)=1+\sum_{j=2}^d I(m_j \neq m_{j-1}),
\end{equation}
where $I$ is the indicator function. 

Although it is a non-convex optimization problem, the sequence of
segmentations $\mathbf{\hat m}^1(\mathbf y), \dots, \mathbf{\hat
  m}^{s_{\text{\text{max}}}}(\mathbf y)$ can be computed in
$O(s_{\text{\text{max}}} d^2)$ time using dynamic programming
\citep{bellman}, or in $O(s_{\text{\text{max}}} d \log d)$
time using pruned dynamic programming \citep{pruned-dp, Segmentor}.

We refer to (\ref{unconstrained}) as the ``unconstrained'' model
since $\mathbf{\hat m}^s(\mathbf y)$ is the most likely segmentation
of all possible models with $s$ piecewise constant segments ($s-1$
change-points). Several unconstrained models are shown on the left of
Figure~\ref{fig:Segmentor-PeakSeg}, and for example the second segment
of the model with $s=3$ segments appears to capture the peak in the
data.
% In general, we would like to use the 2nd, 4th,
% ... segments as peaks, and the 1st, 3rd, ... segments as
% background. 
To construct a peak detector $c$, we first define the peak indicator at base
$j\in\{2, \dots, d\}$ as
\begin{equation}
  \label{eq:peaks}
  P_j(\mathbf m) = \sum_{k=2}^j \sign( m_{k} - m_{k-1} ),
\end{equation}
where $P_1(\mathbf m)=0$ by convention. $P_j(\mathbf m)$ is the
cumulative sum of signs of changes up to point $j$ in the piecewise
constant vector $\mathbf m$. We define the vector of peak indicators
as
\begin{equation}
  \mathbf
P[\mathbf m] = \left[\begin{array}{ccc} P_1(\mathbf m) & \cdots &
    P_d(\mathbf m)
\end{array}\right].
\end{equation}

\subsection{PeakSeg: constrained maximum likelihood}
\label{sec:constrained}

In general for the unconstrained model $P_j(\mathbf m)\in\ZZ$, which
is problematic since we want to use it as a peak detector with binary
outputs $P_j(\mathbf m)\in \{0, 1\}$. 

For example, if $\mathbf m = \left[\begin{array}{ccccccc}1.1 &
    1.1 & 2 & 2 & 4 & 4 & 3\end{array}\right]$, with two changes up
followed by one change down, then $\mathbf P(\mathbf m) =
\left[\begin{array}{ccccccc}0 & 0 & 1 & 1 & 2 & 2 &
    1 \end{array}\right]$.

% This is also a practical problem that can be observed in real data
% sets. For example in Figure~\ref{fig:Segmentor-PeakSeg} there is a position $j$ for
% which $P_j\left[ \mathbf{\hat m}^5(\mathbf y) \right]=2$ (since the
% mean changes up, up, down, down). 

Thus we constrain the peak indicator $P_j(\mathbf
m)\in\{0, 1\}$, which results
in the constrained problem
\begin{align*}
  \label{PeakSeg}
  \mathbf{\tilde m}^s(\mathbf y)  =
    \argmin_{\mathbf m\in\RR^{d}} &\ \ 
    \rho(\mathbf m, \mathbf y) 
    \tag{\textbf{PeakSeg}}
\\
    \text{such that} &\ \  \Segments(\mathbf m)=s,  \\
     \forall j\in\{1, \dots, d\}, &\ \ P_j(\mathbf m) \in\{0, 1\}.
\end{align*}
Note that one must specify the number of segments $s$ or,
equivalently, the number of peaks $p=(s-1)/2$. Another way to
interpret the constrained \ref{PeakSeg} problem is that the sequence
of changes in the segment means $\mathbf m$ must begin with a positive
change and then alternate: up, down, up, down, ... (and not up, up,
down). Thus the even-numbered segments may be interpreted as peaks
$P_j(\mathbf m)=1$, and the odd-numbered segments may be interpreted
as background $P_j(\mathbf m)=0$.

For example, the good peaks in Figure~\ref{fig:good-bad} are the
second and fourth segments of the \ref{PeakSeg} solution for $s=5$
segments.

Figure~\ref{fig:Segmentor-PeakSeg} shows a profile where the constraint is
necessary for the even-numbered segments to be interpreted as
peaks. In particular, it is clear that unconstrained models with
$s\in\{5, 7\}$ segments do not satisfy $P_j[\mathbf{\hat m}^s(\mathbf
y)]\in\{0, 1\}$ for all positions $j\in\{1,\dots, d\}$ (since they
have up, up, down changes).

\section{Algorithm}

\subsection{Constrained PeakSeg model}

For the Segment Neighborhood algorithm we begin as usual by computing
a functional representation of the optimal cost in 1 segment up to
base $b$. 
\begin{equation*}
  \label{eq:C1b}
  C_{1,b}(\mu) = \sum_{i=1}^b \gamma_b(\mu),
\end{equation*}
where $\gamma_b(\mu)$ is the cost of using the mean $\mu$ for single
data point $b$ (for example the Gaussian or Poisson loss).

Next we define the minimum cost in 2 segments up to data point 2 as
\begin{equation*}
  \label{eq:C22}
  C_{2,2}(\mu) = C_{1,1}^{\leq}(\mu) + \gamma_2(\mu),
\end{equation*}
where for a function $f:\RR\rightarrow\RR$ the min-less operator
yields another function $f\leq:\RR\rightarrow\RR$ such that
\begin{equation}
  \label{eq:min-less}
  f^{\leq}(\mu) = \min_{x\leq \mu} f(x).
\end{equation}
The algorithm relies on the ability to compute an exact representation
of functions such as $C_{1,1}^{\leq}$. Since the cost functions $f$
are quasiconvex, we can easily find the minimum $\mu^*$, and then
compute the following exact representation
\begin{equation*}
  f^\leq(\mu)
  \begin{cases}
    f(\mu^*) & \text{ if } \mu \geq \mu^*,\\
    f(\mu) & \text{ otherwise.}
  \end{cases}
\end{equation*}

\begin{figure}[!t]
  \parbox{3in}{
    \begin{center}
    \input{figure-1-min-more-operator}
    \end{center}
  }
  \parbox{3in}{
    \begin{center}
      \input{figure-1-min-less-operator}
    \end{center}
  }
  \caption{\label{fig:min-operators} \textbf{Left:} The min-more
    operator is $C^{\geq}(\mu)=\min_{x\geq \mu}C(x)$. \textbf{Right:}
    The min-less operator is $C^{\leq}(\mu)=\min_{x\leq
      \mu}C(x)$.}
\end{figure}

The next step is to compute the minimum cost in 2 segments up to data
point 3, for which there is a choice of two change-points.
\begin{equation*}
  C_{2,3}(\mu) = \min
  \begin{cases}
    C_{2,2}(\mu)+\gamma_3(\mu), \\
    C_{1,2}^{\leq}(\mu)+\gamma_3(\mu)
  \end{cases}
\end{equation*}
We have already computed an exact representation of the $C_{2,2}$
term, which is the cost a change after the first data point. Now we
need to compare it with the $C_{1,2}^{\leq}$ term, which is the cost
of a change after the second data point. This is a crucial step in
which the \texttt{MinEnvelope} sub-routine computes an exact
representation of the minimum of these two functions.

\begin{figure}[!t]
  \begin{center}
    \input{figure-2-min-envelope}
  \end{center}
  \caption{\label{fig:min-envelope} The cost $C_{s,t}$ in $s$ segments
    up to $t$ data points is computed using the min envelope
    $M_{s,t-1}$. \textbf{Left:} the min envelope for $s=3$ segments up
    to data point $t=34$ is the minimum of two functions:
    $C^{\geq}_{2,34}$ is the cost if the second segment ends at data
    point $t=34$, and $C_{3,34}$ is the cost if the second segment
    ends before that. \textbf{Middle:} the optimal cost for $s=3$
    segments up to data point $t=35$ is the sum of the min envelope
    $M_{3,34}$ and the cost of the next data point
    $\gamma_{35}$. \textbf{Right:} in the next step, the
    algorithm prunes all previously considered change-points (cost
    $C_{3,35}$), and only considers the model with a the second segment
    ending at data point $t=35$ (cost $C^{\geq}_{2,35}$).}
\end{figure}

The updates continue for every data point $b\in\{3, ..., B\}$
\begin{equation*}
  C_{2,b}(\mu) = \min
  \begin{cases}
    C_{2,b-1}(\mu) + \gamma_b(\mu),\\
    C_{1,b-1}^{\leq}(\mu) + \gamma_b(\mu).
  \end{cases}
\end{equation*}

For the third segment we first compute the minimum cost up to data point 3
\begin{equation*}
  C_{3,3}(\mu) = C_{2,2}^{\geq}(\mu) + \gamma_3(\mu),
\end{equation*}
where the more-min operator $f^\geq$ is defined analogously. The
update formula for the minimum cost up to data point
$b\in\{4, ..., B\}$ is
\begin{equation*}
  C_{3,b}(\mu) = \min
  \begin{cases}
    C_{3,b-1}^{\geq}(\mu)+\gamma_b(\mu),\\
    C_{2,b-1}^{\geq}(\mu)+\gamma_b(\mu)
  \end{cases}
\end{equation*}
In general for $s$ segments, we use
\begin{equation}
  C_{s,s}(\mu) = C_{s-1,s-1}^{*}(\mu) + \gamma_s(\mu),
\end{equation}
and for $b\in\{s+1, ..., B\}$
\begin{equation}
  C_{s,b}(\mu) = \min
  \begin{cases}
    C_{s,b-1}^{*}(\mu)+\gamma_b(\mu),\\
    C_{s-1,b-1}^{*}(\mu)+\gamma_b(\mu),
  \end{cases}
\end{equation}
where * means less-min for even-numbered segments $s$, and more-min
for odd-numbered segments.

\subsection{Optimal Partitioning}

\section{Figures}

\subsection{Simple data set analysis}

In this section we analyze the simple data
$\mathbf z = \left[\begin{array}{cccc} 1 & 10 & 14 & 13
\end{array}\right]\in\ZZ_+^4
$. For $s=3$ segments there are only 3 possible segmentations:
$[1][10][14, 13]$, $[1][10, 14][13]$ and $[1, 10][14][13]$. If we use
max-likelihood estimates for each segment mean, then only the last
segmentation obeys the \ref{PenPeakSeg} model constraints. However the
constrained Dynamic Programming Algorithm of \citet{PeakSeg} does not
recover it. However this last segmentation is more costly than a
simpler segmentation with a single change after the first data point
(see figure below).

The figure above plots the cost of a segmentation in two segments, up
to data point 3. There are two possible change-points: after 1 [1][10
14], and after 2 [1 10][14]. The minimum cost of a change after 2 is
40.5, but the other change is less costly for those mean values (5.5,
14). Thus it is clear that although the segmentation [1 10][14][13] is
feasible for the PeakSeg problem, it is clearly not optimal.

%%%% update rules
\newcommand{\FCC}{\widetilde{C}}
\newcommand{\MK}{\mathcal{M}^k}
\section{Constrained DP Problem}
\subsection{Some definition}

We aim at optimizing over all possible segmentations $m$ in $\MK_n$
described as a set of segments $\{s_1, ... s_K\}$ the quantity
$\sum_{k \in [1..K]} \sum_{i \in s_{k}} (y_i - \mu_{k})^2$ subject to
the following $K-1$ linear constraints. For every
$k\in\{1, \dots, K-1\}$ we have
\begin{equation}
a_{kk} \mu_1 + a_{k,k+1} \mu_2 + b_k \geq 0.
\end{equation}
Each of these constraint imply only 

\subsection{Example}
Isotonic regression

Up-Down model

\subsection{Fonctional cost representation}

We will consider the optimal fonctionnal cost up to in $k$ which is 
\begin{equation*}
\begin{aligned}
& \FCC^k_t(\mu) = \underset{x}{\text{minimize}} 
& & \sum_{r \in [1..k]} \sum_{i \in r} (x_i - \mu_r)^2 \\
& \text{subject to}
& & \mu_{2i} \geq \mu_{2i+1} \ \text{and} \ \mu_{2i} \geq \mu_{2i-1}
\end{aligned}
\end{equation*}
$\FCC^k_t(\mu) = min $
If

\subsection{Update rule}
%%%%

\section{Timings}

TODO: plot the number of intervals over time.

TODO: show that the time complexity is empirically linear in the
number of data points.

\section{Comparison with two-state HMM}

TODO: first run two-state HMM, which will assign a certain number of
peaks. Compute the Poisson loss for that model, and compare it to the
optimal Poisson loss recovered by the PDPA.

\includegraphics[width=\textwidth]{figure-NA-timings}

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}

