\documentclass{article}
\usepackage{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amssymb,amsmath}
%\usepackage{natbib}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}

\begin{document}

\title{A linear time algorithm for peak detection using constrained
  optimal segmentation}

\author{
  Toby Dylan Hocking\\
  Department of Human Genetics\\
  McGill University\\
  Montreal, QC H2R-2G9 Canada \\
  \texttt{toby.hocking@mail.mcgill.ca} \\
  %% examples of more authors
  \And
  Guillem Rigaill \\
  University of Evry \\
  Evry, France \\
  \texttt{guillem.rigaill@evry.fr} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\maketitle

\begin{abstract}
  Change-point detection is a central problem in time series and
  genomic data sets. We consider the PeakSeg constrained optimal
  segmentation model, for which the only previous solver is heuristic
  and quadratic time complexity. We propose a new Pruned Dynamic
  Programming Algorithm (PDPA) which recovers the exact solution in
  linear time. 
\end{abstract}

\section{Introduction}

\subsection{Peak detection in ChIP-seq data}

Chromatin immunoprecipitation sequencing (ChIP-seq) is a biological
experiment for genome-wide profiling of histone modifications and
transcription factor binding sites, with many experimental and
computational steps \citep{practical}. Briefly, each experiment yields
a set of sequence reads which are aligned to a reference genome, and
then the number of aligned reads are counted at each genomic position
(Figure~\ref{fig:good-bad}). Although these read counts can be
interpreted as quantitative data, they are most often interpreted
using one of the many available peak detection algorithms
\citep{evaluation2010, rye2010manually, chip-seq-bench}. A peak
detection algorithm is a binary classifier for each genomic
position. The positive class is enriched (peaks) and the negative
class is background noise. Importantly, peaks and background occur in
long contiguous segments across the genome.

More concretely, a single ChIP-seq profile on a genomic region with
$d$ base pairs can be represented as a vector $\mathbf y= \left[
  \begin{array}{ccc}
    y_1 & \cdots & y_d
  \end{array}
\right]\in\ZZ_+^d$ of counts of aligned sequence reads. A peak
detection algorithm is a function $c:\ZZ_+^d
\rightarrow \{0, 1\}^d$ which returns 0 for background noise and 1 for
a peak. 

\citet{pruned-dp} proposed a Pruned Dynamic Programming Algorithm
(PDPA) to exactly solve the penalized segmentation problem. In this
paper we use the FPOP algorithm to solve the penalized version of the
PeakSeg problem \citep{PeakSeg}.

\subsection{Contributions 
%and organization
}

The main contribution of this paper is a new Constrained Pruned
Dynamic Programming Algorithm (cPDPA) which is guaranteed to solve the
PeakSeg optimal segmentation problem. a peak detection model that can
be trained using supervised learning, and is effective for several
pattern types (sharp and broad peaks). Rather than taking unsupervised
model parameters for granted, we propose to train model parameters
using labeled data of the same pattern type. Importantly, and in
contrast to existing unsupervised approaches which can only be trained
using grid search, we propose efficient discrete and convex
optimization algorithms for model training. Our method is the first
peak detector with an efficient supervised learning algorithm, and the
first method that achieves state-of-the-art peak detection across
several patterns in the benchmark of \citet{hocking2014visual}.

A second contribution of this paper is a detailed study of the peak
detection accuracy of several unsupervised and supervised penalty
function learning methods. The two main results are that the oracle
penalty of \citet{cleynen2013segmentation} is more accurate than
asymptotic penalties like the AIC/BIC, and that supervised penalty
learning can be used to further increase peak detection accuracy.

\section{Related work}
\label{sec:related}

Our work is based on and inspired by roughly three types of related
work:
%Previous work can be divided into roughly three categories:
unsupervised ChIP-seq peak detectors from the bioinformatics
literature, maximum likelihood segmentation models, 
and criteria for model selection.

\subsection{Unsupervised ChIP-seq peak detectors}

There are literally dozens of unsupervised algorithms for peak
detection in ChIP-seq data sets, and the bioinformatics literature
contains several published comparison studies \citep{evaluation2010,
  rye2010manually, chip-seq-bench}. \citet{hocking2014visual} proposed
a benchmark of labeled ChIP-seq data sets, with two different histone
mark types: H3K4me3 (sharp peak pattern) and H3K36me3 (broad peak
pattern). The best peak detection algorithm in these H3K4me3 data was
macs \citep{MACS}, and the best for H3K36me3 was HMCan
\citep{HMCan}. Both of these algorithms are unsupervised, but were
calibrated via grid search using the annotated region labels to choose
the best scalar significance threshold hyperparameter.  Others have
proposed to train the hyperparameters of unsupervised peak detectors
\citep{picking2012, DFilter}. In contrast, we propose the supervised
\ref{PeakSeg} model which can be efficiently trained using discrete
and convex optimization algorithms (Sections~\ref{sec:algorithms} and
\ref{sec:penalty-learning}).

\subsection{Maximum likelihood segmentation models}

The \ref{PeakSeg} model we propose in this paper is a constrained
version of the Poisson segmentation model of \citet{Segmentor}.
Their unconstrained model can be computed using a dynamic programming
algorithm (DPA) \citep{bellman}, or a pruned dynamic programming
algorithm (pDPA) \citep{pruned-dp}. Both algorithms are guaranteed to
recover the exact solution to the unconstrained model, but there are
two important differences. The pDPA is more complicated to implement,
but is also computationally faster than the DPA. For segmenting a
sequence of $d$ data points, the pDPA takes on average $O(d\log d)$
time whereas the DPA takes $O(d^2)$ time. 

Relative to the unconstrained segmentation model of \citet{Segmentor},
our proposed \ref{PeakSeg} model has an additional constraint. Rather
than searching all possible change-points to find the most likely
model with $s$ segments, we propose to constrain the possible
change-points to the subset of models that can be interpreted as peaks
(with segment means that change up, down, up, etc, see
Section~\ref{sec:constrained}).

Due to the simplicity of its implementation, we propose a constrained
dynamic programming algorithm (cDPA) that requires a small
modification to the standard DPA solver. Although the DPA is an exact
solver for the unconstrained problem, we show that the cDPA is a
heuristic that is not guaranteed to solve the constrained
problem (Section~\ref{sec:dp-fails}). However, we show that it is
still very useful and accurate in practice on real data
(Section~\ref{sec:results}).


\section{Problem setting}

\subsection{Unconstrained model}

We have a genomic data set $\mathbf{z}\in\ZZ_+^B$ of counts on $B$
bases. The unconstrained problem for a positive penalty parameter
$\lambda\in\RR_+$ is
\begin{equation}
  \label{unconstrained}
  \mathbf{\hat m}^\lambda(\mathbf z)  =\ 
  \argmin_{\mathbf m\in\RR^{B}}\ 
  \rho
  %\tag{\textbf{Unconstrained}}
  (\mathbf m, \mathbf z) 
  +\lambda\Changes(\mathbf m),
\end{equation}
where the Poisson loss function is
\begin{equation}\label{eq:rho}
  \rho(\mathbf m, \mathbf z)= \sum_{b=1}^B m_b - z_b \log m_b.
\end{equation} 
The model complexity is the number of change-points
\begin{equation}
  \Changes(\mathbf m)=\sum_{b=2}^B I(m_{b-1} \neq m_b),
\end{equation}
where $I$ is the indicator function.

Although it is a non-convex optimization problem, the segmentation
$\mathbf{\hat m}^\lambda(\mathbf z)$ can be computed in linear $O(B)$
time using the FPOP algorithm \citep{FPOP}.

We refer to (\ref{unconstrained}) as the ``unconstrained'' model since
$\mathbf{\hat m}^\lambda(\mathbf z)$ can have any sequence of changes
up or down. However for the purposes of peak detection, we are only
interested in segmentations with alternating changes that can be
interpreted as peaks and background \citep{PeakSeg}.

More concretely, we first define the peak indicator at base
$b\in\{2, \dots, B\}$ as
\begin{equation}
  \label{eq:peaks}
  P_b(\mathbf m) = \sum_{j=2}^b \sign( m_{j} - m_{j-1} ),
\end{equation}
where $P_1(\mathbf m)=0$ by convention. $P_b(\mathbf m)$ is the
cumulative sum of signs of changes up to point $b$ in the piecewise
constant vector $\mathbf m$. We define the vector of peak indicators
as
\begin{equation}
  \mathbf
  P[\mathbf m] = \left[
    \begin{array}{ccc}
      P_1(\mathbf m) & \cdots & P_B(\mathbf m)
    \end{array}\right].
\end{equation}

\subsection{PenPeakSeg: penalized constrained segmentation}
\label{sec:constrained}

In general for the unconstrained model $P_b(\mathbf m)\in\ZZ$, which
is problematic since we want to use it as a peak detector with binary
outputs $P_b(\mathbf m)\in \{0, 1\}$. 
For example, if $\mathbf m = \left[\begin{array}{ccccccc}1.1 &
    1.1 & 2 & 2 & 4 & 4 & 3\end{array}\right]$, with two changes up
followed by one change down, then $\mathbf P(\mathbf m) =
\left[\begin{array}{ccccccc}0 & 0 & 1 & 1 & 2 & 2 &
    1 \end{array}\right]$.
Thus we constrain the peak indicator $P_b(\mathbf m)\in\{0, 1\}$,
which results in the constrained problem
\begin{align*}
  \label{PenPeakSeg}
  \mathbf{\tilde m}^\lambda(\mathbf z)  =
  \argmin_{\mathbf m\in\RR^{B}} &\ \ 
    \rho(\mathbf m, \mathbf z) + \lambda\Peaks(\mathbf m)
    \tag{\textbf{PenPeakSeg}}
  \\
  \text{such that} &\ \forall b\in\{1, \dots, B\},
                     \ \ P_b(\mathbf m) \in\{0, 1\}.
\end{align*}
The Peaks model complexity term only counts up changes:
\begin{equation}
  \Peaks(\mathbf m) = \sum_{b=2}^B I(m_{b-1} < m_b).
\end{equation}
The constraint in the \ref{PenPeakSeg} problem forces the sequence of
changes in the segment means $\mathbf m$ to begin with a positive
change and then alternate: up, down, up, down, ... (and not up, up,
down). Thus the even-numbered segments may be interpreted as peaks
$P_b(\mathbf m)=1$, and the odd-numbered segments may be interpreted
as background $P_b(\mathbf m)=0$.

\section{Algorithm}

\subsection{Segment Neighborhood}

For the Segment Neighborhood algorithm we begin as usual by computing
a functional representation of the optimal cost in 1 segment up to
base $b$. 
\begin{equation*}
  \label{eq:C1b}
  C_{1,b}(\mu) = \sum_{i=1}^b \gamma_b(\mu),
\end{equation*}
where $\gamma_b(\mu)$ is the cost of using the mean $\mu$ for single
data point $b$ (for example the Gaussian or Poisson loss).

Next we define the minimum cost in 2 segments up to data point 2 as
\begin{equation*}
  \label{eq:C22}
  C_{2,2}(\mu) = C_{1,1}^{\leq}(\mu) + \gamma_2(\mu),
\end{equation*}
where for a function $f:\RR\rightarrow\RR$ the min-less operator
yields another function $f\leq:\RR\rightarrow\RR$ such that
\begin{equation}
  \label{eq:min-less}
  f^{\leq}(\mu) = \min_{x\leq \mu} f(x).
\end{equation}
The algorithm relies on the ability to compute an exact representation
of functions such as $C_{1,1}^{\leq}$. Since the cost functions $f$
are quasiconvex, we can easily find the minimum $\mu^*$, and then
compute the following exact representation
\begin{equation*}
  f^\leq(\mu)
  \begin{cases}
    f(\mu^*) & \text{ if } \mu \geq \mu^*,\\
    f(\mu) & \text{ otherwise.}
  \end{cases}
\end{equation*}

\begin{figure}[!t]
  \parbox{3in}{
    \begin{center}
    \input{figure-1-min-more-operator}
    \end{center}
  }
  \parbox{3in}{
    \begin{center}
      \input{figure-1-min-less-operator}
    \end{center}
  }
  \caption{\label{fig:min-operators} \textbf{Left:} The min-more
    operator is $C^{\geq}(\mu)=\min_{x\geq \mu}C(x)$. \textbf{Right:}
    The min-less operator is $C^{\leq}(\mu)=\min_{x\leq
      \mu}C(x)$.}
\end{figure}

The next step is to compute the minimum cost in 2 segments up to data
point 3, for which there is a choice of two change-points.
\begin{equation*}
  C_{2,3}(\mu) = \min
  \begin{cases}
    C_{2,2}(\mu)+\gamma_3(\mu), \\
    C_{1,2}^{\leq}(\mu)+\gamma_3(\mu)
  \end{cases}
\end{equation*}
We have already computed an exact representation of the $C_{2,2}$
term, which is the cost a change after the first data point. Now we
need to compare it with the $C_{1,2}^{\leq}$ term, which is the cost
of a change after the second data point. This is a crucial step in
which the \texttt{MinEnvelope} sub-routine computes an exact
representation of the minimum of these two functions.

\begin{figure}[!t]
  \begin{center}
    \input{figure-2-min-envelope}
  \end{center}
  \caption{\label{fig:min-envelope} The cost $C_{s,t}$ in $s$ segments
    up to $t$ data points is computed using the min envelope
    $M_{s,t-1}$. \textbf{Left:} the min envelope for $s=3$ segments up
    to data point $t=34$ is the minimum of two functions:
    $C^{\geq}_{2,34}$ is the cost if the second segment ends at data
    point $t=34$, and $C_{3,34}$ is the cost if the second segment
    ends before that. \textbf{Middle:} the optimal cost for $s=3$
    segments up to data point $t=35$ is the sum of the min envelope
    $M_{3,34}$ and the cost of the next data point
    $\gamma_{35}$. \textbf{Right:} in the next pruning step, the
    algorithm prunes all previously considered change-points (cost
    $C_{3,35}$), and only considers the model with a the second segment
    ending at data point $t=35$ (cost $C^{\geq}_{2,35}$).}
\end{figure}

The updates continue for every data point $b\in\{3, ..., B\}$
\begin{equation*}
  C_{2,b}(\mu) = \min
  \begin{cases}
    C_{2,b-1}(\mu) + \gamma_b(\mu),\\
    C_{1,b-1}^{\leq}(\mu) + \gamma_b(\mu).
  \end{cases}
\end{equation*}

For the third segment we first compute the minimum cost up to data point 3
\begin{equation*}
  C_{3,3}(\mu) = C_{2,2}^{\geq}(\mu) + \gamma_3(\mu),
\end{equation*}
where the more-min operator $f^\geq$ is defined analogously. The
update formula for the minimum cost up to data point
$b\in\{4, ..., B\}$ is
\begin{equation*}
  C_{3,b}(\mu) = \min
  \begin{cases}
    C_{3,b-1}^{\geq}(\mu)+\gamma_b(\mu),\\
    C_{2,b-1}^{\geq}(\mu)+\gamma_b(\mu)
  \end{cases}
\end{equation*}
In general for $s$ segments, we use
\begin{equation}
  C_{s,s}(\mu) = C_{s-1,s-1}^{*}(\mu) + \gamma_s(\mu),
\end{equation}
and for $b\in\{s+1, ..., B\}$
\begin{equation}
  C_{s,b}(\mu) = \min
  \begin{cases}
    C_{s,b-1}^{*}(\mu)+\gamma_b(\mu),\\
    C_{s-1,b-1}^{*}(\mu)+\gamma_b(\mu),
  \end{cases}
\end{equation}
where * means less-min for even-numbered segments $s$, and more-min
for odd-numbered segments.

\subsection{Optimal Partitioning}

\section{Figures}

\subsection{Simple data set analysis}

In this section we analyze the simple data
$\mathbf z = \left[\begin{array}{cccc} 1 & 10 & 14 & 13
\end{array}\right]\in\ZZ_+^4
$. For $s=3$ segments there are only 3 possible segmentations:
$[1][10][14, 13]$, $[1][10, 14][13]$ and $[1, 10][14][13]$. If we use
max-likelihood estimates for each segment mean, then only the last
segmentation obeys the \ref{PenPeakSeg} model constraints. However the
constrained Dynamic Programming Algorithm of \citet{PeakSeg} does not
recover it. However this last segmentation is more costly than a
simpler segmentation with a single change after the first data point
(see figure below).

\includegraphics[width=\textwidth]{figure-constrained-PDPA-normal-grid}

The figure above plots the cost of a segmentation in two segments, up
to data point 3. There are two possible change-points: after 1 [1][10
14], and after 2 [1 10][14]. The minimum cost of a change after 2 is
40.5, but the other change is less costly for those mean values (5.5,
14). Thus it is clear that although the segmentation [1 10][14][13] is
feasible for the PeakSeg problem, it is clearly not optimal.

\subsection{Unconstrained PDPA and FPOP}

In the figures below I took the R implementation of PDPA and FPOP from
the FPOP paper, and I ran both on this simple data set. The plot shows
the functions and intervals at each time step. The first column shows
the model ofthe cost before adding a new data point, the second column
shows after adding a new data point but before pruning, and the third
column shows after pruning.

\includegraphics[width=\textwidth]{figure-unconstrained-FPOP-normal}

\includegraphics[width=\textwidth]{figure-unconstrained-PDPA-normal}

The figure above shows the same results for the unconstrained PDPA.

\subsection{Results with and without strict inequalities}

\includegraphics[width=\textwidth]{figure-constrained-PDPA-normal-panels-pruning}

The figure above compares the results of the two different cost models
(with and without strict inequalities). Note that for strict
inequalities it is a bit more complicated to compute the less-min
operator. In fact the minimum $\mu^*$ may be undefined, in which case
we set $f^{<}(\mu)=\infty$ (infinite cost intervals are not
stored). Otherwise if $\mu^*$ is achieved somewhere in the domain of
$f$ we use
\begin{equation*}
  f^{<}(\mu) = \min_{x<\mu}f(x) = 
  \begin{cases}
    f(\mu^*) & \text{ if } \mu > \mu^*,\\
    \infty & \text{ otherwise.}
  \end{cases}
\end{equation*}


\includegraphics[width=\textwidth]{figure-constrained-PDPA-normal-panels}

%%%% update rules
\newcommand{\FCC}{\widetilde{C}}
\newcommand{\MK}{\mathcal{M}^k}
\section{Constrained DP Problem}
\subsection{Some definition}

We aim at optimizing over all possible segmentations
$m$ in $\MK_n$ described as a set of segments $\{s_1, ... s_K\}$
the quantity
$\sum_{k \in [1..K]} \sum_{i \in s_{k}} (y_i - \mu_{k})^2$
under the following linear (K-1) constraints 
\begin{equation}
a_{kk} \mu_1 + a_{k(k+1}} \mu_2 + b_k \geq 0.
\end{equation}
Each of these constraint imply only 

\subsection{Example}
Isotonic regression

Up-Down model

\subsection{Fonctiona cost representation}

We will consider the optimal fonctionnal cost up to in $k$ which is 
\begin{equation*}
\begin{aligned}
& \FCC^k_t(\mu) = \underset{x}{\text{minimize}} 
& & \sum_{r \in [1..k]} \sum_{i \in r} (x_i - \mu_r)^2 \\
& \text{subject to}
& & \mu_{2i} \geq \mu_{2i+1} \ \text{and} \ \mu_{2i} \geq \mu_{2i-1}
\end{aligned}
\end{equation*}
$\FCC^k_t(\mu) = min $
If

\subsection{Update rule}
%%%%

\section{Timings}

TODO: plot the number of intervals over time.

TODO: show that the time complexity is empirically linear in the
number of data points.

\section{Comparison with two-state HMM}

TODO: first run two-state HMM, which will assign a certain number of
peaks. Compute the Poisson loss for that model, and compare it to the
optimal Poisson loss recovered by the PDPA.

\includegraphics[width=\textwidth]{figure-NA-timings}

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}

