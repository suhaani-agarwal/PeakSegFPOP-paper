% -*- compile-command: "make HOCKING-PeakSegFPOP-pipeline-slides.pdf" -*-
\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{Genome-wide supervised ChIP-seq peak detection}

\author{
  Toby Dylan Hocking\\
  toby.hocking@mail.mcgill.ca\\
  joint work with Guillem Rigaill, Paul Fearnhead, 
  Guillaume Bourque}

\date{11 Nov 2016}

\maketitle

\section{Problem: optimizing ChIP-seq peak detection}

\begin{frame}
  \frametitle{Chromatin immunoprecipitation sequencing (ChIP-seq)}
  Analysis of DNA-protein interactions.

  \includegraphics[width=\textwidth]{Chromatin_immunoprecipitation_sequencing_wide.png}

  Source: ``ChIP-sequencing,'' Wikipedia.
\end{frame}

\begin{frame}
  \frametitle{Problem: find peaks in each of several samples}
  \includegraphics[width=\textwidth]{screenshot-ucsc-edited}

  Grey profiles are normalized aligned read count signals.

  Black bars are ``peaks'' called by MACS2 (Zhang et al, 2008):
  \begin{itemize}
  \item \alert<1>{many false positives}.
  \item overlapping peaks have different start/end positions.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Previous work in genomic peak detection}
  \begin{itemize}
  \item Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
  \item SICER, Zang et al, 2009.
  \item HOMER, Heinz et al, 2010.
  \item CCAT, Xu et al, 2010.
  \item RSEG, Song et al, 2011.
  \item Triform, Kornacker et al, 2012.
  \item Histone modifications in cancer (HMCan), Ashoor et al, 2013.
  \item PeakSeg, Hocking, Rigaill, Bourque, ICML 2015.
  \item PeakSegJoint Hocking and Bourque, arXiv:1506.01286.
  \item ... dozens of others.
  \end{itemize}
  Two big questions: how to choose the best...
  \begin{itemize}
  \item ...algorithm? (testing)
  \item \alert<1>{...parameters? (training)}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{How to choose parameters of unsupervised peak
    detectors?}
\scriptsize
19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
\begin{verbatim}
  [-g GSIZE]
  [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
  [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
  [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
  [--down-sample] [--seed SEED] [--nolambda]
  [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
  [--shift-control] [--half-ext] [--broad]
  [--broad-cutoff BROADCUTOFF] [--call-summits]
\end{verbatim}
10 parameters for Histone modifications in cancer (HMCan),
Ashoor et al, 2013.
\begin{verbatim}
minLength 145
medLength 150
maxLength 155
smallBinLength 50
largeBinLength 100000
pvalueThreshold 0.01
mergeDistance 200
iterationThreshold 5
finalThreshold 0
maxIter 20
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Which macs parameter is best for these data?}
  \includegraphics[width=1\textwidth]{figure-macs-problem.png}
\end{frame}

\begin{frame}
  \frametitle{Compute likelihood/loss of piecewise constant model}
  \includegraphics[width=1\textwidth]{figure-macs-problem-7-5.png}
  % $\PoissonLoss(\mathbf z, \mathbf m) = \sum_{i=1}^n m_i - z_i \log(m_i)$
  % for count data $\mathbf z\in\ZZ_+^n$ 
  % and segment mean model $\mathbf m\in\RR^n$.
\end{frame}

\begin{frame}
  \frametitle{Idea: choose the parameter with a lower loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-15.png}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}

  Choose the number of peaks via standard penalties (AIC, BIC,
    ...)\\or learned penalties based on visual labels (more on this later).
\end{frame}

\begin{frame}
  \frametitle{Maximum likelihood Poisson segmentation models}
  \includegraphics[width=1\textwidth]{figure-Segmentor-PeakSeg}

  \begin{itemize}
  \item Previous work: unconstrained maximum likelihood mean\\
    for $s$ segments ($s-1$ changes), Cleynen et al 2014.
  \item Hocking et al, ICML 2015: PeakSeg constraint enforces up, down, up,
    down changes (and not up, up, down). 
  \item Odd-numbered segments are background noise,\\
    even-numbered segments are peaks.
  \item Constrained Dynamic Programming Algorithm, 
    $O(N^2)$ time for $N$ data points.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{But quadratic time is not fast enough for genomic data!}
  \includegraphics[width=\textwidth]{figure-PDPA-timings-dp}
  \begin{itemize}
  \item Genomic data is large, $N \geq 10^6$.
  \item Split into subsets? What if we split a peak in half?
  \item Need linear time algorithm for analyzing whole data set.
  \end{itemize}
\end{frame}

\section{Segment neighborhood model (constraint on number of peaks)}

\begin{frame}
  \frametitle{Statistical model is Poisson with change constraints}
  \begin{itemize}
  \item We have $N$ count data $z_1, \dots, z_N\in\ZZ_+$.
  \item Fix the number of segments $S\in\{1, 2, \dots, N\}$.
  \item PeakSeg Model: $z_t \sim \text{Poisson}(m_t)$ such that $m_t$
    has $S-1$ up-down changes.
  \item Want to find means $m_t$ which maximize the Poisson likelihood:
    $P(Z = z_t|m_t) = m_t^{z_t} e^{-m_t} / (z_t!)$.
  \item Equivalent to finding means $m_t$ which minimize the Poisson
    loss: $\ell(m_t, z_t) = m_t - z_t\log m_t$.
  \item Naive computation is $O(N^S)$, since there are $O(N^{S-1})$ possible
    positions for $S-1$ change-points, and it takes $O(N)$ operations to
    compute the mean and loss for each.
  \item Comparison to Hidden Markov Model:
    \begin{description}
    \item[Likelihood] Same emission terms, no transition terms.
    \item[Constraint] Number of changes rather than values.
    \end{description}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Relation to previous dynamic programming algorithms}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & \alert<1>{Dynamic Programming} & \alert<2>{Pruned DP} \\
     & \alert<1>{exact $O(SN^2)$} & \alert<2>{exact $O(SN\log N)$}\\
    R pkgs: & \alert<1>{changepoint} & \alert<2>{cghseg, Segmentor}\\
    \hline
    up-down constrained & \alert<3>{constrained DP} & \alert<4>{\textbf{this work}} \\
     & \alert<3>{inexact $O(SN^2)$} & \alert<4>{exact $O(SN\log N)$}\\
    R pkgs: & \alert<3>{PeakSegDP} & \alert<4>{coseg}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item \alert<1>{Auger and Lawrence, Algorithms for the optimal
      identification of segment neighborhoods (Bull Math Biol 1989).}
  \item \alert<2>{Independent discovery: Rigaill 2010, Johnson 2013. R pkg: Cleynen et al 2014.}
  \item \alert<3>{Hocking, Rigaill, Bourque 2015.}
  \item \alert<4>{\textbf{Contribution}: new algorithm that
      \textbf{exactly} computes the \textbf{constrained} optimal
      segmentation for $N$ data points and $1,\dots,S$ segments in $O(S N\log N)$ time.}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynamic programming and functional pruning}
  \textbf{Classical dynamic programming} ( 1989) computes the matrix of optimal loss
  values in $S$ segments up to $N$ data points, $O(S N^2)$
$$
\begin{array}{ccc}
  \mathcal L_{1,1} & \cdots &   \mathcal L_{1,N}\\
  \vdots &  & \vdots\\
  \mathcal L_{S,1} & \cdots & \mathcal L_{S,N}\\
\end{array}
$$
\textbf{Dynamic programming with functional pruning} (Rigaill
2010) computes a matrix of loss \textbf{functions}, the
optimal loss up to $N$ data points if segment $S$ has mean $\mu_S$,
$O(S N\log N)$
$$
\begin{array}{ccc}
   L_{1,1}(\mu_1) & \cdots & L_{1,N}(\mu_1)\\
  \vdots &  & \vdots\\
   L_{S,1}(\mu_S) & \cdots & L_{S,N}(\mu_S),\\
\end{array}
$$
\textbf{Contribution of this work}: a new algorithm that applies the
functional pruning technique to the up-down constrained model.
\end{frame}

\begin{frame}
  \frametitle{First segment, first data point}
  \begin{itemize}
  \item For data $z_1, \dots, z_N\in\ZZ_+$ let
  \begin{equation*}
    \gamma_t(\mu) = \ell(\mu, z_t) = \mu - z_t \log \mu
  \end{equation*}
  be the Poisson loss for each $t\in\{1, \dots, N\}$.
\item For example $z = 2, 1, 9, 5, 10, 3$.
\item Then $\gamma_1(\mu)=L_{1,1}(\mu)= \alert{1}\mu - \alert{2}\log \mu + \alert{0}$.
\item Need to store 3 coefficients (\alert{linear}, \alert{log}, \alert{constant}).
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-1data}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{First segment, other data points}
  \begin{itemize}
\item
  The loss of the first segment up to data point $t$ is
  \begin{equation*}
    \label{eq:C1b}
    L_{1,t}(\mu) = \sum_{i=1}^t \gamma_i(\mu).
  \end{equation*}
\item For example $z = 2, 1, 9, 5, 10, 3$.
% \item $L_{1,2}(\mu) = (2\mu - 3\log\mu + 0)/2 = 1\mu - 1.5\log\mu + 0$.
% \item $L_{1,3}(\mu) = (3\mu - 13\log\mu + 0)/3 = 1\mu - 4.333\log\mu + 0$.
\item $L_{1,2}(\mu) = 2\mu - 3\log\mu + 0$.
\item $L_{1,3}(\mu) = 3\mu - 12\log\mu + 0$.
\item ...
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-2data}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-3data}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 2}
  \begin{itemize}
  \item The mean cost in 2 segments up to data point 2 is
\begin{eqnarray*}
  L_{2,2}(\mu_2) 
  &=&  \gamma_2(\mu_2)+\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)\\
  &=& \gamma_2(\mu_2)+L_{1,1}^{\leq}(\mu_2)
\end{eqnarray*}
\item Min-less operator is $L^\leq(\mu) = \min_{x\leq\mu} L(x),$
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Comparison with unconstrained Pruned DPA}
  \begin{itemize}
  \item For our constrained algorithm, the first segment mean must be
    less than the second, and the first segment cost is a function:
    \begin{equation*}
      L_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \underbrace{\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)}_{L^\leq_{1,1}(\mu_2)}.
    \end{equation*}
  \item For the unconstrained algorithm, it is \alert<1>{constant}:
    \begin{equation*}
      \widehat{L}_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \alert<1>{\underbrace{\min_{\mu_1} L_{1,1}(\mu_1)}_{\mathcal L_{1,1}}}.
    \end{equation*}
  \item For example $z = 2, 1, 9, 5, 10, 3$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-mincompare-2segments-2data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Storage as a piecewise function on intervals}
  \begin{itemize}
  \item For example $z = 2, 1, 9, 5, 10, 3$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
  \item Storage: 
    \begin{equation*}
      L_{2,2}(\mu) = \gamma_2(\mu) + 
      \begin{cases}
        L_{1,1}(\mu) = 1\mu - 2\log \mu + 0 & \text{ if } \mu\in[1,2],\\
        \mathcal L_{1,1} = 0\mu -0\log\mu + 0.6137 & \text{ if } \mu\in[2,10].
      \end{cases}
    \end{equation*}
  \end{itemize}
\end{frame}

 
\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 3}
  \begin{itemize}
  \item For data point 3 we need to consider two change-points:
    \begin{equation*}
      L_{2,3}(\mu) =  \gamma_3(\mu) + \min
      \begin{cases}
        L_{1,2}^{\leq}(\mu), & \text{ change up after data point 2},\\
        L_{2,2}(\mu), & \text{ change up after data point 1}. 
      \end{cases}
    \end{equation*}
  \item For $z = 2, 1, 9, 5, 10, 3$ the min operation prunes a
    change after data point 1.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-3data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Second segment, up to data point t}
  \begin{itemize}
  \item The updates continue for every data point $t\in\{3, ..., N\}$
    \begin{equation*}
      L_{2,t}(\mu) =  \gamma_t(\mu) + \min
      \begin{cases}
        L_{1,t-1}^{\leq}(\mu), & \text{change up after $t-1$,}\\
        L_{2,t-1}(\mu), & \text{change up before $t-1$.}
      \end{cases}
    \end{equation*}
  \item For example for $z = 2, 1, 9, 5, 10, 3$, at data point $t=4$
    we only need to consider changes after 2 and 3 (1 has been
    pruned).
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-4data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Time complexity depends on number of intervals}
  \begin{itemize}
  \item Dynamic programming update rule: the constrained cost of a
    mean $\mu$ for the segment $s$, up to data point $t$:
    \begin{equation*}
      L_{s,t}(\mu) = \gamma_t(\mu) + \min
      \begin{cases}
        L_{s,t-1}(\mu),\\
        L_{s-1,t-1}^{*}(\mu),
      \end{cases}
    \end{equation*}
  \item Time complexity of min and min-less/more * is linear in the
    number of intervals, empirically sub-linear $O(\log N)$.
    \includegraphics[width=0.5\textwidth]{figure-PDPA-intervals-all}
  \item Total time complexity: $O(S N\log N)$.
  \end{itemize}
\end{frame}

\section{Results on benchmark data (labeled chromosome subsets)}

\begin{frame}
  \frametitle{Previous work in computer vision: look and add labels
    to...}
  \begin{tabular}{ccc}
    Photos & Cell images & Copy number profiles \\
    \includegraphics[width=1.3in]{../chip-seq-paper/faces} &
    \includegraphics[width=1.3in]{../chip-seq-paper/cellprofiler} &
    \includegraphics[width=1.5in]{../chip-seq-paper/regions-axes}\\
    Labels: names & phenotypes & alterations \\ \\
    CVPR 2013 & CellProfiler & SegAnnDB \\
    246 papers & 873 citations & Hocking et al, 2014. \\
     &
  \end{tabular}
  Sources: \url{http://en.wikipedia.org/wiki/Face_detection}\\
  Jones et al PNAS 2009. Scoring diverse cellular morphologies in
  image-based screens with iterative feedback and machine learning.
\end{frame}

\begin{frame}
  \frametitle{Benchmark data sets, algorithms}

 \url{http://cbio.ensmp.fr/~thocking/chip-seq-chunk-db/}
  \begin{itemize}
  \item Hocking \emph{et al} Bioinformatics (2016). Optimizing
    ChIP-seq peak detectors using visual labels and supervised machine
    learning.
  \item 37 labeled H3K4me3 samples (sharp peak pattern).
  \item 29 labeled H3K36me3 samples (broad peak pattern).
  \item 12,826 labeled regions with and without peaks.
  \item 2,752 separate segmentation problems.
  \end{itemize}

  Algorithms for segmenting $N$ data points:
  \begin{center}
  \begin{tabular}{ccccc}
    package & constraint & exact? & complexity \\
    \hline
    \textbf{coseg (this work)} & $\mu_1 \leq \mu_2 \geq \mu_3 \dots$ & yes & $O(N\log N)$ \\
    PeakSegDP & $\mu_1 < \mu_2 > \mu_3 \dots$ & no & $O(N^2)$\\
    Segmentor & none & yes & $O(N\log N)$
  \end{tabular}

  \vskip 0.5cm

  Segmentor loss $\leq$ coseg loss $\leq$ PeakSegDP loss.
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Linear time algorithms faster for larger data sets}
  \includegraphics[width=1\textwidth]{figure-PDPA-timings.pdf}

  Total time to compute 10 models (0, ..., 9 peaks) for all data sets:
  \begin{itemize}
  \item PeakSegDP: 156 hours, inexact.
  \item coseg: 6 hours, exact.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{8 false negative labels for models with 0 peaks}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-0peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 1 peak are better (6 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-1peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 2 peaks are better still (4 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-2peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 3 peaks are the same (4 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-3peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 4 peaks are better (2 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-4peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 5 peaks have no incorrect labels}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-5peaks}
\end{frame}


\begin{frame}
  \frametitle{Models with 6 peaks are worse (1 false positive)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-6peaks}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization better than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-best}
\end{frame}

\begin{frame}
  \frametitle{0 errors for coseg/PeakSegDP, 6 errors for Segmentor}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem2-best.png}
\end{frame}


% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-0peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-1peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-2peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-3peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-4peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-0peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-1peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-2peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-3peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-4peaks.png}
% \end{frame}

\begin{frame}
  \frametitle{Minimum train error in all data sets}
  \begin{center}
      \input{table-min-train-error}
  \end{center}
  \begin{itemize}
  \item Segmentor, PeakSegDP, coseg were used to compute up to 10
    models for each problem (1,...,19 segments = 0,...,9 peaks).
  \item errors = fp + fn = total number of incorrect labels, after
    picking best parameter for each of the 2752 separate problems.
  \item models = number that obey the up-down constraint.
  \item New coseg algorithm has minimum train error almost as good as
    slower PeakSegDP algorithm.
  \item Other algorithms much less accurate.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Max-margin penalty learning algortihm}
  \includegraphics[width=\textwidth]{figure-large-margin}

\scriptsize \url{http://bl.ocks.org/tdhock/raw/9311ca39d643d127e04a088814c81ee1/}

\end{frame}

\begin{frame}
  \frametitle{ROC curves for one test fold}
  \includegraphics[width=\textwidth]{screenshot-roc-test-interactive}
  
  \scriptsize \url{http://cbio.ensmp.fr/~thocking/chip-seq-chunk-db/figure-roc-test/}
\end{frame}


\begin{frame}
  \frametitle{Test AUC on 7 benchmark data sets}
  \includegraphics[width=\textwidth]{figure-test-error-dots}
  \begin{itemize}
  \item 4-fold cross-validation: train on 3/4 of labels, test on 1/4.
  \item HMCanBroad is accurate for broad H3K36me3 data but not sharp H3K4me3 data.
  \item MACS is accurate for sharp H3K4me3 but not broad H3K36me3 data.
  \item Unconstrained Segmentor algorithm not as accurate as up-down
    constrained algorithms (coseg, PeakSegDP).
  \item Proposed algorithm in coseg R package yields state-of-the-art
    accuracy in all benchmark data sets.
  \end{itemize}
  \scriptsize
\url{http://bl.ocks.org/tdhock/raw/886575874144c3b172ce6b7d7d770b9f/}
\end{frame}

\section{Optimal partitioning model (penalize number of peaks)}

\begin{frame}
  \frametitle{Optimal partitioning model penalizes the number of changes}
  \begin{itemize}
  \item We have $N$ count data $z_1, \dots, z_N\in\ZZ_+$.
  \item Fix the number of segments $S\in\{1, 2, \dots, N\}$.
  \item PeakSeg Segment Neighborhood Model: find means $m_t$ which
    minimize the Poisson loss $\sum_{t=1}^N \ell(m_t, z_t)$,
    such that there are exactly $S-1$ up-down changes.
  \item PeakSeg Optimal Partitioning Model: find means $m_t$ with
    up-down changes which minimize the \alert<1>{penalized} Poisson
    loss
    $$\alert<1>{\lambda\sum_{t=1}^{N-1}I(m_t\neq m_{t+1})} + \sum_{t=1}^N \ell(m_t, z_t).$$
  \item No constraint on the number of changes, but $\lambda > 0$
    penalizes models with more changes.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Relation to previous dynamic programming algorithms}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & \alert<1>{Dynamic Programming} & \alert<2>{FPOP} \\
     & \alert<1>{exact $O(N^2)$} & \alert<2>{exact $O(N\log N)$}\\
    R pkgs: &  & \alert<2>{fpop}\\
    \hline
    up-down constrained &  & \alert<3>{\textbf{this work}} \\
     &  & \alert<3>{exact $O(N\log N)$}\\
    R pkgs: &  & \alert<3>{coseg}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item \alert<1>{Jackson et al 2005, An algorithm for optimal partitioning of data on an interval.}
  \item \alert<2>{Independent discovery: Johnson 2013, Maidstone et
      al 2016 (FPOP = Functional Pruning Optimal Partitioning).}
  \item \alert<3>{\textbf{Contribution}: new dynamic programming algorithm that
      \textbf{exactly} computes the \textbf{constrained} optimal
      segmentation for $N$ data points and a single penalty $\lambda$ in $O(N\log N)$ time.}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dynamic programming and functional pruning}
  \textbf{Classical dynamic programming} (Jackson et al 2005) 
  computes the optimal loss values up to $N$ data points, $O(N^2)$
$$
\begin{array}{ccc}
  \mathcal L_{1} & \cdots &   \mathcal L_{N}\\
\end{array}
$$
\textbf{Dynamic programming with functional pruning} (Rigaill
2010) computes loss \textbf{functions}, the
optimal loss up to $N$ data points with last segment mean $\mu$,
$O(N\log N)$
$$
\begin{array}{ccc}
   L_{1}(\mu) & \cdots & L_{N}(\mu)\\
\end{array}
$$
\textbf{Contribution of this work}: a new algorithm that applies the
functional pruning technique to the up-down constrained model.
\end{frame}

\begin{frame}
  \frametitle{Target interval computation time}
  \includegraphics[width=\textwidth]{figure-target-interval-time}
\end{frame}

\begin{frame}
  \frametitle{More peaks in sharp H3K4me3 than in broad H3K36me3}
  \includegraphics[width=0.7\textwidth]{figure-min-err-peaks-compare}
\end{frame}

\begin{frame}
  \frametitle{BerkeleyDB C++ Standard Template Library}
  TODO
\end{frame}

\begin{frame}
  \frametitle{Pipeline}
Input: $S$ bigWig files, problems.bed file with $P$ problems, labels.txt file(s).
For example $S=37$ samples, $P=374$ problems in hg19 (regions between gaps).
  \begin{tabular}{ccc}
    Step& Jobs& Time/job \\
    \hline
    Create separate problems & 1 & 5 min\\
    Compute separate targets & $S P$ & 1--10 hours\\
    Train separate model & 1 & 5 min \\
    Separate peak prediction & $P$ & 1--10 hours\\
    Train joint model & 1 & 5 min \\
    Joint peak prediction & $P$ & 1--10 hours\\
    Combine and plot predictions & 1 & 10--60 minutes
  \end{tabular}
Output: matrix of joint peak predictions, samples x peaks.
\end{frame}

\section{Conclusions and future work}

\begin{frame}[fragile]
  \frametitle{How to choose parameters of unsupervised peak
    detectors?}
\scriptsize
19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
\begin{verbatim}
  [-g GSIZE]
  [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
  [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
  [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
  [--down-sample] [--seed SEED] [--nolambda]
  [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
  [--shift-control] [--half-ext] [--broad]
  [--broad-cutoff BROADCUTOFF] [--call-summits]
\end{verbatim}
10 parameters for Histone modifications in cancer (HMCan),
Ashoor et al, 2013.
\begin{verbatim}
minLength 145
medLength 150
maxLength 155
smallBinLength 50
largeBinLength 100000
pvalueThreshold 0.01
mergeDistance 200
iterationThreshold 5
finalThreshold 0
maxIter 20
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}
  
  Simple model with only one parameter to train (maxPeaks).
\end{frame}

\begin{frame}
  \frametitle{Linear time algorithms faster for larger data sets}
  \includegraphics[width=1\textwidth]{figure-PDPA-timings.pdf}

  Total time to compute 10 models (0, ..., 9 peaks) for all data sets:
  \begin{itemize}
  \item PeakSegDP: 156 hours, inexact.
  \item coseg: 6 hours, exact.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Conclusions}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Dynamic Programming & Pruned DP \\
     & exact $O(N^2)$ & exact $O(N\log N)$\\
    R pkgs: & changepoint & cghseg, Segmentor\\
    \hline
    up-down constrained & constrained DP & \textbf{this work} \\
     & inexact $O(N^2)$ & exact $O(N\log N)$\\
    R pkgs: & PeakSegDP & \textbf{coseg}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item New algorithm that \textbf{exactly} computes the
    \textbf{constrained} optimal change-points/peaks for $N$ data points.
  \item C++ code in coseg R package, $O(N \log N)$ memory
    \url{https://github.com/tdhock/coseg}
  \item PeakSegFPOP program for big $N > 10^6$ data,
    $O(\log N)$ memory and $O(N\log N)$ disk space.
  \item TODO: regularized isotonic regression solver.
  \item TODO: supervised peak calling for ENCODE, Roadmap, ...
  \item TODO: interactive web app for creating labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Thanks for your attention!}

  Questions? toby.hocking@mail.mcgill.ca
  \begin{itemize}
  \item 
  \textbf{coseg} R package, \\
  \url{https://github.com/tdhock/coseg}
  \item 
    \textbf{PeakSegFPOP} command line program, 
  \url{https://github.com/tdhock/PeakSegFPOP}
  \item source code for these slides:
  \url{https://github.com/tdhock/PeakSegFPOP-paper}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Segmenting whole chromosomes?}
  \includegraphics[width=\textwidth]{screenshot-gap-peaks}
  \begin{itemize}
  \item 365 regions with no gaps in hg19.
  \item 272 regions with no gaps on chr1-22, X, Y.
  \item Smallest: 31,833 bases (chr6:157,609,467-157,641,300).
  \item Largest: 115,591,997 bases (chr4:75,452,279-191,044,276).
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Reasonable time to segment biggest region on chr1}
  \includegraphics[width=\textwidth]{figure-cosegData-timings.pdf}
  \begin{itemize}
  \item R package in memory: $O(N \log N)$ time.
  \item Command line program on disk: $O(N \log N)$ time.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Memory requirements reasonable for on-disk version}
  \includegraphics[width=\textwidth]{figure-cosegData-timings-memory-disk.pdf}
  \begin{itemize}
  \item R package in memory: $O(N \log N)$ memory.
  \item Command line program on disk: $O(\log N)$ memory.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Disk usage reasonable}
  \includegraphics[width=\textwidth]{figure-cosegData-timings-disk.pdf}
  \begin{itemize}
  \item R package in memory: no disk usage.
  \item Command line program: $O(N \log N)$ disk space (temporary).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two annotators provide consistent labels, but different
    precision}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/screenshot-several-annotators}

  \begin{itemize}
  \item TDH peakStart/peakEnd more precise than AM peaks.
  \item AM noPeaks more precise than TDH no label.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-TDH-experiments.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-TDH-experiments.pdf}
\end{frame}

\end{document}
