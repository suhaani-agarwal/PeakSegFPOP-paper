\documentclass{article}
\usepackage{fullpage}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography

%\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}

\begin{document}

\title{A functional pruning algorithm for constrained optimal segmentation}

\author{
  Toby Dylan Hocking\\
  Department of Human Genetics\\
  McGill University\\
  Montreal, QC H2R-2G9 Canada \\
  \texttt{toby.hocking@mail.mcgill.ca} \\
  %% examples of more authors
   \\
  Guillem Rigaill \\
  University of Evry \\
  Evry, France \\
  \texttt{guillem.rigaill@evry.fr} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\maketitle

\begin{abstract}
  Change-point detection is a central problem in time series and
  genomic data sets. In several kinds of data it is desirable to
  constrain the possible change-points to obtain a more interpretable
  model. We propose a new constrained Pruned Dynamic Programming
  Algorithm (cPDPA) which recovers the optimal change-points subject
  to affine constraints on adjacent segment means. We use this
  algorithm for isotonic regression and peak detection.
\end{abstract}

\section{Introduction}

Change-point detection is a central problem in many fields. When there
are no constraints other than the number of change-points or segments,
the Pruned Dynamic Programming Algorithm (PDPA) can be used to recover
the change-points with minimum cost \citep{pruned-dp}. The Functional
Pruning Optimal Partitioning (FPOP) algorithm can be used when there
is no constraint on the number of change-points, but there is a
positive penalty constant \citep{FPOP}. 

In the unconstrained change-point detection model, there is no
constraint on the direction of changes that can be recovered. However,
in several kinds of data it is desirable to constrain the possible
change-points to obtain a more interpretable model. For example, in
genomic data it is desirable to only consider models that can be
easily interpreted in terms of peaks and background
\citep{HOCKING-PeakSeg}. This amounts to forcing an up change after each down
change, and vice versa.

The isotonic regression problem is another example of constrained
change-point detection. Typically there is no limit on the number of
changes, as long as they are all in the same direction. This problem
can be solved using the pool-adjacent-violators algorithm
\citep{mair2009isotone}. An L1 relaxed version of this problem is
nearly-isotonic regression \citep{tibshirani2011nearly}.

TODO discuss isotonic DP \citep{isotonic-dp}, functional pruning
\citep{phd-johnson}, reduced isotonic regression
\citep{hardwick2014optimal, reduced-monotonic-regression}, unimodal
segmentation \citep{haiminen2008algorithms}, histogram construction
\citep{halim2009fast}.

\subsection{Contributions}

The main contribution of this paper is a family of algorithms for
solving constrained segmentation problems. These algorithms are
guaranteed to recover the exact solution to the constrained
segmentation problem. Furthermore, we show that these algorithms have
empirical time complexity which is linear in the number of data
points.

\section{Related work}
\label{sec:related}

The models we consider in this paper can be considered constrained
versions of optimal segmentation \citep{Segmentor} and isotonic
regression \citep{mair2009isotone}. The optimal segmentation model can
be computed using a dynamic programming algorithm (DPA)
\citep{bellman, segment-neighborhood, optimal-partitioning}, or a
pruned dynamic programming algorithm (pDPA) \citep{pruned-dp}. Both
algorithms are guaranteed to recover the exact solution to the
unconstrained model, but there are two important differences. The pDPA
is more complicated to implement, but is also computationally faster
than the DPA. For segmenting a sequence of $n$ data points, the pDPA
takes on average $O(n\log n)$ time whereas the DPA takes $O(n^2)$
time.

The constraints that we consider in this paper are a generalization of
the peak detection model \citep{HOCKING-PeakSeg}. Rather than
searching all possible change-points to find the most likely model
with $K$ segments, we propose to constrain the possible change-points
so that the segment means may be more easily interpreted.

\section{Isotonic regression and segment neighborhood models}

We first explain the classical isotonic regression model which has
inequality constraints on segment means, but no limit on the number of
segments \citep{mair2009isotone}. Then we explain the segment
neighborhood model \citep{segment-neighborhood}, which limits the
number of segment means, but has no inequality constraints. Finally,
we discuss a combination of the two models which we call segment
neighborhood isotonic regression (SNIR).

\subsection{Classical isotonic regression}

For a data set $\mathbf y\in\RR^n$, the classical isotonic regression
model is defined as the most likely sequence of increasing segment
means $\mathbf m = \left[
\begin{array}{ccc}
  m_1& \cdots &m_n
\end{array}
\right]
\in\RR^n$. More precisely, consider the following definition
of the set of possible mean vectors.
\begin{equation}
  \mathcal I^n = \{\mathbf m\in\RR^n \mid m_i \leq m_{i+1} \ \forall\ i<n\}
\end{equation}
Now, assume that the data are a realization of a probability
distribution with mean parameter $\mathbf m$. For example, assuming
$y_i \sim \mathcal N(m_i, \sigma^2)$ results in the following maximum
likelihood isotonic regression problem

\begin{equation}
  \label{eq:max_lik}
  \maximize_{\mathbf m\in\mathcal I^n, \sigma} \sum_{i=1}^n \Lik(y_i| m_i, \sigma^2).
\end{equation}

It is easy to show that the above concave maximization problem is equivalent
to the convex minimization problem below,
\begin{equation}
  \label{eq:isotonic}
  %\tag{\textbf{IsotonicRegression}}
  \minimize_{\mathbf m\in\mathcal I^n} \sum_{i=1}^n \ell(y_i, m_i),
\end{equation}
where the convex loss function $\ell:\RR\times \RR\rightarrow\RR$ in
this case is the square loss $\ell(y, m) = (y-m)^2$. This optimization
problem (\ref{eq:isotonic}) is referred to as isotonic regression, and
can be efficiently solved in $O(n)$ time using the
Pool-Adjacent-Violators Algorithm (PAVA) \citep{isotonic-unifying}.

One potential problem with the isotonic regression model is that it
has no limit on the number of change-points where $m_i < m_{i+1}$. In
particular, for an ever-increasing data set with $y_i < y_{i+1}$ for
all $i<n$, it is clear that the solution to the isotonic regression
problem (\ref{eq:isotonic}) is $m_i=y_i$ for all $i$, which implies
$n-1$ change-points. 

In contrast, it may be preferable to recover the $K\leq n$ most likely
segments (the $K-1$ most likely changes). For example, consider the
toy data set $\mathbf y= \left[
\begin{array}{cccccc}
  2 & 5 & 30 & 34 & 600 & 621
\end{array}
\right] \in\RR^6$. These data are strictly increasing, so the isotonic
regression (\ref{eq:isotonic}) solution is the trivial model
$m_i=y_i$. However, these data contain only two abrupt changes. To
recover these changes, we could instead use the segment
neighborhood model, which we discuss in the next section.

\subsection{Segment neighborhood}

The segment neighboorhood model of \citet{segment-neighborhood} uses
the same cost function as isotonic regression, but a different
constraint set. Let $K\leq n$ be the number of segments, and define
the set of segment means with $K-1$ change-points as
\begin{equation}
  \label{eq:Sk}
  \mathcal S_K^n = \left\{
  \mathbf m\in\RR^n
  \mid
  \sum_{i=1}^{n-1} I(m_i \neq m_{i+1}) = K-1
  \right\}.
\end{equation}
Using this constraint set with the same objective function results in
the following segment neighborhood problem:
\begin{equation}
  \label{eq:optimal_segment_neighborhood}
  \minimize_{\mathbf m\in\mathcal S_K^n} \sum_{i=1}^n \ell(y_i, m_i).
\end{equation}
The segment neighborhood problem can be efficiently solved in
$O(K n \log n)$ time using the pruned dynamic programming algorithm of
\citet{pruned-dp}.

Unlike isotonic regression, the segment neighborhood model
does not constrain the direction of the changes. Thus, for some
data sets $\mathbf y$, the segment neighborhood model may
recover a change down ($m_i > m_{i+1}$). For applications where
isotonic regression is used, it would be desirable to compute a model
with $K$ non-decreasing segment means. This results in the segment neighborhood
isotonic regression problem, which we introduce in the next section.

\subsection{Segment neighborhood isotonic regression (SNIR)}

The idea of fitting a non-decreasing function with a limited number of
change-points has been previously described as ``reduced isotonic
regression'' \citep{reduced-monotonic-regression,
  hardwick2014optimal}. In this paper we refer to the optimization
problem as Segment Neighborhood Isotonic Regression (SNIR) in order to
emphasize the similarity to the segment neighborhood problem
(\ref{eq:optimal_segment_neighborhood}). Both problems impose a
constraint of at most $K$ segment means ($K-1$
change-points). However, the SNIR problem imposes an additional
constraint that the segment mean must never decrease.

\begin{definition}[Segment neighborhood isotonic regression constraints]
  \label{def:I}
  Let $(\mathbf m, \mathbf c)\in\mathcal I_K^n$ be the set of all segment means
  $\mathbf m\in\RR^n$ and change-point indicators
  $\mathbf c\in\{0,1\}^{n-1}$ such that the following constraints are
  verified. The total number of non-zero change-point indicators is $K-1$:
  \begin{equation}
    \label{eq:isotonic_segments}
    \sum_{i=1}^{n-1} I(c_i = 1) = K-1.
  \end{equation}
  Every zero-valued change-point indicator has an equal segment mean
  after:
  \begin{equation}
    \label{eq:isotonic_0}
    c_i = 0 \Rightarrow m_i = m_{i+1}.
  \end{equation}
  Every one-valued change-point indicator may have a greater segment
  mean after:
  \begin{equation}
    \label{eq:isotonic_1}
    c_i = 1 \Rightarrow m_i \leq m_{i+1}.
  \end{equation}
\end{definition}

For a given data set $\mathbf y\in\RR^n$, loss function $\ell$, and
number of segments $K$, we define the segment neighborhood isotonic regression
problem as
\begin{equation}
  \label{eq:regularized_isotonic}
  \minimize_{(\mathbf m, \mathbf c)\in\mathcal I_K^n} \sum_{i=1}^n \ell(y_i, m_i).
\end{equation}

In the next section, we propose to compute the segment neighborhood isotonic regression solution
using an efficient dynamic programming algorithm.

%%%% update rules
\newcommand{\FCC}{\widetilde{C}}
\newcommand{\M}{\mathcal{M}}
\section{Dynamic programming algorithm for segment neighborhood isotonic regression}

In this section we propose a new dynamic programming algorithm that
can be used to compute the solution to the segment neighborhood isotonic
regression problem (\ref{eq:regularized_isotonic}). First, we show
that it is equivalent to solve a simpler problem with fewer
optimization variables. Then, we show how to recursively compute the
solution using dynamic programming.


\subsection{Equivalent problem with fewer optimization 
variables}

First, consider the following definition of a smaller optimization
space with only $K$ segment means and $K-1$ change-point indices.

\begin{definition}[Smaller optimization space for segment neighborhood isotonic regression]
\label{def:Ibar}
  Let $(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K$ be the set of
  all non-decreasing segment means $u_1\leq\cdots\leq u_K$ and
  increasing change-point indices $0=t_0<t_1<\cdots<t_{K-1}<t_K=n$.
\end{definition}

Each segment mean $u_k$ in this optimization problem is assigned to
data points $i\in(t_{k-1},t_k]$, resulting in the following cost
for each segment $k\in\{1, \dots, K\}$,

\begin{equation}
  \label{eq:h}
  h_{t_{k-1}, t_k}(u_k) = \sum_{i=t_{k-1}+1}^{t_k} \ell(y_i, u_k).
\end{equation}
This results in the following optimization problem,
\begin{equation}
  \label{eq:isotonic_ut}
  \minimize_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K}
  \sum_{k=1}^K
  h_{t_{k-1}, t_k}(u_k)
\end{equation}
Rather than explicitly summing over data points $i$ as in problem
(\ref{eq:isotonic}), this problem sums over segments $k$. The next lemma
proves that it is equivalent to solve this simpler optimization problem.

\begin{lemma}[Equivalence of problem with fewer optimization variables]
  \label{lemma:fewer-variables}
  Let $(\mathbf u, \mathbf t)$ be the solution to problem with fewer
  optimization variables (\ref{eq:isotonic_ut}), and consider the
  following mapping from the smaller space $\bar{\mathcal I}_K^n$ to
  the original larger space $\mathcal I_K^n$. For each segment
  $k\in\{1,\dots,K\}$, we assign $m_i = u_k$ for all data points
  $i\in(t_{k-1},t_k]$ on that segment. Furthermore, for all data
  points before change-points $i\in\{t_1,\dots,t_{K-1}\}$ we assign
  $c_i=1$, and we assign $c_i=0$ for all other data points
  $i\not\in\{t_1,\dots,t_{K-1}\}$. Then $(\mathbf m, \mathbf c)$ is
  the solution to problem (\ref{eq:isotonic}).
\end{lemma}

\begin{proof}
  It is clear that the mapping defined in Lemma~\ref{lemma:fewer-variables} is
  a bijection between $\bar{\mathcal I}_K^n$ and $\mathcal I_K^n$,
  since the constraints in Definitions~\ref{def:I} and~\ref{def:Ibar}
  are satisfied. Since the objective functions are equivalent, the
  optimization problems are equivalent:
  \begin{eqnarray}
  \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K}
  \sum_{k=1}^K
  h_{t_{k-1}, t_k}(u_k) 
    &=& \label{eq:proof_lh}
  \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K}
\sum_{k=1}^K 
        \sum_{i=t_{k-1}+1}^{t_k} \ell(y_i, u_k)\\
    &=& \label{eq:proof_bijection}
  \min_{(\mathbf m, \mathbf c)\in\mathcal I_n^K}
        \sum_{i=1}^{n} \ell(y_i, m_i)
  \end{eqnarray}
  Equation (\ref{eq:proof_lh}) holds by definition of
  $h_{t_{k-1},t_k}$ in (\ref{eq:h}), and equation
  (\ref{eq:proof_bijection}) holds because of the bijection.
\end{proof}

\subsection{Dynamic programming solution}

Optimization problem (\ref{eq:isotonic_ut}) has $K$ segment mean
variables and $K-1$ change-point index variables. In this section we
will further eliminate all variables except a single segment mean
variable using dynamic programming. First, for any function
$f:\RR\rightarrow\RR$, we define the min-less operator as
\begin{equation}
  \label{eq:min-less-def}
  f^\leq(u)=\min_{x\leq u} f(x).
\end{equation}
The min-less operator is used in the following definition of the
optimal cost.
\begin{definition}[Dynamic programming recursion]
\label{def:fcc}
  We refer to $\FCC_{K,n}(u)$ as the optimal cost of the segmentation
  with $K$ segments, up to data point $n$, with last segment mean
  $u$. For the first segment $K=1$, we define
  $\FCC_{1,1}(u)=\ell(y_1,u)$ for the first data point, and
  $\FCC_{1,t}(u)=\FCC_{1,t-1}(u)+\ell(y_t,u)$ for the other data
  points $t>1$. For $K>1$ segments, we define
  $\FCC_{K,K}(u)=\ell(y_K, u)+\FCC_{K-1,K-1}^\leq(u)$ for the $K$-th
  data point and for $t>K$ data points,
  \begin{equation}
\nonumber
  \FCC_{K,t}(u)=\ell(y_t,u)+
  \min\{
  \FCC_{K-1,t-1}^\leq(u),\,
  \FCC_{K,t-1}(u)
  \}.
  \end{equation}
\end{definition}
Note that in the original pruned dynamic programming algorithm for
solving the segment neighborhood problem \citep{pruned-dp}, the
min-less cost functions $\FCC_{k,t}^\leq(u)$ are replaced by cost
constants $C_{k,t}$. The main novelty of our proposed algorithm is the
computation of the min-less functions $\FCC_{k,t}^\leq(u)$ in closed
form.

Now, consider the following lemma, which shows that the dynamic
programming minimization over two cost functions is equivalent to the
minimization over all possible change-points.
\begin{lemma}[Dynamic programming minimizes with respect to all possible change-points]
\label{lemma:t_change_points}
  For the cost up to any data point $t> K$, the recursive dynamic
  programming cost $\FCC_{K,t}(u)$ is equivalent to the minimum cost
  over all possible change points
  $\min_{\tau\in[K-1,t)}\FCC_{K-1,\tau}^\leq(u)+h_{\tau,t}(u)$.
\end{lemma}

\begin{proof}
  We proceed by induction on data points $t$. First, we show that the
  equivalence holds for $t=K+1$ data points. By definition, we have
  \begin{eqnarray}
    \FCC_{K,K+1}(u)
    &=&\label{eq:proof_fcc1}\ell(y_{K+1},u)+\min\{\FCC_{K-1,K}^\leq(u),\,\FCC_{K,K}(u)\}\\
    &=&\min\label{eq:proof_fcc2}
        \begin{cases}
          \FCC_{K-1,K}^\leq(u)+\ell(y_{K+1},u)\\
          \FCC_{K-1,K-1}^\leq(u)+\ell(y_{K+1},u)+\ell(y_K,u)
        \end{cases}\\
    &=&\min\label{eq:proof_h}
        \begin{cases}
          \FCC_{K-1,K}^\leq(u)+h_{K,K+1}(u)\\
          \FCC_{K-1,K-1}^\leq(u)+h_{K-1,K+1}(u)
        \end{cases}\\
    \label{eq:proof_tau}
    &=&\min_{\tau\in[K-1,K+1)} \FCC_{K-1,\tau}^\leq(u)+h_{\tau,K+1}(u)
  \end{eqnarray}
  Equations (\ref{eq:proof_fcc1}-\ref{eq:proof_fcc2}) result by
  expanding $\FCC_{K,K+1}$ and $\FCC_{K,K}$ using
  Definition~\ref{def:fcc}. Equation (\ref{eq:proof_h}) follows from
  the definition of $h_{K,K+1}$ and $h_{K-1,K+1}$ in
  (\ref{eq:h}). Finally, equation (\ref{eq:proof_tau}) results from
  introducing the change-point optimization variable $\tau$. Thus, we
  have proved that the equivalence holds for $t=K+1$ data points.

  Now, we
  assume that the equivalence holds for $t$ data points, and prove it to be true
  for $t+1$ data points.
  \begin{eqnarray}
    \FCC_{K,t+1}(u)\label{eq:proof_fcct1}
    &=&\ell(y_{t+1},u)+\min\{\FCC_{K-1,t}^\leq(u),\,\FCC_{K,t}(u)\}\\
    &=&\min\label{eq:proof_induction_h}
        \begin{cases}
          \FCC_{K-1,t}^\leq(u)+h_{t,t+1}(u)\\
          \min_{\tau\in[K-1,t)} \FCC^\leq_{K-1,\tau}(u)+h_{\tau,t+1}(u)
        \end{cases}\\
    &=&\min_{\tau\in[K-1,t+1)} \FCC^\leq_{K-1,\tau}(u)+h_{\tau,t+1}(u)
        \label{eq:proof_tau2}
  \end{eqnarray}
  Equation (\ref{eq:proof_fcct1}) results by expanding $\FCC_{K,t+1}$
  using Definition~\ref{def:fcc}. Equation
  (\ref{eq:proof_induction_h}) follows from the definition of
  $h_{t,t+1}$ and the induction assumption. Finally, equation
  (\ref{eq:proof_tau2}) results from re-writing the top $h_{t,t+1}$ term using the
  change-point optimization variable $\tau$.  This concludes the proof
  by induction.
\end{proof}

Having proved Lemma~\ref{lemma:t_change_points}, we now use it to
prove the following theorem about the optimality of the dynamic
programming solution.
\begin{theorem}[Dynamic programming recovers the segment neighborhood isotonic regression solution]
  For a data set $\mathbf y\in\RR^n$, and any number of segments $K\leq n$,
  the optimal dynamic programming cost $\min_u \FCC_{K,n}(u)$ is
  equivalent to the minimum value of the segment neighborhood isotonic
  regression problem (\ref{eq:isotonic_ut}).
\end{theorem}
\begin{proof}
  We proceed by induction on segments $K$. First, consider the case of $K=2$ segments:
\begin{eqnarray}
  \label{eq:isotonic_ut_2}
  \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^2}
  \sum_{k=1}^2
  h_{t_{k-1}, t_k}(u_k)
  &= &
  \min_{t_1, u_2}
  h_{t_1,n}(u_2)
  +\min_{u_1\leq u_2}
  h_{0,t_1}(u_1)\\
  &=&
      \label{eq:min-less-2}
  \min_{t_1, u_2}
  h_{t_1,n}(u_2)
  +
  h^\leq_{0,t_1}(u_2)\\
  &=&
      \label{eq:u2}
  \min_{u_2} \FCC_{2, n}(u_2).
\end{eqnarray}
The first equality (\ref{eq:isotonic_ut_2}) follows from
expanding the optimization variables and the sum. The second equality
(\ref{eq:min-less-2}) follows from the definition of the min-less
operator (\ref{eq:min-less-def}). The final equality (\ref{eq:u2})
follows from Lemma~\ref{lemma:t_change_points}. Thus, the dynamic
programming recursion solves the segment neighborhood isotonic regression
problem for $K=2$ segments.

To complete the proof by induction, we assume that the equality holds for
$K$ segments, and prove that it holds for $K+1$ segments.
\begin{eqnarray}
  \label{eq:proof_separate_tau}
  \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^{K+1}}
  \sum_{k=1}^{K+1}
  h_{t_{k-1}, t_k}(u_k)
  &= & \label{eq:proof_hkexpand}\min_\tau
  \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_\tau^K}
       \left[
       \sum_{k=1}^K
       h_{t_{k-1},t_k}(u_k)
       \right]
       +\min_{u_{K+1}\geq u_K}
       h_{\tau, n} (u_{K+1})\\
&=& \min_\tau\min_{u_K} \FCC_{K,\tau}(u_K)\label{eq:proof_Ckt_induction}
    +\min_{u_{K+1}\geq u_K} h_{\tau,n}(u_{K+1})\\
% &=& \min_\tau \min_{u_{K+1}} h_{\tau,n}(u_{K+1})
% +\min_{u_K\leq u_{K+1}} \FCC_{K,\tau}(u_K)\\
&=& \min_\tau \min_{u_{K+1}} h_{\tau,n}(u_{K+1})\label{eq:proof_min_less_def_2}
+\FCC_{K,\tau}^\leq(u_{K+1})\\
&=& \min_{u_{K+1}} \FCC_{K+1,n}(u_{K+1}).\label{eq:proof_remove_tau}
\end{eqnarray}
Equation (\ref{eq:proof_hkexpand}) follows by removing the $k=K+1$
term from the sum, and (\ref{eq:proof_Ckt_induction}) follows from the
induction assumption. Equation (\ref{eq:proof_min_less_def_2}) follows
from the definition of the min-less operator (\ref{eq:min-less-def}),
and the final equality (\ref{eq:proof_remove_tau}) follows from
Lemma~\ref{lemma:t_change_points}. This concludes the proof by induction.
\end{proof}

\subsection{Storage and recovery of optimal parameters}

In the previous sections we have only discussed computation of the
optimal cost, but in this section we discuss how to store and compute
the optimal segment mean and change-point parameters. The dynamic
programming algorithm requires storage of real-valued cost functions
$\FCC_{k,t}(u)$ for all possible values of the last segment mean
parameter $u$. We propose to store each cost function as a piecewise
function on intervals, as in the original pruned dynamic programming
algorithm \citep{pruned-dp}.

To clarify the discussion, consider the 
toy data set $\mathbf y= \left[
\begin{array}{cccccc}
  2 & 1 & 0 & 4
\end{array}
\right] \in\RR^4$ and the square loss $\ell(y,m)=(y-m)^2$. The first
step of the algorithm is to compute the minimum and the maximum of the
data (0,4). Then the algorithm computes the optimal cost in $k=1$
segment up to data point $t=1$:
\begin{equation}
  \FCC_{1,1}(u) = (2-u)^2=4 - 4u + u^2.
\end{equation}
It is clear that this function can be stored for all values of $u$ via
the three real-valued coefficients ($\text{constant}=4$,
$\text{linear}=-4$, $\text{quadratic}=1$). To compute the optimal cost
in $K=2$ segments, we first need to compute the min-less operator of
this function:
\begin{equation}
  \FCC_{1,1}^\leq(u) =\min_{u'\leq u}\FCC_{1,1}(u')=
  \begin{cases}
    \FCC_{1,1}(u) = 4 - 4u + u^2 &\text{ if }u\in[0,2],\, t'=1,\, u'=u\\
    \FCC_{1,1}(2) = 0 + 0u + 0u^2 & \text{ if }u\in[2,4],\, t'=1,\, u'=2
  \end{cases}
\end{equation}
It is clear that this min-less function can be stored as a list of
coefficients and associated intervals of $u$ values. In addition, to
facilitate recovery of the optimal parameters, we store the previous
segment endpoint $t'$ and mean $u'$. Note that $u'=u$ means that the
equality constraint is active ($u_1=u_2$), so if the optimum occurs in that
interval, then the precise value of the mean will be computed
later. The optimal cost in $K=2$ segments up to data point $t=2$ is
then
\begin{equation}
  \FCC_{2,2}(u) = \FCC_{1,1}^\leq(u)+(1-u)^2 = 
  \begin{cases}
    5 - 6u + 2u^2 &\text{ if }u\in[0,2],\, t'=1,\, u'=u\\
    1 - 2u + 1u^2 &\text{ if }u\in[2,4],\, t'=1,\, u'=2
  \end{cases}
\end{equation}
Note that the minimum of this function is achieved at $u=1.5$ which
occurs in the first of the two function pieces, with an equality
constraint active ($u'=u$). This implies the optimal model up to data
point $t=2$ with $k=2$ up-down segments actually has no changes
($u_1=u_2=1.5$). After computing the optimal cost in $k=1$ segment up
to data point $t=2$
\begin{equation}
  \FCC_{1,2}(u) = \FCC_{1,1}(u)+(1-u)^2=5 - 6u + 2u^2,
\end{equation}
and its min-less operator
\begin{equation}
  \FCC^\leq_{1,2}(u) =
  \begin{cases}
    \FCC_{1,2}(u)=5-6u + 2u^2 & \text{ if }u\in[0, 1.5],\, t'=2,\, u'=u\\
    \FCC_{1,2}(1.5)=0.5 + 0u +0u^2 & \text{ if }u\in[1.5, 4],\, t'=2,\, u'=1.5
  \end{cases}
\end{equation}
we can use dynamic programming to compute $\FCC_{2,3}$, which is
defined as the minimum of two functions $\FCC_{2,2}$ and
$\FCC_{1,2}^\leq$ that have already been computed. Note that when
taking the minimum it is important to store the previous
change-points $t'$ and segment means $u'$ that were computed for the
min-less operators. The algorithm continues computing optimal cost
functions $\FCC_{k,t}$ for all segments $k\leq K$ and data points
$t\leq n$.

The last step of the dynamic programming recursion is computation of
$\FCC_{K,n}$. After that, we can begin computing optimal parameters
by starting with the last segment mean
$u_K = \argmin_u \FCC_{K,n}(u)$. This can be computed in $O(I)$
operations, where $I$ is the number of intervals or function pieces in
$\FCC_{K,n}$. Finding the optimal $u_K$ also yields the optimal
$(t_{K-1},u_{K-1})$ since they were stored as $(t',u')$ during the
computation of the function pieces in $\FCC_{K,n}$. The next step is
to find the function piece stored at $\FCC_{K-1,t_{K-1}}(u_{K-1})$,
which contains the optimal values for $(t_{K-2},u_{K-2})$, again
stored as $(t',u')$. Note that since we already know the optimal value
of the previous segment ($u_{K-1}$), no minimization is necessary,
and we can instead just look for the function piece with an interval
that contains that value. This decoding process continues for $k=K-1$
to $k=2$: the function piece stored at $\FCC_{k,t_{k}}(u_{k})$ 
contains the optimal values for $(t_{k-1},u_{k-1})$.
% By Lemma~\ref{lemma:t_change_points}, we can then compute the previous
% optimal change-point for $k=K-1$
% \begin{equation}
% \label{eq:decode_t}
%   t_{k} = \argmin_{\tau\in[k,n)} \FCC_{k,\tau}(u_{k+1}) + h_{\tau,n}(u_{k+1}).
% \end{equation}
% The next optimal segment mean is a constrained optimization for $k=K-1$
% \begin{equation}
% \label{eq:decode_u}
%   u_{k} = \argmin_{u\leq u_{k+1}} \FCC_{k,t_{k}}(u).
% \end{equation}
% Equations (\ref{eq:decode_t}-\ref{eq:decode_u}) are then used for all
% segment values $k$ from $K-2$ to 1.

We therefore propose the following data structures and algorithms for
the computation:
\begin{itemize}
\item FunctionPiece: a data structure which represents one piece of a
  cost function. It has coefficients which depend on the convex loss
  function (for the square loss it has three coefficients), and it
  always has elements for min/max mean values, and previous segment
  endpoint $t'$ and mean $u'$.
\item FunctionPieceList: an ordered list of FunctionPiece objects,
  which exactly stores a cost function $\FCC_{k,t}(u)$ for all values
  of last segment mean $u$.
\item $\text{MinLess}(t, f)$: an algorithm that inputs a change-point
  and a FunctionPieceList, and outputs the corresponding min-less
  operator $f^\leq$ (another FunctionPieceList), with the previous change-point set to
  $t'=t$ for each of its pieces. This algorithm also needs to store
  the previous mean value $u'$ for each of the function pieces. See
  Section~\ref{sec:implementation-details} for more details and
  pseudocode for this algorithm.
\item $\text{MinOfTwo} (f_1, f_2)$: an algorithm that inputs two
  FunctionPieceList objects, and outputs another FunctionPieceList
  object which is their minimum. See
  Section~\ref{sec:implementation-details} for more details and
  pseudocode for this algorithm.
\item Minimize TODO
\item FindMean TODO
\end{itemize}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf y\in\RR^n$, maximum number of segments $K\in\{2,\dots, n\}$.
\STATE Output: matrices of optimal segment means $U\in\RR^{K\times K}$ 
and ends $T\in\{1,\dots,n\}^{K\times K}$
\STATE Compute min $\underline y$ and max $\overline y$ of $\mathbf y$.
\STATE $\FCC_{1,1}\gets \text{FunctionPiece}(y_1, \underline y, \overline y)$
\STATE for data points $t$ from 2 to $n$:
\begin{ALC@g}
  \STATE $\FCC_{1,t}\gets \text{FunctionPiece}(y_t, \underline y, \overline y) + \FCC_{1,t-1}$
\end{ALC@g}
\STATE for segments $k$ from 2 to $K$: for data points $t$ from $k$ to $K$: // dynamic programming
\begin{ALC@g}
  \STATE $\text{min\_prev}\gets \text{MinLess}(t-1, \FCC_{k-1,t-1})$
  % \STATE if $t=k$:
  % \begin{ALC@g}
  %   \STATE $\text{min\_new}\gets\text{min\_prev}$ // there is only one
  %   possible change-point, before $t$
  % \end{ALC@g}
  % \STATE else:
  % \begin{ALC@g}
  %   \STATE $\text{min\_new}\gets\text{MinOfTwo}(\text{min\_prev}, \FCC_{k, t-1})$
  % \end{ALC@g}
    \STATE $\text{min\_new}\gets\text{min\_prev}$ if $t=k$, 
else $\text{MinOfTwo}(\text{min\_prev}, \FCC_{k, t-1})$
  \STATE $\FCC_{k,t}\gets \text{min\_new} + \text{FunctionPiece}(y_t, \underline y, \overline y)$
\end{ALC@g}
\STATE for segments $k$ from 1 to $K$: // decoding
\begin{ALC@g}
  \STATE $u^*,t',u'\gets \text{Minimize}(\FCC_{k,n})$
  \STATE $U_{k,k}\gets u^*;\, T_{k,k}\gets t'$
  \STATE for segment $s$ from $k-1$ to $1$:
  \begin{ALC@g}
    \STATE if $u' < \infty$: $u^*\gets u'$
    \STATE $t',u'\gets\text{FindMean}(u^*, \FCC_{s,t'})$
    \STATE $U_{k,s}\gets u^*;\, T_{k,s}\gets t'$
  \end{ALC@g}
\end{ALC@g}
\caption{\label{algo:minless}Segment Neighborhood Isotonic Regression (SNIR) solver.}
\end{algorithmic}
\end{algorithm}

\section{Variants of the dynamic programming algorithm for other models}

In this section we discuss some other optimization problems that are
solvable using small modifications to our proposed dynamic programming
algorithm.

% \subsection{Comparison with unconstrained functional pruning
%   algorithm}

% The algorithm we propose generalizes the functional pruning technique
% that has been previously described \citep{pruned-dp, johnson,
%   Segmentor, FPOP}. In fact, the original pruned dynamic programming
% algorithm is to compute the functional cost $\FCC_n^K(\mu)$ of the
% best segmentation in $K$ segments up to data point $n$, given that the
% last segment mean has value $\mu$. Then the optimal mean and cost
% values can be determined by minimizing the functional cost
% $C_n^K = \min_\mu \FCC_n^K(\mu)$. These cost functions can be
% efficiently computed using the recursive formula
% \begin{equation}
%   \FCC_n^K(\mu) = \ell(y_n, \mu) + \min\{
%   \FCC_{K-1}^{n-1},\, 
%   \FCC_K^{n-1}(\mu)\}.
% \end{equation}

\subsection{Optimal partitioning isotonic regression (OPIR)}

As in the Functional Pruning Optimal Partitioning (FPOP) algorithm of
\citet{FPOP}, we can define the OPIR constraints as follows.
\begin{definition}[Optimal partitioning isotonic regression constraints]
  \label{def:P}
  Let $(\mathbf m, \mathbf c)\in\mathcal P_K^n$ be the set of all segment means
  $\mathbf m\in\RR^n$ and change-point indicators
  $\mathbf c\in\{0,1\}^{n-1}$ such that the following constraints are
  verified. 
  Every zero-valued change-point indicator has an equal segment mean
  after:
  \begin{equation}
    \label{eq:P_0}
    c_i = 0 \Rightarrow m_i = m_{i+1}.
  \end{equation}
  Every one-valued change-point indicator may have a greater segment
  mean after:
  \begin{equation}
    \label{eq:P_1}
    c_i = 1 \Rightarrow m_i \leq m_{i+1}.
  \end{equation}
\end{definition}
Note that unlike the SNIR constraints (Definition~\ref{def:I}), the
OPIR constraints do not include any limit on the number of
segments. Instead, the objective function includes a positive penalty
$\lambda>0$ for each non-zero change-point variable:
\begin{equation}
    \minimize_{
        (\mathbf m, \mathbf c)\in\mathcal P^n
      } \ 
\lambda\sum_{t=1}^{n-1} I(c_t =1)
+
\sum_{t=1}^n \ell(y_t, m_t).
\end{equation}

For this model, the initialization for the first data point is
$C_1(u) = \ell(y_1, u)$. The dynamic programming update rule for all data points $t>1$ is
\begin{equation}
  C_t(u) = \ell(y_t, u) + \min\{
  C_{t-1}^\leq(u) + \lambda,\, C_{t-1}(u)
  \}.
\end{equation}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf y\in\RR^n$, penalty constant $\lambda\geq 0$.
\STATE Output: vectors of optimal segment means $\mathbf u\in\RR^{n}$ and ends $\mathbf t\in\{1,\dots,n\}^{n}$
\STATE TODO
\caption{\label{algo:minless}Optimal Partitioning Isotonic Regression (OPIR) solver.}
\end{algorithmic}
\end{algorithm}

\subsection{The PeakSeg up-down constraint}

To introduce the PeakSeg model \citep{HOCKING-PeakSeg}, we first
define the constraint set as follows.
\begin{definition}[PeakSeg constraints]
  \label{def:U}
  Let $(\mathbf m, \mathbf c)\in\mathcal U_K^n$ be the set of all segment means
  $\mathbf m\in\RR^n$ and change-point indicators
  $\mathbf c\in\{-1, 0,1\}^{n-1}$ such that the following constraints are
  verified. The total number of non-zero change-point indicators is $K-1$:
  \begin{equation}
    \label{eq:U_segments}
    \sum_{i=1}^{n-1} I(c_i \neq 0) = K-1.
  \end{equation}
  Every zero-valued change-point indicator has an equal segment mean
  after:
  \begin{equation}
    \label{eq:U_0}
    c_i = 0 \Rightarrow m_i = m_{i+1}.
  \end{equation}
  Every positive change-point indicator may have a greater segment
  mean after:
  \begin{equation}
    \label{eq:U_1}
    c_i = 1 \Rightarrow m_i \leq m_{i+1}.
  \end{equation}
  Every negative change-point indicator may have a smaller segment
  mean after:
  \begin{equation}
    \label{eq:U-1}
    c_i = -1 \Rightarrow m_i \geq m_{i+1}.
  \end{equation}
  The first change is an up change, and every up change is followed by a
  down change, and vice versa. Mathematically, the cumulative sum of
  change-point variables is either zero or one, for all $t<n$:
  \begin{equation}
    \label{eq:U-cum}
    \sum_{i=1}^t c_i \in \{0, 1\}.
  \end{equation}
\end{definition}
The PeakSeg optimization problem is
\begin{equation}
    \minimize_{
        (\mathbf m, \mathbf c)\in\mathcal U^n_K
      } \ 
\sum_{t=1}^n \ell(y_t, m_t).
\end{equation}

For this model, the initialization $k=1$ is the same as for SNIR
(Definition~\ref{def:fcc}). The dynamic programming updates for even
$k\in\{2, 4, \dots\}$ are also the same. However, the updates for odd
$k\in\{3, 5, \dots\}$ are
\begin{equation}
  C_{k,t}(u) = \ell(y_t, u) + \min\{
  C_{k-1,t-1}^\geq(u),\, C_{k,t-1}(u)
  \},
\end{equation}
where the min-more operator is defined for any function $f:\RR\rightarrow\RR$ as
\begin{equation}
  \label{eq:min-more-def}
  f^\geq(u) = \min_{x\geq u} f(x).
\end{equation}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf y\in\RR^n$, maximum number of segments $K$.
\STATE Output: matrices of optimal segment means $U\in\RR^{K\times K}$ and ends $T\in\{1,\dots,n\}^{K\times K}$
\STATE TODO
\caption{\label{algo:minless}Solver for Segment Neighborhood problem with PeakSeg up-down constraints.}
\end{algorithmic}
\end{algorithm}

\subsection{General linear inequality constraints
  between adjacent segment means}

A segmentation $m$ is described as a set of contiguous segments $\{s_1, ... s_{|m|} \}$, where $|m|$ is the number of segments of $m$
We consider the set of all segmentation up to $n$: $\M_n$ 
or the set of all possible segmentation in $K$ segments: $\M^K_n$.
We define $r_m$ as the last segment of $m$.

We aim at optimizing over all possible segmentations $m$ in $\M^K_n$ or $\M_n$
 the quantity
$\sum_{r \in m} \sum_{i \in s_{r}} \ell(y_i, \mu_{r})$ subject to
the following $K-1$ linear constraints. 

\begin{eqnarray*}
a_{1,1}.\mu_1 \ + & a_{1,2}.\mu_2  & \geq  b_1 \\
\cdots \ +&  \cdots & \geq \cdots \\
a_{k,k}.\mu_{k} + & a_{k,k+1}.\mu_{k+1}  & \geq  b_{k} \\
\cdots \ +&  \cdots & \geq \cdots  \\
a_{K-1,K-1}.\mu_{K-1} \ +& a_{K-1,K}.\mu_K & \geq  b_{K-1},
\end{eqnarray*}
with all $a_{k,k+1} \neq 0$, $a_{k,k} \in \mathbb{R}$ and
$b_{k} \in \bar{\mathbb{R}}.$ In other words we aim at recovering the
best segmentation with successive mean parameters that obey the
constraints.

Some examples:
\begin{enumerate}
\item If we take all $a_{k,k+1} =1$, $a_{k,k}=0$ and $b_{k} = - \infty$ we recover the standard segmentation in the mean problem.
\item If we take all $a_{k,k+1} =1$, $a_{k,k}=-1$ and $b_{k} = 0$ we
  recover the isotonic regression problem (segment means always
  increasing).
\item For the PeakSeg model we take all $b_{k} = 0$. For odd $k$ we
  take $a_{k,k+1} =1$, $a_{k,k}=-1$ and for even $k$ we take
  $a_{k,k+1} =-1$, $a_{k,k}=1$.
\end{enumerate}

%\subsection{Functional cost representation}
To optimize this quantity we will consider the following functional quantity:

\begin{equation}
\FCC^k_t(\mu) =  \underset{m \in \M^K_n, \mu_r |  r \neq r_m}{\min} 
		\{ 
		   \underset{r \in m, r \neq r_m}{\sum} 
		   \underset{i \in r, i \leq t  }{\sum} \ell(y_i, \mu_{r}) 
		+ 
		   \underset{i \in r_m, i \leq t}{\sum} \ell(y_i, \mu)
		\}  
\end{equation}



\begin{eqnarray*}
\text{subject to} \\
a_{1,1}. \mu_1 \ + & a_{1,2}. \mu_2  & \geq  b_1 \\
\cdots \ + & \cdots & \geq \cdots \\
a_{k-1,k-1}. \mu_{k-1} \ + &a_{k-1,k}. \mu_{k}  & \geq  b_{k-1} \\
\end{eqnarray*}

$\FCC^k_t(\mu)$ is the best possible cost achievable in $k$ segment up to point $t$ with a $k$-th
segment mean of $\mu$.

%\subsection{Update rule}
We can then consider the following update rule

\begin{equation}
\FCC^{k+1}_{t+1}(\mu) = \min \{ \FCC^{k+1}_{t}(\mu)  , \underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}  \} + \ell(y_{t+1}, \mu)
\label{update}
\end{equation}

This update rule states that the best segmentation up to $t+1$ in $k+1$ segment with a last mean element of $\mu$ either has its $k$-th changepoint:
\begin{itemize}
\item before $t$ and in that case we should take the best possible segmentation up to $t$ in $k+1$
segments with a last mean of $\mu$ and then add  $\ell(y_{t+1}, \mu)$, that is:
$$\FCC^{k+1}_{t}(\mu) + \ell(y_{t+1}, \mu),$$

\item at $t$ and in that case we should take the best possible segmentation up to $t$ in $k$ segments
such that the last mean $\mu_k=\mu'$ validates the $k-th$ constraint with $\mu_{k+1}=\mu$ and then add  $\ell(y_{t+1}, \mu)$, that is:
 $$\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \} + \ell(y_{t+1}, \mu).$$
\end{itemize}


%\subsection{Constraint}
Assuming we have a piecewise description of $\FCC^{k}_{t}(\mu')$ on $I$ ordered intervals of $\mathbb{R}$
then it is straightforward to recover the function:
$\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}.$

The update rule is a priori valid for more complex constraints, typically quadratic constraints, yet recovering
$\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}$ from $\FCC^{k}_{t}(\mu')$ would possibly be much more difficult.


\section{Results}

\subsection{Accuracy on ChIP-seq data}

We ran the Segmentor, PeakSegDP, and coseg algorithms on the McGill
ChIP-seq benchmark data sets \citep{HOCKING2016-chipseq}. We
considered 2752 segmentation problems consisting of chromosome subsets
in samples with labels that indicate presence or absence of peaks. For
each segmentation problem we attempt to compute models with 0, ..., 9
peaks, so there are a total of 27520 possible models for each
algorithm. The PeakSegDP algorithm either recovers an up-down model or
no model at all. When the coseg and Segmentor algorithms recovered
models that did not obey the PeakSeg up-down constraints, we discarded
those infeasible models.

We show the number of models which are feasible for the PeakSeg
up-down constraint in Table~\ref{tab:min-train-error}. The PeakSegDP
package computed the most feasible models (27469/27520=99.8\%),
followed by the coseg package (21278/27520=77.3\%), and the Segmentor
package computed the fewest (8106/27520=29.4\%).

In terms of optimality, the cDPA (PeakSegDP R pkg) computes a
sub-optimal model for 7246/27520 = 26.3\% of models. For 1032/7246 of
these, the PeakSeg solution exists and is recovered by our new algo
(coseg R pkg) but not the unconstrained algo (Segmentor R pkg). These
results suggest that in ChIP-seq data sets, the new coseg algorithm is
more accurate than PeakSegDP, in terms of the Poisson
likelihood. Furthermore, these results suggest that coseg is more
useful than Segmentor, since there are many cases for which Segmentor
does not recover a model that verifies the \ref{PeakSeg} up-down
constraint on the segment means.
% Numbers come from figure-PDPA-cDPA-compare.R



To compute the accuracy of the algorithms in the benchmark data sets,
we quantified false positive and false negative peak detection error
rates using the labels that indicate presence or absence of peaks in
specific samples and genomic regions
\citep{HOCKING2016-chipseq}. Briefly, a false negative occurs when no
peak is predicted in a region with a positive label, and a false
positive occurs when a peak is predicted in a region with a negative
label.

Finally, we performed 4-fold cross-validation to estimate the test
error of each algorithm. For each of the 7 data sets, we randomly
assigned labeled data to one of four folds. For each fold, we treat it
as a test set, and train a model using all other folds. For the
optimal segmentation models (coseg, PeakSegDP, Segmentor), we select
the number of segments using an oracle penalty
\citep{cleynen2013segmentation}. For a given data set
$\mathbf y\in\ZZ_+^n$, let $\mathbf m^s$ for
$s\in\mathcal S\subseteq \{1, 3,\dots, 19\}$ be the segment mean
vectors that obey the PeakSeg up-down constraint. The oracle.1 model
selection criterion described by \citet{HOCKING-PeakSeg} is

\begin{equation}
  \label{eq:oracle}
  s^*(\lambda) = \argmin_{s\in\mathcal S}
  \lambda s\left(1 + 4\sqrt{1.1 + \log(n/s)}\right)^2
  +\sum_{i=1}^n \ell(y_i, m_i^s)
\end{equation}

We compute the ROC curve by varying the
$\lambda\in\{10^{-2}, \dots,10^4\}$ penalty parameter. The baseline
MACS algorithm was trained by varying the qvalue parameter between 0
and 0.8. The baseline HMCanBroad algorithm was trained by varying the
finalThreshold parameter between $10^{-10}$ and $10^5$. 

The previous algorithm with state-of-the-art accuracy on this
benchmark was PeakSegDP, and we observed that the new coseg algorithm
achieves comparable test AUC (Figure~\ref{fig:test-error-dots}). In
contrast, the unconstrained Segmentor algorithm has TODO

\begin{figure}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-test-error-dots}
  \vskip -0.5cm
  \caption{Four-fold cross-validation was used to estimate prediction
    accuracy of each algorithm in each of the 7 ChIP-seq data
    sets. Each black circle shows the test AUC in one of four
    cross-validation folds, the shaded grey circle is the mean, and
    the vertical line shows the maximum mean in each data set. It is
    clear that the new algorithm implemented in the coseg R package is
    just as accurate as the slower PeakSegDP heuristic, and both are
    more accurate than the other baseline methods. Interactive version
    available at
    \url{http://bl.ocks.org/tdhock/raw/886575874144c3b172ce6b7d7d770b9f/}}
  \label{fig:test-error-dots}
\end{figure}


\begin{table}[b!]
  \centering
  \input{table-min-train-error}
  \caption{Comparison of algorithms in the ChIP-seq data sets,
    in terms of minimum train error and number of feasible models. 
    For each of the 2752 separate segmentation problems, 
    each algorithm was run with several parameter values (see text for details), 
    and we selected the parameter with the minimum number of incorrect labels
    (errors = fp + fn). 
    The new algorithm implemented in the coseg R package 
    commits fewer false positives than the slower PeakSegDP heuristic, 
    and fewer errors than the other baseline methods.
    The new algorithm computed models that are feasible for the PeakSeg up-down constraint
    more frequently than the unconstrained Segmentor algo,
    but less frequently than the PeakSegDP algo.}
  \label{tab:min-train-error}
\end{table}

\subsection{Timings}

\begin{figure}[b!]
  \centering
  \parbox{0.49\textwidth}{
    %\includegraphics[width=0.45\textwidth]{figure-PDPA-intervals-all}
    %\input{figure-PDPA-intervals-small}
  }
  \parbox{0.49\textwidth}{
    %\includegraphics[width=0.45\textwidth]{figure-PDPA-timings}
    %\input{figure-PDPA-timings-small}
  }
  \caption{Timing results on 2752 segmentation problems from the
    McGill histone mark ChIP-seq benchmark data. For each problem we
    ran the PeakSegPDPA with $K_{\text{max}}=19$ segments.
    \textbf{Left}: number of intervals stored by the algorithm,
    \textbf{Right}: timings in seconds (median line and min/max
    band).}
  \label{fig:timings}
\end{figure}

\section{Discussion}


\section{Conclusions}



\bibliographystyle{abbrvnat}
\bibliography{refs}


\section{Appendix: PeakSeg model}
\label{sec:model}

In this section we first discuss the existing unconstrained maximum
likelihood model, and then we discuss a more general framework for
constrained maximum likelihood segmentation.

\subsection{Unconstrained maximum likelihood segmentation}

Assume we have a sequence of $n$ count data $\mathbf y\in\ZZ_+^n$ to
segment. For the Segment Neighborhood model we first fix a maximum
number of segments $ K_{\max}\leq n$. The unconstrained maximum
likelihood segmentation model is defined as the most likely mean
vector $\mathbf m\in\RR^n$ with exactly 
$K\in\{1, \dots, K_{\max}\}$ distinct piecewise constant segments:
\begin{align}
  \label{unconstrained}
  \mathbf{\hat m}^K(\mathbf y) =
    \argmin_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( y_t, m_t) 
\\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = K, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}
\nonumber
\end{align}
where the Poisson loss function is
\begin{equation}\label{eq:loss}
  \ell( y, m)= m - y \log m.
\end{equation} 
The $c_t = \sign(m_{t+1} - m_{t})$ variable is the sign of the change
after data point $t$. Every $t$ such that $c_t \neq 0$ is a
change-point, and the model complexity is the number of segments
$1+\sum_{t=1}^{n-1} I(c_t \neq 0)$, where $I$ is the indicator
function. 

We refer to (\ref{unconstrained}) as the ``unconstrained'' model
since $\mathbf{\hat m}^K(\mathbf y)$ is the most likely segmentation
of all possible models with $K$ piecewise constant segments ($K-1$
change-points). 
Although (\ref{unconstrained}) is a non-convex optimization problem,
the sequence of segmentations
$\mathbf{\hat m}^1(\mathbf y), \dots, \mathbf{\hat m}^{K}(\mathbf y)$
can be computed in $O(K n^2)$ time using the standard dynamic
programming algorithm \citep{bellman}, or in $O(K n \log n)$ time
using dynamic programming with functional pruning \citep{pruned-dp,
  johnson, Segmentor}.

\subsection{The PeakSeg problem (strict inequality constraints)}
\label{sec:constrained}

To introduce the PeakSeg model constraint \citep{HOCKING-PeakSeg}, we first define
the peak indicator for $t\in\{1, \dots, n-1\}$ as
\begin{equation}
  \label{eq:peaks}
  P_t(\mathbf c) = \sum_{i=1}^t c_i,
\end{equation}
which is the cumulative sum of change-point variables $c_i$ up to data
point $t$. In general for the unconstrained model
$P_t(\mathbf c)\in\ZZ$, which is problematic in our biological
application (ChIP-seq peak detection), since we want to classify each
segment and data point into one of two states
$P_t(\mathbf c)\in \{0, 1\}$ (0 for background noise after a change
down, 1 for a peak after a change up).
% For example, if
% $\mathbf m = \left[\begin{array}{ccccccc}1.1 & 1.1 & 2 & 2 & 4 & 4 &
%     3\end{array}\right]$, with two changes up followed by one change
% down, then
% $\mathbf P(\mathbf m) = \left[\begin{array}{ccccccc}0 & 0 & 1 & 1 & 2
%     & 2 & 1 \end{array}\right]$ and so this model is not feasible for
% the peaks constraint $P_t(\mathbf m)\in \{0, 1\}$.
Thus we constrain the peak indicator $P_t(\mathbf
c)\in\{0, 1\}$, which results
in the constrained problem
\begin{align}
  \label{PeakSeg}
  \mathbf{\tilde m}^K(\mathbf y) =
    \argmin_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell(y_t, m_t) 
\tag{\textbf{PeakSeg}}
\\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = K, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}
\nonumber\\
\forall t\in\{1, \dots, n-1\}, &\ \ P_t(\mathbf c) \in\{0, 1\}.
\nonumber
\end{align}
The only difference with the unconstrained problem
(\ref{unconstrained}) is that we have added the constraint
$P_t(\mathbf c) \in\{0, 1\}$. Another way to interpret the constrained
\ref{PeakSeg} problem is that the sequence of changes in the segment
means $\mathbf m$ must begin with a positive change and then
alternate: up, down, up, down, ... (and not up, up, down). Thus the
even-numbered segments may be interpreted as peaks $P_t(\mathbf c)=1$,
and the odd-numbered segments may be interpreted as background
$P_t(\mathbf c)=0$.

Note that in the both the unconstrained problem (\ref{unconstrained})
and the \ref{PeakSeg} problem, the change-point variable $\mathbf c$
can be removed, resulting in a simpler problem with only one
optimization variable (the segment mean $\mathbf m$). However, we
state the problem in this form in order to make an explicit connection
with the non-strict inequality constrained problem, which we introduce
in the next section.

\subsection{The non-strict inequality constrained problem}

Consider the following optimization problem, which replaces the strict
inequality constraints of the \ref{PeakSeg} problem with non-strict
inequality constraints.
\begin{align}
  \mathbf{\bar m}^K(\mathbf y) =
    \argmin_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell(y_t, m_t) 
       \label{PeakSegPDPA}
\\
    \text{subject to} &\ \  1+ \sum_{t=1}^{n-1} I(c_t \neq 0) = K, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} \geq m_{t+1} \text{ (change down or no change)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} \leq m_{t+1} \text{ (change up or no change)}
\nonumber\\
\forall t\in\{1, \dots, n-1\}, &\ \ P_t(\mathbf c) \in\{0, 1\}.
\nonumber
\end{align}
Thus the \ref{PeakSeg} model has exactly $K$ distinct
segment means, but this model (\ref{PeakSegPDPA}) has \emph{at most}
$K$ distinct segment means (some may be equal due to the non-strict
equality constraints). Note that for $K=4$, the solution
$(\mathbf m = \left[\begin{array}{cccc}1 & 2 & 2 & 3 \end{array}\right],
  \mathbf c = \left[\begin{array}{ccc}  1 & -1 & 1 \end{array}\right])$
is feasible for this problem (\ref{PeakSegPDPA}), 
but is not feasible for \ref{PeakSeg} (since $c_2=-1$ but $m_2 = m_3$).

Note that unlike the unconstrained model, increasing the number of
non-zero change variables ($c_t\neq 0$) does not always decrease the
cost. For example, consider the data set
$\mathbf y = \left[\begin{array}{ccc}1 & 3 & 9 \end{array}\right]$.
For $K=2$ segments both models have the same optimal solution,
$(\mathbf m = \left[\begin{array}{ccc}2 & 2 & 9 \end{array}\right],
  \mathbf c = \left[\begin{array}{cc}  0 & 1 \end{array}\right])$,
which has a Poisson loss of $\approx -9.6$. 
For $K=3$ segments the solution of unconstrained model
(\ref{unconstrained}) has a smaller loss of $\approx -10.1$
$(\mathbf m = \left[\begin{array}{ccc}1 & 3 & 9 \end{array}\right],
  \mathbf c = \left[\begin{array}{cc}  1 & 1 \end{array}\right])$,
but the solution of the constrained model (\ref{PeakSegPDPA})
has a larger loss of $\approx -8.5$
$(\mathbf m = \left[\begin{array}{ccc}1 & 6 & 6 \end{array}\right],
  \mathbf c = \left[\begin{array}{cc}  1 & -1  \end{array}\right])$. 
Note that the \ref{PeakSeg} solution with $K=3$ is undefined for this data set.

More generally, we have the following result which relates the
solutions of the \ref{PeakSeg} and the non-strict inequality
constrained problems.

\begin{proposition}
  Let $(\mathbf m, \mathbf c)$ be the solution to
  (\ref{PeakSegPDPA}). If $c_t=0 \Leftrightarrow m_t = m_{t+1}$ for
  all $t\in\{1, \dots, n-1\}$ then $(\mathbf m, \mathbf c)$ is also
  the solution to \ref{PeakSeg}. Otherwise, the \ref{PeakSeg} solution
  is undefined.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\subsection{Segment Neighborhood version}

For the Segment Neighborhood algorithm we begin as usual by computing
a functional representation of the optimal cost in 1 segment up to
data point $t$. 
\begin{equation*}
  \label{eq:C1b}
  \FCC_{1,t}(\mu) = \sum_{i=1}^t \gamma_i(\mu),
\end{equation*}
where $\gamma_t(\mu)=\ell(y_t, \mu)$ is the cost of using the mean
$\mu$ for single data point $t$ (for example the Gaussian or Poisson
loss).

Next we define the minimum cost in 2 segments up to data point 2 as
\begin{equation*}
  \label{eq:C22}
  \FCC_{2,2}(\mu) = \FCC_{1,1}^{\leq}(\mu) + \gamma_2(\mu),
\end{equation*}
where for a function $f:\RR\rightarrow\RR$ the min-less operator
yields another function $f\leq:\RR\rightarrow\RR$ such that
\begin{equation}
  \label{eq:min-less}
  f^{\leq}(\mu) = \min_{x\leq \mu} f(x).
\end{equation}
The algorithm relies on the ability to compute an exact representation
of functions such as $C_{1,1}^{\leq}$
(Figure~\ref{fig:min-operators}). Since the cost functions $C_{1,t}$
are convex, we can easily find the minimum $\mu_t^*$, and then compute
the following exact representation
\begin{equation*}
  \FCC_{1,t}^\leq(\mu)=
  \begin{cases}
    \FCC_{1,t}(\mu_t^*) & \text{ if } \mu \geq \mu_t^*,\\
    \FCC_{1,t}(\mu) & \text{ otherwise.}
  \end{cases}
\end{equation*}

\begin{figure}[!t]
  \parbox{3in}{
    \begin{center}
      %\input{figure-1-min-more-operator}
    \end{center}
  }
  \parbox{3in}{
    \begin{center}
      %\input{figure-1-min-less-operator}
    \end{center}
  }
  \caption{\label{fig:min-operators} \textbf{Left:} The min-more
    operator is $C^{\geq}(\mu)=\min_{x\geq \mu}C(x)$. \textbf{Right:}
    The min-less operator is $C^{\leq}(\mu)=\min_{x\leq
      \mu}C(x)$.}
\end{figure}

The next step is to compute the minimum cost in 2 segments up to data
point 3, for which there is a choice of two change-points.
\begin{equation*}
  \FCC_{2,3}(\mu) = \gamma_3(\mu)+\min
  \begin{cases}
    \FCC_{2,2}(\mu) \\
    \FCC_{1,2}^{\leq}(\mu)
  \end{cases}
\end{equation*}
We have already computed an exact representation of the $C_{2,2}$
term, which is the cost a change after the first data point. Now we
need to compare it with the $C_{1,2}^{\leq}$ term, which is the cost
of a change after the second data point. This is a crucial step in
which the \texttt{MinEnvelope} sub-routine computes an exact
representation of the minimum of these two functions
(Figure~\ref{fig:min-envelope}).

\begin{figure}[!t]
  \begin{center}
    %\input{figure-2-min-envelope}
  \end{center}
  \caption{\label{fig:min-envelope} The cost $C_{s,t}$ in $s$ segments
    up to $t$ data points is computed using the min envelope
    $M_{s,t-1}$. \textbf{Left:} the min envelope for $s=3$ segments up
    to data point $t=34$ is the minimum of two functions:
    $C^{\geq}_{2,34}$ is the cost if the second segment ends at data
    point $t=34$, and $C_{3,34}$ is the cost if the second segment
    ends before that. \textbf{Middle:} the optimal cost for $s=3$
    segments up to data point $t=35$ is the sum of the min envelope
    $M_{3,34}$ and the cost of the next data point
    $\gamma_{35}$. \textbf{Right:} in the next step, the
    algorithm prunes all previously considered change-points (cost
    $C_{3,35}$), and only considers the model with a the second segment
    ending at data point $t=35$ (cost $C^{\geq}_{2,35}$).}
\end{figure}

The updates continue for every data point $t\in\{3, ..., n\}$
\begin{equation*}
  \FCC_{2,t}(\mu) =  \gamma_t(\mu) + \min
  \begin{cases}
    \FCC_{2,t-1}(\mu),\\
    \FCC_{1,t-1}^{\leq}(\mu) 
  \end{cases}
\end{equation*}

For the third segment we first compute the minimum cost up to data point 3
\begin{equation*}
  \FCC_{3,3}(\mu) = \FCC_{2,2}^{\geq}(\mu) + \gamma_3(\mu),
\end{equation*}
where the more-min operator $f^\geq$ is defined analogously to the
min-less operator (Figure~\ref{fig:min-operators}). The update formula
for the minimum cost up to data point $t\in\{4, ..., n\}$ is
\begin{equation*}
  \FCC_{3,t}(\mu) = \gamma_t(\mu) + \min
  \begin{cases}
    \FCC_{3,t-1}(\mu),\\
    \FCC_{2,t-1}^{\geq}(\mu)
  \end{cases}
\end{equation*}
In general for $s$ segments, we use
\begin{equation}
  \FCC_{s,s}(\mu) = \FCC_{s-1,s-1}^{*}(\mu) + \gamma_s(\mu),
\end{equation}
and for $t\in\{s+1, ..., n\}$
\begin{equation}
  \FCC_{s,t}(\mu) = \gamma_t(\mu)+\min
  \begin{cases}
    \FCC_{s,t-1}(\mu)\\
    \FCC_{s-1,t-1}^{*}(\mu)
  \end{cases}
\end{equation}
where * means less-min for even-numbered segments $s$, and more-min
for odd-numbered segments.

\subsection{Optimal Partitioning}

In real ChIP-seq data sets we are only interested in the \ref{PeakSeg}
solutions
$\mathbf{\tilde m}^1(\mathbf y), \mathbf{\tilde m}^3(\mathbf y), \dots$
which start with a change up, and end with a change down. To select
one of those segmentations we typically use the following criterion
\begin{equation}
  \label{eq:selection}
  K^*_\lambda = \argmin_{k\in 1,3, \dots}
  \lambda k/2 +
\sum_{t=1}^n
  \ell[ y_t, \tilde m_t^k(\mathbf y) ],
\end{equation}
where $\lambda\in\RR_+$ is a non-negative penalty parameter 
that controls the optimal number of segments $K^*_\lambda$
(larger $\lambda$ yields fewer segments).
When the number of segments is very large $K =O(N)$, 
it computationally expensive to recover this model
using the $O(K N\log N)$ PeakSegPDPA, 
since it computes all models from 1 to $K$ segments.

It would be computationally faster if we could recover the model
$\mathbf{\tilde m}^{K}(\mathbf y)$ by itself, as in the $O(N\log N)$
FPOP algorithm \citep{johnson, FPOP}. This motivates the following
optimization problem,
\begin{align}
  \mathbf{\bar m}^\lambda(\mathbf y) =
    \argmin_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
\lambda\sum_{t=1}^{n-1} I(c_t =1)
+
\sum_{t=1}^n \ell(y_t, m_t) 
  \label{PeakSegFPOP}
\\
    \text{subject to} 
& \ \ c_t = -1 \Rightarrow m_{t} \geq m_{t+1} \text{ (change down or no change)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} \leq m_{t+1} \text{ (change up or no change)}
\nonumber\\
\forall t\in\{1, \dots, n-1\}, &\ \ P_t(\mathbf c) \in\{0, 1\}.
\nonumber
\end{align}

We refer to problem (\ref{PeakSegFPOP}) as the penalized version of
the constrained problem (\ref{PeakSegPDPA}). The constraint on the
number of non-zero change variables $c_t\neq 0$ has been removed, and
term penalizing the number of up changes $c_t=1$ has been added to the
optimization objective.  
%The solution to this problem is computed by the PeakSegFPOP function of the coseg R package.

\begin{proposition}
  For any data set $\mathbf y\in\ZZ_+^n$ and any penalty parameter
  $\lambda\in\RR_+$, let $(\mathbf m,\mathbf c)$ be the solution to
  the penalized problem (\ref{PeakSegFPOP}). It is the same as the
  solution to the contrained problem (\ref{PeakSegPDPA}) with
  $K=\sum_{t=1}^{n-1} c_t$ segments.
\end{proposition}

\begin{proof}
  TODO
\end{proof}

\subsection{Implementation details}
\label{sec:implementation-details}
TODO discuss intervals, pseudocode, mean, weights, root finding that
were used for implementing the PeakSeg model.

\begin{itemize}
\item $\text{GetCost}(p, u)$: an algorithm that takes a FunctionPiece
  object $p$, and a mean value $u$, and computes the cost at $u$. For
  a square loss FunctionPiece $p$ with coefficients $a,b,c\in\RR$, we
  have $\text{GetCost}(p,u)=au^2+bu+c$.
\item $\text{OptimalMean}(p)$: an algorithm that takes one
  FunctionPiece object, and computes the optimal mean value. For a
  square loss FunctionPiece $p$ we have
  $\text{OptimalMean}(p)=-b/(2a)$.
\item $\text{ComputeRoots}(p, d)$: an algorithm that takes one
  FunctionPiece object, and computes the solutions to $p(u)=d$. For
  the square loss we propose to use the quadratic formula. For other
  convex losses that do not have closed form expressions for their
  roots, we propose to use Newton's root finding method. Note that for
  some constants $d$ there are no roots, and the algorithm needs to
  report that.
\end{itemize}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Input: The previous segment end $t_{\text{prev}}$ (an integer), 
 and $f_{\text{in}}$ (a FunctionPieceList).
\STATE Output: FunctionPieceList $f_{\text{out}}$, initialized as an empty list.
\STATE $\text{prev\_cost} \gets\infty$
\STATE $\text{new\_lower\_limit}\gets \text{LowerLimit}(f_{\text{in}}[0])$.
\STATE $i\gets 0$; // start at FunctionPiece on the left
\STATE while $i < $ Length($f_{\text{in}}$): // continue until FunctionPiece on the right
\begin{ALC@g}
  \STATE FunctionPiece $p\gets f_{\text{in}}[i]$
  \STATE if prev\_cost = $\infty$: // look for min in this interval.
  \begin{ALC@g}
    \STATE $\text{candidate\_mean}\gets \text{OptimalMean}(p)$ 
    \STATE if $\text{LowerLimit}(p)< \text{candidate\_mean} < \text{UpperLimit}(p)$:
    \begin{ALC@g}
      \STATE $\text{new\_upper\_limit}\gets \text{candidate\_mean}$ // Minimum found in this interval.
      \STATE $\text{prev\_cost}\gets \text{GetCost}(p, \text{candidate\_mean})$
      \STATE $\text{prev\_mean}\gets \text{candidate\_mean}$
    \end{ALC@g}
    \STATE else: // No minimum in this interval.
    \begin{ALC@g}
      \STATE 
      $\text{new\_upper\_limit}\gets
 \text{UpperLimit}(p)$
    \end{ALC@g}
    \STATE $f_{\text{out}}\text{.push\_piece}(\text{new\_lower\_limit},\text{new\_upper\_limit},p,\infty)$
    \STATE $\text{new\_lower\_limit}\gets\text{new\_upper\_limit}$
    \STATE $i\gets i+1$
  \end{ALC@g}
  \STATE else: // look for equality of $p$ and prev\_cost
  \begin{ALC@g}
    \STATE $(\text{small\_root},\text{large\_root})\gets\text{ComputeRoots}(p, \text{prev\_cost})$
    \STATE if $\text{LowerLimit}(p) < \text{small\_root} < \text{UpperLimit}(p)$:
    \begin{ALC@g}
      \STATE $f_{\text{out}}\text{.push\_piece}(
      \text{new\_lower\_limit}, 
      \text{small\_root}, 
      \text{ConstPiece}(\text{prev\_cost}), 
      \text{prev\_mean})$
      \STATE $\text{new\_lower\_limit}\gets \text{small\_root}$
      \STATE $\text{prev\_cost}\gets \infty$ 
    \end{ALC@g}
    \STATE else: // no equality in this interval
    \begin{ALC@g}
      \STATE $i\gets i+1$ // continue to next FunctionPiece
    \end{ALC@g}
  \end{ALC@g}
\end{ALC@g}
  \STATE if $\text{prev\_cost} < \infty$: // ending on constant piece
  \begin{ALC@g}
    \STATE $f_{\text{out}}\text{.push\_piece}(
    \text{new\_lower\_limit}, 
    \text{UpperLimit}(p), 
    \text{ConstPiece}(\text{prev\_cost}), 
    \text{prev\_mean})$
  \end{ALC@g}
\STATE Set all previous segment end $t'=t_{\text{prev}}$  for all FunctionPieces in $f_{\text{out}}$
\caption{\label{algo:minless}MinLess algorithm.}
\end{algorithmic}
\end{algorithm}

Consider Algorithm~\ref{algo:minless} which contains pseudocode for
the computation of the min-less operator. The algorithm follows the
convex function pieces from left to right until finding a
minimum. Then TODO discuss lines.

\subsection{Results on very large data sets}

TODO: Empirical time and memory analysis FPOP vs PeakSegFPOP on really
large chromosome subsets $N>10^6$.

\end{document}

