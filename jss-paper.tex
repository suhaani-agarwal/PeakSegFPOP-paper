\documentclass[article]{jss}

\newcommand{\R}{\proglang{R}}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{Ckt}{HTML}{E41A1C}
\definecolor{Min}{HTML}{4D4D4D}%grey30
%{B3B3B3}%grey70
\definecolor{MinMore}{HTML}{377EB8}
\definecolor{Data}{HTML}{984EA3}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2016} 
%\usepackage{fullpage}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}


%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking\\Northern Arizona University
   \And Guillem Rigaill\\INRA
   \And Paul Fearnhead\\Lancaster University
   \And Guillaume Bourque\\McGill University}
\Plainauthor{Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque} 

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{A disk-based functional pruning algorithm for optimal changepoint
  detection in genomic data} 

\Plaintitle{A disk-based functional pruning algorithm for optimal changepoint
  detection in large genomic data} 

%\Shorttitle{A Short Demo Article in \proglang{R}}
\Shorttitle{Disk-based optimal changepoint detection in large data}

%% - \Abstract{} almost as usual
\Abstract{ This article describes a new algorithm and \proglang{R}
  package for optimal changepoint detection in large genomic data
  sets. Previous packages have used in-memory implementations, which
  limit application to relatively small data sets. The proposed
  PeakSegPipeline package implements disk-based storage in order to
  compute optimal changepoint models for the large data sets which are
  widespread in genomics.}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, \proglang{R}} 

\Plainkeywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, R} 

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  Northern Arizona University\\
  School of Informatics, Computing, and Cyber Systems\\
  Flagstaff, AZ, USA\\
  E-mail: \email{Toby.Hocking@R-project.org}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section[Introduction: optimal changepoint detection in R]{Introduction: optimal changepoint detection in \proglang{R}} \label{sec:intro}

\subsection{Models with no constraints between segment means}

Several dynamic programming algorithms for inferring optimal multiple
changepoint models have been proposed in recent years. Such algorithms
seek to compute the optimal model with $S$ segments ($S-1$
changepoints) for a sequence of $N$ data, $z_1,\dots,z_N$. The
optimization variables are the $S-1$ discrete changepoints (which take
integer values from 1 to $N-1$), and the $S$ segment means (typically
real-valued). The goal of optimal changepoint algorithms is to find
the arrangement of changepoints and segment means which maximizes a
likelihood function (or equivalently minimizes a loss
function). Optimizing by naively computing the best mean and
likelihood for each of the $O(N^{S-1})$ possible arrangements of
changepoints is an exponential $O(N^S)$ time operation.

The classical dynamic programming algorithms have time complexity
which is quadratic in the number of data $N$. The ``Segment
Neighborhood'' algorithm computes a sequence of optimal models from 1
to $S$ segments, in $O(SN^2)$ time \citet{segment-neighborhood}. The
$O(N^2)$ ``Optimal Partitioning'' algorithm inputs a non-negative
penalty parameter $\lambda\in RR_+$, and yields one of the optimal
changepoint models \citet{optimal-partitioning}. Although optimal
partitioning is faster by a factor of $O(S)$, the user is unable to
directly choose the number of segments $S$. The Segment Neighborhood
algorithm is available in \R\ as
\verb|changepoint::cpt.mean(method="SegNeigh")|.

\citet{pruned-dp} proposed a Pruned Dynamic Programming Algorithm
(PDPA) for solving the Segment Neighborhood problem in faster
$O(S N\log N)$ time. The original implementation of the PDPA was
available in \R\ as \verb|cghseg:::segmeanCO| for the Normal
homoscedastic model. More recently, \citet{cleynen2013segmentation}
described a generalization of the PDPA for other likelihood/loss
functions (Poisson, negative binomial, Normal heteroscedastic). These
are available in R as \verb|Segmentor3IsBack::Segmentor|.

\citet{fpop} proposed a Functional Pruning Optimal Partitioning (FPOP)
algorithm for solving the Optimal Partitioning problem in $O(N\log N)$
time. The FPOP algorithm is available in R as \verb|fpop::Fpop|.

TODO table with unconstrained algos.
% \begin{table*}[t!]
%   \centering
%   \begin{tabular}{r|c|c}
%     Constraint & No pruning & Functional pruning \\
%     \hline
%     None & Dynamic Prog. Algo. (DPA) & Pruned DPA (PDPA) \\
%     & Optimal, $O(Kn^2)$ time & Optimal, $O(Kn\log n)$ time\\
%     % & \citet{segment-neighborhood} & \\
%     % & \citet{optimal-partitioning} & \\
%     & \citet{segment-neighborhood}     & \citet{pruned-dp, phd-johnson} \\
%     \hline
%     Up-down & Constrained DPA (CDPA) & Generalized Pruned DPA (GPDPA) \\
%     & Sub-optimal, $O(Kn^2)$ time & Optimal, $O(Kn\log n)$ time\\
%     & \citet{HOCKING-PeakSeg} & \textbf{This paper} \\
%     \hline
%   \end{tabular}
%   \caption{Our contribution is 
% the Generalized Pruned Dynamic Programming Algorithm (GPDPA), 
%  which uses a functional pruning technique 
%     to compute the constrained optimal $K-1$ changepoints 
% in a sequence of $n$ data. 
% Time complexity is on average, 
% in our empirical tests on real ChIP-seq data sets.}
% \label{tab:contribution}
% \end{table*}

% Oracle penalties \citep{cleynen2013segmentation} or learned penalties
% \citep{HOCKING-penalties} can be used to select the number of segments
% $K$.  Because this model sometimes has several consecutive up changes,
% it is non-trivial to interpret in terms of peaks and background
% (Figure~\ref{fig:data-models}, top).

\subsection{Models with constraints between adjacent segment means}

% Hidden Markov Models (HMMs) with common mean parameters for the peak
% or background regions could be used to model such sequence data, but
% we do not explore them in this paper for two reasons. First, we have
% observed in real ChIP-seq data that background and peak means are not
% constant throughout the genome (Supplementary Figure 4), so shared
% mean parameters would not be a good fit. Second, inference algorithms
% for HMMs are only guaranteed to find a local maximum of the
% likelihood; we are more interested in changepoint detection models
% with dynamic programming algorithms that can provably compute the
% global maximum of the corresponding likelihood.

The models discussed above are unconstrained in the sense that there
are no constraints between segment means. However some data need to be
interpreted in terms of pre-defined domain-specific states. For
example in genomic data such as ChIP-seq \citep{chip-seq}, the
changepoint model needs to be interpreted in terms of peaks (large
values which represent protein binding) and background (small values
which represent background noise). 

In this context, \citet{HOCKING-PeakSeg} introduced a Constrained
Dynamic Programming Algorithm (CDPA) for computing a model where up
changes are followed by down changes, and vice versa. These
constraints ensure that odd-numbered segments can be interpreted as
background, and even-numbered segments can be interpreted as peaks.
In a recent comparison study, \citet{HOCKING2016-chipseq} showed that
the CDPA achieves state-of-the-art peak detection accuracy in a
benchmark of several ChIP-seq data sets.

\begin{figure}[t!]
\centering
\includegraphics{jss-figure-more-likely-models-three-peaks}
\caption{\label{fig:three-peaks} One ChIP-seq data set with three peak
  models. \textbf{Top:} the peak model from the baseline MACS2
  algorithm detected five peaks. \textbf{Middle:} the most likely
  model with five peaks contains one equality constraint between
  segment means, which suggests that five peaks is overfitting for
  these data. \textbf{Bottom:} the most likely model with three peaks
  is also more likely than the MACS2 model, and more interpretable
  (fewer small peaks).}
\end{figure}
 
The size of these aligned read count data depends on the size of the
chromosomes in the reference genome. For example, the largest
chromosome in the human genome (hg19) is chr1, which has 249,250,621
bases (distinct positions at which the number of aligned reads is
measured). Analysis of such data thus requires computationally
efficient algorithms which scale to data sets of arbitrarily large
size.

% The approach taken by the PeakSeg algorithm can be formalised as follows. For any given number, $K$,
% of segments we try to find the best segmentation. We define how good a segmentation is by fitting
% a constant mean for each segment of the chromosome, subject to the up-down constraint that
% the segment means alternate between increasing and decreasing. Subject to this constraint we find the
% best choice of segment means to minimize some loss function that measures how close the mean is to each 
% observation. The cost of a segmentation is defined to be the resulting value of this loss summed over 
% all observations. For a sequence of $n$ data points, PeakSeg attempts to search over the $O(n^{K-1})$ 
% possible segmentations to find the one that minimizes this cost. 

The PeakSegOptimal package provides an in-memory implementation of the
up-down constrained model \citep{Hocking-constrained-changepoint-detection}.

%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\section{Models and software} \label{sec:models}

The optimization problem that our algorithm solves is
\begin{align*}
  \label{eq:penalized_peakseg}
  \minimize_{
  \substack{
  \mathbf m\in\RR^N,\ \mathbf s\in\{0, 1\}^N\\
  \mathbf c\in\{-1, 0,1\}^{N-1}\\
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(c_i \neq 0) \\
  \text{subject to\ \ } &\ \text{no change: }c_t = 0 \Rightarrow m_t = m_{t+1}\text{ and }s_t=s_{t+1}
                          \nonumber\\
    &\ \text{go up: }c_t = 1 \Rightarrow m_t \leq m_{t+1}\text{ and }(s_t,s_{t+1})=(0,1),
      \nonumber\\
    &\ \text{go down: } c_t = -1 \Rightarrow m_t \geq m_{t+1}\text{ and }(s_t,s_{t+1})=(1,0).
      \nonumber
\end{align*}

It is implemented in \R\ as
\verb|PeakSegPipeline::PeakSegFPOP_disk|. Its complexity is
$O(N\log N)$ time, $O(N\log N)$ disk, $O(\log N)$ memory.

\begin{figure}
  \centering
  \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2cm,
    thick,main node/.style={circle,draw}]

    \node[main node] (1) {$s=1$};
    \node[main node] (2) [below of=1] {$s=0$};

    \path[every node/.style={font=\sffamily\small}]
    (2) edge [bend left] node {$c=1, \leq, \lambda$} (1)
    (1) edge [bend left] node {$c=-1, \geq, \lambda$} (2);
  \end{tikzpicture}
  \caption{State graph for PeakSeg model.}
  \label{fig:state-graph}
\end{figure}

The algorithm recursively computes two vectors of real-valued cost
functions:
$$
\begin{array}{cccl}
  \overline C_{1}(m_1) & \cdots & \overline C_N(m_{N})& \text{ optimal cost in peak state $s=1$}\\
  \underline C_{1}(m_1) & \cdots & \underline C_N(m_{N})& \text{ optimal cost in background state $s=0$}\\
\end{array}
$$
using the update rules:
  $$\overline C_{t+1}(\mu) = \ell(\mu, z_i) + \min\{
  \overline C_t(\mu),\, 
  \underline C_t^\leq(\mu)+\lambda
\},$$
  $$\underline C_{t+1}(\mu) = \ell(\mu, z_i) + \min\{
  \underline C_t(\mu),\, 
  \overline C_t^\geq(\mu)+\lambda
\},$$
where $f^\leq(\mu) = \min_{x\leq\mu} f(x)$,\\
$f^\geq(\mu) = \min_{x\geq\mu} f(x)$.

  \begin{figure}
    \centering
    \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=3cm,
      thick,main node/.style={circle,draw}]
      \node[main node] (peak_t) {$\overline C_t$};
      \node[main node] (bkg_t) [below of=peak_t] {$\underline C_t$};
      \node[main node] (peak_t1) [right of=peak_t] {$\overline C_{t+1}$};
      \node[main node] (bkg_t1) [right of=bkg_t] {$\underline C_{t+1}$};
      \path[every node/.style={font=\small}]
      (peak_t) edge [dotted] node {$\overline C_{t}$} (peak_t1)
      (peak_t) edge [black, bend right] node [right] {$\overline C_{t}^{\geq}+\lambda$} (bkg_t1)
      (bkg_t) edge [dotted] node[midway, below] {$\underline C_{t}$} (bkg_t1)
      (bkg_t) edge [black, bend left] node[right] {$\underline C_{ t}^{\leq}+\lambda$} (peak_t1)
      ;
    \end{tikzpicture}
    \caption{Computation graph for up-down constrained model}
    \label{fig:computation-graph}
  \end{figure}


\begin{algorithm}
    \begin{algorithmic}[1]
  \STATE Input: data $\mathbf z\in\RR^N$, target peaks $P^*$.
  \STATE $\overline L,\overline p \gets \text{GFPOP}(\mathbf z, \lambda=0)$ // max peak model
  \STATE $\underline L,\underline p \gets \text{GFPOP}(\mathbf z, \lambda=\infty)$ // 0 peak model
  \STATE While $\underline p\neq P^*$ and $\overline p\neq P^*$:
  \begin{ALC@g}
    \STATE $\lambda=(\overline L-\underline L)/(\underline p-\overline p)$
    \STATE $L_{\text{new}},p_{\text{new}}\gets\text{GFPOP}(\mathbf z, \lambda)$
    \STATE If $p_{\text{new}}\in\{\underline p, \overline p\}$: return model with $\underline p$ peaks.
    \STATE If $p_{\text{new}} < P^*$: $\underline p\gets p_{\text{new}}$
    \STATE Else: $\overline p\gets p_{\text{new}}$
% > prob.i <- 3
% > fit.list$others[order(iteration)][, list(target.peaks=prob$peaks, iteration, under, over, penalty, peaks, total.cost)]
%    target.peaks iteration under over    penalty peaks total.cost
% 1:           33         1    NA   NA     0.0000  7487 -201361.96
% 2:           33         1    NA   NA        Inf     0  920923.98
% 3:           33         2     0 7487   149.8979   753  -24385.02
% 4:           33         3     0  753  1255.3904    47  153676.28
% 5:           33         4     0   47 16324.4191    10  310043.81
% 6:           33         5    10   47  4226.1495    21  214200.02
% 7:           33         6    21   47  2327.8360    33  177484.99
% > 
  \end{ALC@g}
  \STATE If $\underline p=P^*$: return model with $\underline p$ peaks.
  \STATE Else: return model with $\overline p$ peaks.
  \end{algorithmic}
\caption{\label{algo:seq-search}Sequential search for $P^*$ peaks using GFPOP.}
\end{algorithm}

Algorithm~\ref{algo:seq-search} (sequential search for $P^*$ peaks)
uses GFPOP as a sub-routine to find the optimal model with $P^*$
peaks. Its complexity is $O(N(\log N)^2)$ time, $O(N\log N)$ space,
$O(\log N)$ memory.

% \begin{leftbar}
% Note that around the \verb|{equation}| above there should be no spaces (avoided
% in the {\LaTeX} code by \verb|%| lines) so that ``normal'' spacing is used and
% not a new paragraph started.
% \end{leftbar}

% \proglang{R} provides a very flexible implementation of the general GLM
% framework in the function \fct{glm} \citep{Chambers+Hastie:1992} in the
% \pkg{stats} package. Its most important arguments are
% \begin{Code}
% glm(formula, data, subset, na.action, weights, offset,
%   family = gaussian, start = NULL, control = glm.control(...),
%   model = TRUE, y = TRUE, x = FALSE, ...)
% \end{Code}
% where \code{formula} plus \code{data} is the now standard way of specifying
% regression relationships in \proglang{R}/\proglang{S} introduced in
% \cite{Chambers+Hastie:1992}. The remaining arguments in the first line
% (\code{subset}, \code{na.action}, \code{weights}, and \code{offset}) are also
% standard  for setting up formula-based regression models in
% \proglang{R}/\proglang{S}. The arguments in the second line control aspects
% specific to GLMs while the arguments in the last line specify which components
% are returned in the fitted model object (of class \class{glm} which inherits
% from \class{lm}). For further arguments to \fct{glm} (including alternative
% specifications of starting values) see \code{?glm}. For estimating a Poisson
% model \code{family = poisson} has to be specified.

% \begin{leftbar}
% As the synopsis above is a code listing that is not meant to be executed,
% one can use either the dedicated \verb|{Code}| environment or a simple
% \verb|{verbatim}| environment for this. Again, spaces before and after should be
% avoided.

% Finally, there might be a reference to a \verb|{table}| such as
% Table~\ref{tab:overview}. Usually, these are placed at the top of the page
% (\verb|[t!]|), centered (\verb|\centering|), with a caption below the table,
% column headers and captions in sentence style, and if possible avoiding vertical
% lines.
% \end{leftbar}

% \begin{table}[t!]
% \centering
% \begin{tabular}{lllp{7.4cm}}
% \hline
% Type           & Distribution & Method   & Description \\ \hline
% GLM            & Poisson      & ML       & Poisson regression: classical GLM,
%                                            estimated by maximum likelihood (ML) \\
%                &              & Quasi    & ``Quasi-Poisson regression'':
%                                            same mean function, estimated by
%                                            quasi-ML (QML) or equivalently
%                                            generalized estimating equations (GEE),
%                                            inference adjustment via estimated
%                                            dispersion parameter \\
%                &              & Adjusted & ``Adjusted Poisson regression'':
%                                            same mean function, estimated by
%                                            QML/GEE, inference adjustment via
%                                            sandwich covariances\\
%                & NB           & ML       & NB regression: extended GLM,
%                                            estimated by ML including additional
%                                            shape parameter \\ \hline
% Zero-augmented & Poisson      & ML       & Zero-inflated Poisson (ZIP),
%                                            hurdle Poisson \\
%                & NB           & ML       & Zero-inflated NB (ZINB),
%                                            hurdle NB \\ \hline
% \end{tabular}
% \caption{\label{tab:overview} Overview of various count regression models. The
% table is usually placed at the top of the page (\texttt{[t!]}), centered
% (\texttt{centering}), has a caption below the table, column headers and captions
% are in sentence style, and if possible vertical lines should be avoided.}
% \end{table}


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

In one data set, MACS2 detected five peaks, and our GFPOP
algorithm can be used to find a more likely model with three peaks
(Figure~\ref{fig:three-peaks}). TODO

% \begin{leftbar}
% For code input and output, the style files provide dedicated environments.
% Either the ``agnostic'' \verb|{CodeInput}| and \verb|{CodeOutput}| can be used
% or, equivalently, the environments \verb|{Sinput}| and \verb|{Soutput}| as
% produced by \fct{Sweave} or \pkg{knitr} when using the \code{render_sweave()}
% hook. Please make sure that all code is properly spaced, e.g., using
% \code{y = a + b * x} and \emph{not} \code{y=a+b*x}. Moreover, code input should
% use ``the usual'' command prompt in the respective software system. For
% \proglang{R} code, the prompt \code{"R> "} should be used with \code{"+  "} as
% the continuation prompt. Generally, comments within the code chunks should be
% avoided -- and made in the regular {\LaTeX} text instead. Finally, empty lines
% before and after code input/output should be avoided (see above).
% \end{leftbar}

\begin{figure}[t!]
\centering
\includegraphics{jss-figure-label-error}
\caption{\label{fig:label-error} Labels are used to compute an error
  rate for each peak model (blue bars), defined as the sum of false
  positive and false negative labels (rectangles with black
  outline). This H3K36me3 ChIP-seq data set has $N=1,254,751$ data to
  segment on a subset of chr12, but in the plot above we show only the
  82,233 data (grey signal) in the region around the labels (colored
  rectangles). The model with penalty=6682 results in 320 peaks, which
  is too many (three false positive labels with more than one peak
  start/end). Conversely, the model with penalty=278653 results in 33
  peaks, which is too few (only two peaks in the plotted region,
  resulting in two false negative labels on the right where there
  should be exactly one peak start/end). The range of penalties between 9586 and
  267277 results in models with between 34 and 236 peaks, and achieves
  zero label errors. }
\end{figure}

\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-data-peaks}
\input{jss-figure-data-peaks}
\caption{\label{fig:data-peaks} The model with minimal label
  errors has $O(\sqrt{N})$ peaks in a data set of size $N$. For each data set we
  computed peak models with minimal label errors (see
  Figure~\ref{fig:label-error}); we then plot the number of peaks in
  minimal error models as a function of data set size $N$. Black median line
  and grey quartile band computed over several data sets of a given size
  $N$; asymptotic reference lines shown in red.}
\end{figure}
  
% \begin{figure}[t!] 
% \centering
% \includegraphics{jss-figure-more-likely-models-one-peak}
% \caption{\label{fig:one-peak} Default peak model from the baseline
%   MACS2 algorithm (top) and the optimal peak model computed using the
%   proposed PeakSegFPOP algorithm (bottom).}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-penalty}
% \caption{\label{fig:target-intervals-models-penalty} The PeakSegFPOP
%   algorithm was used to compute optimal models for six of the largest
%   problems (panels from left to right) for a variety of penalty
%   parameters (x-axis). Y-axes show empirical measurements of cost
%   function pieces (intervals, top panel), timings (minutes, middle
%   panel), and disk usage (gigabytes, bottom panel).}
% \end{figure}

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed-penalty}
\end{minipage}
\caption{\label{fig:disk-memory-compare-speed} The disk-based storage
  method is only a constant factor slower than the memory-based
  method. We benchmarked both methods on several small data sets
  ($N\leq 462,890$) for which optimal models could be computed using
  1GB of storage. \textbf{Left:} computation time is empirically
  $O(N\log N)$ for both storage methods, but the disk-based method is
  slower by a constant factor. Median line and quartile band computed
  over several penalty values for a given data set; black vertical
  line segment highlights the data set with $N=106,569$ used in the
  right panel. \textbf{Right:} computation time increases with penalty
  value $\lambda$ for both methods, with the disk-based method slower
  by a constant factor.}
\end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-all}
% \caption{\label{fig:target-intervals-models-all} The PeakSegFPOP algorithm
%   was used on the large data sets in the
%   benchmark. Data sizes range from $N=10^2$ to $10^7$ weighted data to
%   segment (x-axis); disk usage (top panel) and computation time
%   (bottom panel) are log-linear $O(N \log N)$.}
% \end{figure}
 
\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models-computation}
\end{minipage}
\caption{\label{fig:target-intervals-models-all} In our empirical
  tests, the computational requirements of the GFPOP algorithm
  were log-linear $O(N \log N)$ in the number of data points $N$ to
  segment. \textbf{Left:} we analyzed the number of intervals $I$
  (candidate changepoints) stored in the $C_t(\mu)$ cost functions,
  because the total time/space complexity is $O(NI)$. We observed
  empirically that the mean number of intervals $I=O(\log N)$ (red
  curve). Even the maximum number of intervals (blue curve) is much
  less than $N$. \textbf{Right:} storage on disk (top panel) and
  computation time (bottom panel) are empirically $O(N \log N)$. Error
  bands show median and 5\%/95\% quantiles over several data sets of a
  given size $N$; black dots and text show computational requirements
  for the most extreme data sets.}
\end{figure}

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  %\includegraphics{jss-figure-evaluations}
  \input{jss-figure-evaluations}
\end{minipage} 
\begin{minipage}{3in} 
  %\includegraphics{jss-figure-evaluations-computation}
  \input{jss-figure-evaluations-computation} 
\end{minipage} 
\vskip -0.5cm
\caption{\label{fig:evaluations} Computing a zero-error model with
  $O(\sqrt{N})$ peaks is possible in $O(N(\log N)^2)$ time using our
  proposed Optimal Partitioning Search algorithm. \textbf{Left:}
  Segment Neighborhood requires $O(\sqrt{N})$ dynamic programming
  iterations to compute a model with $O(\sqrt{N})$ peaks; our proposed
  Optimal Partitioning search algorithm requires only $O(\log N)$
  iterations. \textbf{Right:} Optimal Partitioning solves for one
  penalty in $O(N\log N)$ space/time (median line and 5\%/95\% quantile band
  over data sets and penalties); finding the zero-error model with
  $O(\sqrt{N})$ peaks takes $O(N (\log N)^2)$ time/space -- only a log
  factor more (points).}
\end{figure} 
  
\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-variable-peaks}
\begin{minipage}{3in}
  \input{jss-figure-variable-peaks}
\end{minipage}
\begin{minipage}{3in} 
  \input{jss-figure-variable-peaks-zoom}
\end{minipage}
\vskip -0.5cm
\caption{\label{fig:variable-peaks} Comparison of iterations using
  Segment Neighborhood (grey lines) and Optimal Partitioning (green
  dots). GFPOP with sequential search
  (Algorithm~\ref{algo:seq-search}) was used to compute optimal models
  with different numbers of peaks $P$, for two data sets with
  $N\approx 6\times 10^5$. \textbf{Left:} the number of iterations is
  linear $O(P)$ for Segment Neighborhood (grey lines) but empirically
  $O(\log P)$ for Optimal Partitioning with sequential search (green
  dots). \textbf{Right:} Optimal Partitioning is empirically faster
  for computing models with $P>5$ peaks (10 segments); Segment
  Neighborhood is faster for smaller models.}
\end{figure}



 

% \begin{CodeChunk}
% \begin{CodeInput}
% R> m_pois <- glm(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine,
% +    family = poisson)
% \end{CodeInput}
% \end{CodeChunk}
% %
% To account for potential overdispersion we also consider a negative binomial
% GLM.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> library("MASS")
% R> m_nbin <- glm.nb(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine)
% \end{CodeInput}
% \end{CodeChunk}
% %
% In a comparison with the BIC the latter model is clearly preferred.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> BIC(m_pois, m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
%        df      BIC
% m_pois 18 2046.851
% m_nbin 19 1157.235
% \end{CodeOutput}
% \end{CodeChunk}
% %
% Hence, the full summary of that model is shown below.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> summary(m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
% Call:
% glm.nb(formula = Days ~ (Eth + Sex + Age + Lrn)^2, data = quine, 
%     init.theta = 1.60364105, link = log)

% Deviance Residuals: 
%     Min       1Q   Median       3Q      Max  
% -3.0857  -0.8306  -0.2620   0.4282   2.0898  

% Coefficients: (1 not defined because of singularities)
%             Estimate Std. Error z value Pr(>|z|)    
% (Intercept)  3.00155    0.33709   8.904  < 2e-16 ***
% EthN        -0.24591    0.39135  -0.628  0.52977    
% SexM        -0.77181    0.38021  -2.030  0.04236 *  
% AgeF1       -0.02546    0.41615  -0.061  0.95121    
% AgeF2       -0.54884    0.54393  -1.009  0.31296    
% AgeF3       -0.25735    0.40558  -0.635  0.52574    
% LrnSL        0.38919    0.48421   0.804  0.42153    
% EthN:SexM    0.36240    0.29430   1.231  0.21818    
% EthN:AgeF1  -0.70000    0.43646  -1.604  0.10876    
% EthN:AgeF2  -1.23283    0.42962  -2.870  0.00411 ** 
% EthN:AgeF3   0.04721    0.44883   0.105  0.91622    
% EthN:LrnSL   0.06847    0.34040   0.201  0.84059    
% SexM:AgeF1   0.02257    0.47360   0.048  0.96198    
% SexM:AgeF2   1.55330    0.51325   3.026  0.00247 ** 
% SexM:AgeF3   1.25227    0.45539   2.750  0.00596 ** 
% SexM:LrnSL   0.07187    0.40805   0.176  0.86019    
% AgeF1:LrnSL -0.43101    0.47948  -0.899  0.36870    
% AgeF2:LrnSL  0.52074    0.48567   1.072  0.28363    
% AgeF3:LrnSL       NA         NA      NA       NA    
% ---
% Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

% (Dispersion parameter for Negative Binomial(1.6036) family taken to be 1)

%     Null deviance: 235.23  on 145  degrees of freedom
% Residual deviance: 167.53  on 128  degrees of freedom
% AIC: 1100.5

% Number of Fisher Scoring iterations: 1


%               Theta:  1.604 
%           Std. Err.:  0.214 

%  2 x log-likelihood:  -1062.546 
% \end{CodeOutput}
% \end{CodeChunk}



%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and discussion} \label{sec:summary}


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The pseudocode for the algorithm which implements the dynamic
programming updates (\ref{eq:generalDP}) is stated below.
\begin{algorithm}
\begin{algorithmic}[1]
\STATE Input: data $\mathbf y$ and weights $\mathbf w$ (both size $n$), 
number of vertices/states $|V|$, starts $\underline S\subseteq V$, 
edges/transitions $E$.
\STATE Allocate $|V|\times n$ array of optimal cost functions $C_{s,t}$, 
each initialized to NULL.
\STATE for $t$ from $1$ to $n$:
\begin{ALC@g}
  \STATE if $t==1$:
  \begin{ALC@g}
    \STATE for $s$ in $\underline S$: // initialize cost for all possible starting states
    \begin{ALC@g}
      \STATE $C_{s,t}\gets\text{InitialCost}(y_t, w_t)$
    \end{ALC@g}
  \end{ALC@g}
  \STATE else:
  \begin{ALC@g}
    \STATE for $s$ from $1$ to $|V|$: 
    \begin{ALC@g}
      \STATE if $C_{s,t-1}$ is NOT NULL: // previous cost in this state has been computed
      \begin{ALC@g}
        \STATE $C_{s,t}\gets C_{s,t-1}$ // cost of staying in this state (no change)
      \end{ALC@g}
    \end{ALC@g}
    \STATE for ($\underline v$, $\overline v$, $\lambda$,
    ConstrainedCost) in $E$:
    \begin{ALC@g}
      \STATE if $C_{\underline v,t-1}$ is NOT NULL: // previous cost has been computed
      \begin{ALC@g}
        \STATE
        $\text{cost\_of\_change}\gets
        \text{ConstrainedCost}(C_{\underline v, t-1})$
        \STATE
        $\text{cost\_of\_change.set}
        (\underline v, t-1)$
        \STATE
        $\text{cost\_of\_change.addPenalty}
        (\text{$\lambda$})$
        \STATE if $C_{\overline v,t}$ is NULL:
        \begin{ALC@g}
          \STATE $C_{\overline v,t}\gets\text{cost\_of\_change}$
        \end{ALC@g}
        \STATE else:
        \begin{ALC@g}
          \STATE
          $C_{\overline v,t}\gets \text{MinOfTwo}(C_{\overline v,t},
          \text{cost\_of\_change})$
        \end{ALC@g}
      \end{ALC@g}
    \end{ALC@g}
    \STATE for $s$ from $1$ to $|V|$:
    \begin{ALC@g}
      \STATE if $C_{s,t}$ is NOT NULL:
      \begin{ALC@g}
        \STATE $C_{s,t}\text{.addDataPoint}(y_t, w_t)$
      \end{ALC@g}
    \end{ALC@g}
  \end{ALC@g}
\end{ALC@g}
\STATE Output: $|V|\times n$ array of optimal cost functions $C_{s,t}$.
\caption{\label{algo:GFPOP}Generalized Functional Pruning Optimal
  Partitioning Algorithm, Dynamic Programming (GFPOP-DP)}
\end{algorithmic}
\end{algorithm}

The algorithm above performs several checks if $C_{s,t}$ is NULL or
not (lines 9, 12, 16, 21). All costs are initialized as NULL (line
2). After having performed the cost update for data $t$, a NULL cost
$C_{s,t}$ means that state $s$ is not feasible at data $t$. For each
constraint function $g$ there is a corresponding ConstrainedCost
sub-routine that is mentioned on lines~11 and 13 (e.g. no constraint
$g_0$ MinUnconstrained, non-decreasing change $g_\uparrow$ MinLess,
non-increasing change $g_\downarrow$ MinMore). 

The average time and space complexity of Algorithm~\ref{algo:GFPOP} is
$O(|V| n I)$ where $|V|$ is the number of states and $I$ is the
average number of of intervals stored in the $|V|\times n$ array of
$C_{s,t}$ cost functions. We observed that $I=\log n$ in the empirical
tests of the peak detection model on ChIP-seq data
(Section~\ref{sec:results_time}), so we expect that the average time
complexity of Algorithm~\ref{algo:GFPOP} is $O(|V| n\log n)$.

Note that the algorithm above only performs the dynamic
programming. The decoding of optimal model parameters is achieved
using the algorithm below.

\begin{algorithm}
\begin{algorithmic}[1]
\STATE Output: $|V|\times n$ array of optimal cost functions $C_{s,t}$, 
ends $\overline S\subseteq V$.
\STATE Allocate
$\mathbf m\in\RR^n$ (mean), 
$\mathbf s\in\ZZ^n$ (state), 
$\mathbf t\in\ZZ^n$ (segment end).  
\STATE $u^*,s^*,t',s'\gets \text{ArgMin}(C_{\cdot,n}, \overline S)$ // begin decoding
\STATE $i\gets 1;\, m_{i}\gets u^*;\, s_i\gets s^*;\, t_{i}\gets n$
\STATE while $t' > 0$:
\begin{ALC@g}
  \STATE $i\gets i+1;\, t_{i}\gets t'$
  \STATE $u^*,s^*,t',s'\gets \text{ArgMin}(C_{s',t'})$
  \STATE $m_{i}\gets u^*;\, s_i\gets s^*$
\end{ALC@g}
\STATE Output: 
$\mathbf m$, 
$\mathbf s$, 
$\mathbf t$.
\caption{\label{algo:GFPOP-decode}Generalized Functional Pruning Optimal
  Partitioning Algorithm, decoding (GFPOP-decode)}
\end{algorithmic}
\end{algorithm}


% \begin{leftbar} 
% If necessary or useful, information about certain computational details
% such as version numbers, operating systems, or compilers could be included
% in an unnumbered section. Also, auxiliary packages (say, for visualizations,
% maps, tables, \dots) that are not cited in the main text can be credited here.
% \end{leftbar}

% The results in this paper were obtained using
% \proglang{R}~3.4.1 with the
% \pkg{MASS}~7.3.47 package. \proglang{R} itself
% and all packages used are available from the Comprehensive
% \proglang{R} Archive Network (CRAN) at
% \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

% \begin{leftbar}
% All acknowledgments (note the AE spelling) should be collected in this
% unnumbered section before the references. It may contain the usual information
% about funding and feedback from colleagues/reviewers/etc. Furthermore,
% information such as relative contributions of the authors may be added here
% (if any).
% \end{leftbar}


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{jss-refs}


% %% -- Appendix (if any) --------------------------------------------------------
% %% - After the bibliography with page break.
% %% - With proper section titles and _not_ just "Appendix".

% \newpage

% \begin{appendix}

% \section{More technical details} \label{app:technical}

% \begin{leftbar}
% Appendices can be included after the bibliography (with a page break). Each
% section within the appendix should have a proper section title (rather than
% just \emph{Appendix}).

% For more technical style details, please check out JSS's style FAQ at
% \url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
% which includes the following topics:
% \begin{itemize}
%   \item Title vs.\ sentence case.
%   \item Graphics formatting.
%   \item Naming conventions.
%   \item Turning JSS manuscripts into \proglang{R} package vignettes.
%   \item Trouble shooting.
%   \item Many other potentially helpful details\dots
% \end{itemize}
% \end{leftbar}


% \section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

% \begin{leftbar}
% References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
% references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
% \verb|\citealp| etc.\ (and never hard-coded). This commands yield different
% formats of author-year citations and allow to include additional details (e.g.,
% pages, chapters, \dots) in brackets. In case you are not familiar with these
% commands see the JSS style FAQ for details.

% Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
% when acquiring the entries automatically from mixed online sources. However,
% it is important that informations are complete and presented in a consistent
% style to avoid confusions. JSS requires the following format.
% \begin{itemize}
%   \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
%     be used in the references.
%   \item Titles should be in title case.
%   \item Journal titles should not be abbreviated and in title case.
%   \item DOIs should be included where available.
%   \item Software should be properly cited as well. For \proglang{R} packages
%     \code{citation("pkgname")} typically provides a good starting point.
% \end{itemize}
% \end{leftbar}

% \end{appendix}

% %% -----------------------------------------------------------------------------


\end{document}
