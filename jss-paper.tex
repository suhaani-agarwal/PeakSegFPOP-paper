% -*- compile-command: "make jss-paper.pdf" -*-
\documentclass[article]{jss}

\newcommand{\R}{\proglang{R}}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{Ckt}{HTML}{E41A1C}
\definecolor{Min}{HTML}{4D4D4D}%grey30
%{B3B3B3}%grey70
\definecolor{MinMore}{HTML}{377EB8}
\definecolor{Data}{HTML}{984EA3}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2016} 
%\usepackage{fullpage}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}


%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Toby Dylan Hocking\\Northern Arizona University
   \And Guillem Rigaill\\INRA
   \And Paul Fearnhead\\Lancaster University
   \And Guillaume Bourque\\McGill University}
\Plainauthor{Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead, Guillaume Bourque} 

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{A disk-based functional pruning algorithm for optimal changepoint
  detection in genomic data} 

\Plaintitle{A disk-based functional pruning algorithm for optimal changepoint
  detection in large genomic data} 

%\Shorttitle{A Short Demo Article in \proglang{R}}
\Shorttitle{Disk-based optimal changepoint detection in large data}

%% - \Abstract{} almost as usual
\Abstract{ This article describes a new algorithm and \proglang{R}
  package for optimal changepoint detection in large genomic data
  sets. Previous packages have used in-memory implementations, which
  limit application to relatively small data sets. The proposed
  PeakSegPipeline package implements disk-based storage in order to
  compute optimal changepoint models for the large data sets which are
  widespread in genomics.}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, \proglang{R}} 

\Plainkeywords{Dynamic programming, optimal changepoint detection, peak
  detection, genomic data, R} 

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Toby Dylan Hocking\\
  Northern Arizona University\\
  School of Informatics, Computing, and Cyber Systems\\
  Flagstaff, AZ, USA\\
  E-mail: \email{Toby.Hocking@R-project.org}\\
  %URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section[Introduction: optimal changepoint detection in R]{Introduction: optimal changepoint detection in \proglang{R}} \label{sec:intro}

\subsection{Models with no constraints between segment means}

Several dynamic programming algorithms for inferring optimal multiple
changepoint models have been proposed in recent years
(Table~\ref{tab:unconstrained-algos}). Such algorithms seek to compute
the optimal model with $S$ segments ($S-1$ changepoints) for a
sequence of $N$ data, $z_1,\dots,z_N$. The optimization variables are
the $S-1$ discrete changepoints (which take integer values from 1 to
$N-1$), and the $S$ segment means (typically real-valued). The goal of
optimal changepoint algorithms is to find the arrangement of
changepoints and segment means which maximizes a likelihood function
(or equivalently minimizes a loss function). Optimizing by naively
computing the best mean and likelihood for each of the $O(N^{S-1})$
possible arrangements of changepoints is an exponential $O(N^S)$ time
operation.

The classical dynamic programming algorithms have time complexity
which is quadratic in the number of data $N$. The algorithm which
solves the ``Segment Neighborhood'' problem computes a sequence of
optimal models from 1 to $S$ segments, in $O(SN^2)$ time
\citet{segment-neighborhood}.  \citet{optimal-partitioning} described
an $O(N^2)$ algorithm which solves the related ``Optimal
Partitioning'' problem. Rather than explicitly using the number of
segments $S$ as a parameter, it inputs a non-negative penalty
parameter $\lambda\in \RR_+$. The classical algorithm for solving the
Segment Neighborhood problem is available in \R\ as
\verb|changepoint::cpt.mean(method="SegNeigh")|. We know of no \R\
implementation of the classical $O(N^2)$ algorithm for solving the Optimal
Partitioning problem.

\citet{pruned-dp} proposed a Pruned Dynamic Programming Algorithm
(PDPA) for solving the Segment Neighborhood problem in faster
$O(S N\log N)$ time. The novelty of the PDPA is the functional
representation of the optimal cost, which allows pruning the $O(N)$
possible changepoints to only $O(\log N)$ candidates (while
maintaining optimality). The original implementation of the PDPA was
available in \R\ as \verb|cghseg:::segmeanCO| for the Normal
homoscedastic model, but \pkg{cghseg} has been removed from CRAN as of
18 December 2017. It is now available as \verb|jointseg::Fpsn| on
Bioconductor. \citet{cleynen2013segmentation} described a
generalization of the PDPA for other likelihood/loss functions
(Poisson, negative binomial, Normal heteroscedastic). These are
available in \R\ as \verb|Segmentor3IsBack::Segmentor|.

\citet{fpop} proposed a Functional Pruning Optimal Partitioning (FPOP)
algorithm for solving the Optimal Partitioning problem in $O(N\log N)$
time. As in the PDPA, the ``functional pruning'' technique is used to
reduce the $O(N)$ possible changepoints to only $O(\log N)$ candidates
(while maintaining optimality). The FPOP algorithm is available in R as
\verb|fpop::Fpop|.

\begin{table*}[t!]
  \centering
  \begin{tabular}{r|c|c}
    Problem & No changepoint pruning & Functional pruning \\
    \hline
    Segment  & Dynamic Prog. Algo. (DPA) & Pruned DPA (PDPA) \\
                  Neighborhood & Optimal, $O(SN^2)$ time & Optimal, $O(SN\log N)$ time\\
            $S$ segments& \citet{segment-neighborhood} & \citet{pruned-dp}\\
            & \pkg{changepoint} & \pkg{jointseg}\\
    \hline
    Optimal  & Optimal Partitioning Algorithm & FPOP \\
    Partitioning & Optimal, $O(N^2)$ time & Optimal, $O(N\log N)$ time\\
            Penalty $\lambda$& \citet{optimal-partitioning} & \citet{fpop}  \\
    &  & \pkg{fpop}\\
    \hline
  \end{tabular}
  \caption{Previous work on algorithms for optimal changepoint detection with 
    no constraints between adjacent segment means.}
\label{tab:unconstrained-algos} 
\end{table*}

% Oracle penalties \citep{cleynen2013segmentation} or learned penalties
% \citep{HOCKING-penalties} can be used to select the number of segments
% $K$.  Because this model sometimes has several consecutive up changes,
% it is non-trivial to interpret in terms of peaks and background
% (Figure~\ref{fig:data-models}, top).

\subsection{Models with constraints between adjacent segment means}

% Hidden Markov Models (HMMs) with common mean parameters for the peak
% or background regions could be used to model such sequence data, but
% we do not explore them in this paper for two reasons. First, we have
% observed in real ChIP-seq data that background and peak means are not
% constant throughout the genome (Supplementary Figure 4), so shared
% mean parameters would not be a good fit. Second, inference algorithms
% for HMMs are only guaranteed to find a local maximum of the
% likelihood; we are more interested in changepoint detection models
% with dynamic programming algorithms that can provably compute the
% global maximum of the corresponding likelihood.

The models discussed above are unconstrained in the sense that there
are no constraints between segment means. However constraints can be
useful when data need to be interpreted in terms of pre-defined
domain-specific states. For example in genomic data such as ChIP-seq
\citep{chip-seq}, the changepoint model needs to be interpreted in
terms of peaks (large values which represent protein binding) and
background (small values which represent background noise).

In this context, \citet{HOCKING-PeakSeg} introduced a $O(SN^2)$
Constrained Dynamic Programming Algorithm (CDPA) for computing a model
where up changes are followed by down changes, and vice versa
(\ref{tab:constrained-algos}). These constraints ensure that
odd-numbered segments can be interpreted as background, and
even-numbered segments can be interpreted as peaks. Although the CDPA
provides a sub-optimal solution to the Segment Neighborhood problem in
$O(SN^2)$ time, \citet{HOCKING2016-chipseq} showed that it achieves
state-of-the-art peak detection accuracy in a benchmark of
ChIP-seq data sets. 

% However The size of these aligned read count data depends on the size of the
% chromosomes in the reference genome. For example, the largest
% chromosome in the human genome (hg19) is chr1, which has 249,250,621
% bases (distinct positions at which the number of aligned reads is
% measured). Analysis of such data thus requires computationally
% efficient algorithms which scale to data sets of arbitrarily large
% size.

Because the quadratic time complexity of the CDPA limits its
application to relatively small data sets,
\citet{Hocking-constrained-changepoint-detection} proposed to adapt
the functional pruning to the changepoint model with up-down
constraints between adjacent segment means. The resulting Generalized
Pruned Dynamic Programming Algorithm (GPDPA) reduces the number of
candidate changepoints from $O(N)$ to $O(\log N)$ while maintaining
optimality. The GPDPA computes the optimal solution to the up-down
constrained Segment Neighborhood problem in $O(SN\log N)$ time.  The
\pkg{PeakSegOptimal} \R\ package provides an in-memory implementation
of the up-down constrained model
\citep{Hocking-constrained-changepoint-detection}.

\begin{table}
  \centering
  \begin{tabular}{r|c|c}
    & No changepoint pruning & Functional pruning \\
    \hline
    Segment Neighborhood & Constrained DPA & Generalized PDPA \\
$S$ segments    & Sub-optimal,  $O(SN^2)$ & Optimal, $O(SN\log N)$\\
    & \citet{HOCKING-PeakSeg} & 
% dont know why this isn't abbreviated to etal!!!
\citet{Hocking-constrained-changepoint-detection}
%Hocking \emph{et al.} (2017)
 \\
& \pkg{PeakSegDP} & \pkg{PeakSegOptimal}\\
    \hline
    Optimal Partitioning &  & Generalized FPOP \\
    Penalty $\lambda$&  & Optimal, $O(N\log N)$\\
    &  & \textbf{This work}\\
    & & \pkg{PeakSegPipeline}\\
    \hline
  \end{tabular}
  \caption{Algorithms for optimal changepoint detection with up-down 
constraints on adjacent segment means. Previous work is limited to solvers for the Segment Neighborhood problem; this paper presents Generalized Functional Pruning Optimal Partitioning (GFPOP), Algorithm~\ref{algo:GFPOP}.}
  \label{tab:constrained-algos}
\end{table}

% The approach taken by the PeakSeg algorithm can be formalised as follows. For any given number, $K$,
% of segments we try to find the best segmentation. We define how good a segmentation is by fitting
% a constant mean for each segment of the chromosome, subject to the up-down constraint that
% the segment means alternate between increasing and decreasing. Subject to this constraint we find the
% best choice of segment means to minimize some loss function that measures how close the mean is to each 
% observation. The cost of a segmentation is defined to be the resulting value of this loss summed over 
% all observations. For a sequence of $n$ data points, PeakSeg attempts to search over the $O(n^{K-1})$ 
% possible segmentations to find the one that minimizes this cost. 


%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\subsection{Contributions}

This paper presents two new algorithms for constrained optimal
changepoint detection, along with an analysis of their empirical
time/space complexity in a benchmark of large genomic data. The
algorithms are implemented in the \R\ package
\pkg{PeakSegPipeline} on GitHub.\footnote{
\url{https://github.com/tdhock/PeakSegPipeline}
}

First, we present a new algorithm for solving the Optimal Partitioning
problem with up-down constraints between adjacent segment means
(GFPOP, Algorithm~\ref{algo:GFPOP}). The fastest existing algorithm
for the up-down constrained changepoint model was the
$O(SN\log N)$ solver for the Segment Neighborhood problem
(Table~\ref{tab:constrained-algos}). In large genomic data sets, we
are only interested in models with many segments/changepoints, so it
is a waste of time and space to compute all models from 1 to $S$
segments using Segment Neighborhood algorithms. Our proposed GFPOP
algorithm solves the Optimal Partitioning problem, so yields one
optimal model with $S$ segments (without having to compute the models
from 1 to $S-1$ segments). We show that the empirical complexity of
our GFPOP implementation is $O(N\log N)$ time, $O(N\log N)$ space, and
$O(\log N)$ memory, which makes it possible to compute optimal models
for large genomic data sets on common laptop computers.

Although solving the Optimal Partitioning problem is faster by a
factor of $O(S)$, the user is unable to directly choose the number of
segments $S$. The user inputs a penalty $\lambda$, and gets one of the
possible optimal changepoint models as output. Thus, we also propose a
sequential search (Algorithm~\ref{algo:seq-search}) which computes the
optimal model for a specified number of segments $S$. It repeatedly
calls GFPOP to solve Optimal Partitioning with different penalties
$\lambda$, until it finds a penalty that yields the specified number
of segments $S$. We empirically show that the sequential search only
requires $O(\log S)$ evaluations of GFPOP. Overall the proposed
algorithm is thus $O( N \log(N)\log(S))$ time, $O(N\log N)$ disk,
$O(\log N)$ memory. 

\section{Models and software} \label{sec:models}

The optimization problem that our algorithm solves is
\begin{align*}
  \label{eq:penalized_peakseg}
  \minimize_{
  \substack{
  \mathbf m\in\RR^N,\ \mathbf s\in\{0, 1\}^N\\
  \mathbf c\in\{-1, 0,1\}^{N-1}\\
  }
  } &\ \ 
      \sum_{i=1}^N \ell(m_i, z_i) + \lambda \sum_{i=1}^{N-1} I(c_i = 1) \\
  \text{subject to\ \ \ } &\ \text{no change: }c_t = 0 \Rightarrow m_t = m_{t+1}\text{ and }s_t=s_{t+1}, \\
    &\ \text{non-decreasing change: }c_t = 1 \Rightarrow m_t \leq m_{t+1}\text{ and }(s_t,s_{t+1})=(0,1),\\
    &\ \text{non-increasing change: } c_t = -1 \Rightarrow m_t \geq m_{t+1}\text{ and }(s_t,s_{t+1})=(1,0),\\
  & \ \text{start and end down: } s_1=s_N=0.
\end{align*}

\begin{figure}
  \centering
  \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2cm,
    thick,main node/.style={circle,draw}]

    \node[main node] (1) {$s=1$};
    \node[main node] (2) [below of=1] {$s=0$};
    \node (3) [left of=2] {start};
    \node (4) [right of=2] {end};

    \path[every node/.style={font=\sffamily\small}]
    (2) edge [bend left] node {$c=1, \leq, \lambda$} (1)
    (3) edge (2)
    (2) edge (4)
    (1) edge [bend left] node {$c=-1, \geq, 0$} (2);
  \end{tikzpicture}
  \caption{State graph for changepoint model with up-down constraints
    between adjacent segment means. State $s=0$ represents background
    noise (small values) whereas state $s=1$ represents peaks (large
    values). Constraint $c=1$ enforces a non-decreasing change via the
    min-less operator ($\leq$) with a penalty of $\lambda$; $c=-1$
    enforces a non-increasing change via the min-more operator
    ($\geq$) with a penalty of $0$. The model is additionally
    constrained to start and end in the background noise $s=0$ state
    ($s_1=s_N=0$).}
  \label{fig:state-graph}
\end{figure}

The algorithm recursively computes the two vectors of real-valued cost
functions using the update rules:
\begin{equation}
  \label{eq:dp-over}
  \overline C_{t}(\mu) = \ell(\mu, z_i) + \min\{
  \overline C_{t-1}(\mu),\, 
  \underline C_{t-1}^\leq(\mu)+\lambda
\},
\end{equation}
\begin{equation}
  \label{eq:dp-under}
    \underline C_{t}(\mu) = \ell(\mu, z_i) + \min\{
  \underline C_{t-1}(\mu),\, 
  \overline C_{t-1}^\geq(\mu)+\lambda
\},
\end{equation}
where the min-less operator is defined as 
\begin{equation}
  \label{eq:min-less}
  f^\leq(\mu) = \min_{x\leq\mu} f(x),
\end{equation}
and the min-more operator is defined as
\begin{equation}
  \label{eq:min-more}
  f^\geq(\mu) = \min_{x\geq\mu} f(x).
\end{equation}
The algorithm is implemented in \R\ as
\verb|PeakSegPipeline::PeakSegFPOP_disk|. Its complexity is
$O(N\log N)$ time, $O(N\log N)$ disk, $O(\log N)$ memory.

  \begin{figure}
    \centering
    \begin{tikzpicture}[->,>=latex,shorten >=1pt,auto,node distance=2.5cm,
      thick,main node/.style={circle,draw}]
      \node[main node] (peak_t1) {$\overline C_{t-1}$};
      \node[main node] (bkg_t1) [below of=peak_t1] {$\underline C_{t-1}$};
      \node[main node] (peak_t) [right of=peak_t1] {$\overline C_{t}$};
      \node[main node] (bkg_t) [right of=bkg_t1] {$\underline C_{t}$};
      \node[main node] (peak_2) [left of=peak_t1] {$\overline C_2$};
      \node[main node] (bkg_2) [left of=bkg_t1] {$\underline C_2$};
      \node[main node] (peak_N1) [right of=peak_t] {$\overline C_{N-1}$};
      \node[main node] (bkg_N1) [right of=bkg_t] {$\underline C_{N-1}$};
      \node[main node] (bkg_N) [right of=bkg_N1] {$\underline C_N$};
      \node[main node] (bkg_1) [left of=bkg_2] {$\underline C_1$};
      \path[every node/.style={font=\small}]
      (peak_t1) edge [dotted] node {$\overline C_{t-1}$} (peak_t)
      (peak_t1) edge [black, bend right] node [right] {$\overline C_{t-1}^{\geq}$} (bkg_t)
      (bkg_t1) edge [dotted] node[midway, below] {$\underline C_{t-1}$} (bkg_t)
      (bkg_t1) edge [black, bend left] node[right] {$\underline C_{ t-1}^{\leq}+\lambda$} (peak_t)
      (bkg_1) edge [black, bend left] node[right] {$\underline C_{1}^{\leq}+\lambda$} (peak_2)
      (bkg_1) edge [dotted] node[midway, below] {$\underline C_1$} (bkg_2)
      (peak_N1) edge [black, bend right] node [right] {$\overline C_{N-1}^{\geq}$} (bkg_N)
      (bkg_N1) edge [dotted] node[midway, below] {$\underline C_{N-1}$} (bkg_N)
      (bkg_2) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (bkg_t1)
      (peak_2) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (peak_t1)
      (bkg_N1) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (bkg_t)
      (peak_N1) edge [color=white] node[below, text=black, pos=0.5] {$\cdots$} (peak_t)
      ;
    \end{tikzpicture}
    \caption{Directed acyclic graph (DAG) representing dynamic
      programming computations (Algorithm~\ref{algo:GFPOP}) for
      changepoint model with up-down constraints between adjacent
      segment means. Nodes in the graph repesent cost functions, and
      edges represent inputs to the the MinOfTwo sub-routine. There is
      one column for each data point and one row for each state: the
      optimal cost of the peak state $s=1$ at data point $t$ is
      $\overline C_t$ (top row); the optimal cost of the background
      noise state $s=0$ is $\underline C_t$ (bottom row). There is
      only one edge going to $\underline C_2$ and $\overline C_2$
      because the model is constrained to start in the background
      noise state ($s_1=0$).}
    \label{fig:computation-graph}
  \end{figure}


\begin{algorithm}
    \begin{algorithmic}[1]
  \STATE Input: data $\mathbf z\in\RR^N$, target peaks $P^*$.
  \STATE $\overline L,\overline p \gets \text{GFPOP}(\mathbf z, \lambda=0)$ // initialize upper bound to max peak model
  \STATE $\underline L,\underline p \gets \text{GFPOP}(\mathbf z, \lambda=\infty)$ // initialize lower bound to 0 peak model
  \STATE While $P^*\not\in \{\underline p,\, \overline p\}$:
  \begin{ALC@g}
    \STATE $\lambda=(\overline L-\underline L)/(\underline p-\overline p)$
    \STATE $L_{\text{new}},p_{\text{new}}\gets\text{GFPOP}(\mathbf z, \lambda)$
    \STATE If $p_{\text{new}}\in\{\underline p, \overline p\}$: return model with $\underline p$ peaks.
    \STATE If $p_{\text{new}} < P^*$: $\underline L,\underline p\gets L_{\text{new}},p_{\text{new}}$ // new lower bound
    \STATE Else: $\overline L,\overline p\gets L_{\text{new}},p_{\text{new}}$ // new upper bound
% > prob.i <- 3
% > fit.list$others[order(iteration)][, list(target.peaks=prob$peaks, iteration, under, over, penalty, peaks, total.cost)]
%    target.peaks iteration under over    penalty peaks total.cost
% 1:           33         1    NA   NA     0.0000  7487 -201361.96
% 2:           33         1    NA   NA        Inf     0  920923.98
% 3:           33         2     0 7487   149.8979   753  -24385.02
% 4:           33         3     0  753  1255.3904    47  153676.28
% 5:           33         4     0   47 16324.4191    10  310043.81
% 6:           33         5    10   47  4226.1495    21  214200.02
% 7:           33         6    21   47  2327.8360    33  177484.99
% > 
  \end{ALC@g}
  \STATE If $P^*=\underline p$: return model with $\underline p$ peaks.
  \STATE Else: return model with $\overline p$ peaks.
  \end{algorithmic}
\caption{\label{algo:seq-search}Sequential search for $P^*$ peaks using GFPOP.}
\end{algorithm}

Algorithm~\ref{algo:seq-search} (sequential search for $P^*$ peaks)
uses GFPOP as a sub-routine to find the optimal model with $P^*$
peaks. Its complexity is $O(N(\log N)^2)$ time, $O(N\log N)$ space,
$O(\log N)$ memory.

% \begin{leftbar}
% Note that around the \verb|{equation}| above there should be no spaces (avoided
% in the {\LaTeX} code by \verb|%| lines) so that ``normal'' spacing is used and
% not a new paragraph started.
% \end{leftbar}

% \proglang{R} provides a very flexible implementation of the general GLM
% framework in the function \fct{glm} \citep{Chambers+Hastie:1992} in the
% \pkg{stats} package. Its most important arguments are
% \begin{Code}
% glm(formula, data, subset, na.action, weights, offset,
%   family = gaussian, start = NULL, control = glm.control(...),
%   model = TRUE, y = TRUE, x = FALSE, ...)
% \end{Code}
% where \code{formula} plus \code{data} is the now standard way of specifying
% regression relationships in \proglang{R}/\proglang{S} introduced in
% \cite{Chambers+Hastie:1992}. The remaining arguments in the first line
% (\code{subset}, \code{na.action}, \code{weights}, and \code{offset}) are also
% standard  for setting up formula-based regression models in
% \proglang{R}/\proglang{S}. The arguments in the second line control aspects
% specific to GLMs while the arguments in the last line specify which components
% are returned in the fitted model object (of class \class{glm} which inherits
% from \class{lm}). For further arguments to \fct{glm} (including alternative
% specifications of starting values) see \code{?glm}. For estimating a Poisson
% model \code{family = poisson} has to be specified.

% \begin{leftbar}
% As the synopsis above is a code listing that is not meant to be executed,
% one can use either the dedicated \verb|{Code}| environment or a simple
% \verb|{verbatim}| environment for this. Again, spaces before and after should be
% avoided.

% Finally, there might be a reference to a \verb|{table}| such as
% Table~\ref{tab:overview}. Usually, these are placed at the top of the page
% (\verb|[t!]|), centered (\verb|\centering|), with a caption below the table,
% column headers and captions in sentence style, and if possible avoiding vertical
% lines.
% \end{leftbar}

% \begin{table}[t!]
% \centering
% \begin{tabular}{lllp{7.4cm}}
% \hline
% Type           & Distribution & Method   & Description \\ \hline
% GLM            & Poisson      & ML       & Poisson regression: classical GLM,
%                                            estimated by maximum likelihood (ML) \\
%                &              & Quasi    & ``Quasi-Poisson regression'':
%                                            same mean function, estimated by
%                                            quasi-ML (QML) or equivalently
%                                            generalized estimating equations (GEE),
%                                            inference adjustment via estimated
%                                            dispersion parameter \\
%                &              & Adjusted & ``Adjusted Poisson regression'':
%                                            same mean function, estimated by
%                                            QML/GEE, inference adjustment via
%                                            sandwich covariances\\
%                & NB           & ML       & NB regression: extended GLM,
%                                            estimated by ML including additional
%                                            shape parameter \\ \hline
% Zero-augmented & Poisson      & ML       & Zero-inflated Poisson (ZIP),
%                                            hurdle Poisson \\
%                & NB           & ML       & Zero-inflated NB (ZINB),
%                                            hurdle NB \\ \hline
% \end{tabular}
% \caption{\label{tab:overview} Overview of various count regression models. The
% table is usually placed at the top of the page (\texttt{[t!]}), centered
% (\texttt{centering}), has a caption below the table, column headers and captions
% are in sentence style, and if possible vertical lines should be avoided.}
% \end{table}


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

\subsection{Computing the maximum likelihood model with a given number
  of peaks}

In one data set, MACS2 detected five peaks, and our GFPOP
algorithm can be used to find a more likely model with three peaks
(Figure~\ref{fig:three-peaks}). 

\begin{figure}[t!]
\centering
\includegraphics{jss-figure-more-likely-models-three-peaks}
\caption{\label{fig:three-peaks} One ChIP-seq data set with three peak
  models. \textbf{Top:} the MACS2 algorithm (a heuristic from the
  bioinformatics literature) computed a sub-optimal model with five
  peaks for these data. \textbf{Middle:} the most likely model with
  five peaks contains one equality constraint between segment means,
  which suggests that five peaks is overfitting for these
  data. \textbf{Bottom:} the most likely model with three peaks is
  also more likely than the MACS2 model, and more interpretable (fewer
  small peaks).}
\end{figure}
 


% \begin{leftbar}
% For code input and output, the style files provide dedicated environments.
% Either the ``agnostic'' \verb|{CodeInput}| and \verb|{CodeOutput}| can be used
% or, equivalently, the environments \verb|{Sinput}| and \verb|{Soutput}| as
% produced by \fct{Sweave} or \pkg{knitr} when using the \code{render_sweave()}
% hook. Please make sure that all code is properly spaced, e.g., using
% \code{y = a + b * x} and \emph{not} \code{y=a+b*x}. Moreover, code input should
% use ``the usual'' command prompt in the respective software system. For
% \proglang{R} code, the prompt \code{"R> "} should be used with \code{"+  "} as
% the continuation prompt. Generally, comments within the code chunks should be
% avoided -- and made in the regular {\LaTeX} text instead. Finally, empty lines
% before and after code input/output should be avoided (see above).
% \end{leftbar}

\begin{figure}[t!]
\centering
\includegraphics{jss-figure-label-error}
\caption{\label{fig:label-error} Labels are used to compute an error
  rate for each peak model (blue bars), defined as the sum of false
  positive and false negative labels (rectangles with black
  outline). This H3K36me3 ChIP-seq data set has $N=1,254,751$ data to
  segment on a subset of chr12, but in the plot above we show only the
  82,233 data (grey signal) in the region around the labels (colored
  rectangles). The model with penalty=6682 results in 320 peaks, which
  is too many (three false positive labels with more than one peak
  start/end). Conversely, the model with penalty=278653 results in 33
  peaks, which is too few (only two peaks in the plotted region,
  resulting in two false negative labels on the right where there
  should be exactly one peak start/end). The range of penalties between 9586 and
  267277 results in models with between 34 and 236 peaks, and achieves
  zero label errors. }
\end{figure}

\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-data-peaks}
\input{jss-figure-data-peaks}
\caption{\label{fig:data-peaks} The model with minimal label
  errors has $O(\sqrt{N})$ peaks in a data set of size $N$. For each data set we
  computed peak models with minimal label errors (see
  Figure~\ref{fig:label-error}); we then plot the number of peaks in
  minimal error models as a function of data set size $N$. Black median line
  and grey quartile band computed over several data sets of a given size
  $N$; asymptotic reference lines shown in red.}
\end{figure}
  
% \begin{figure}[t!] 
% \centering
% \includegraphics{jss-figure-more-likely-models-one-peak}
% \caption{\label{fig:one-peak} Default peak model from the baseline
%   MACS2 algorithm (top) and the optimal peak model computed using the
%   proposed PeakSegFPOP algorithm (bottom).}
% \end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-penalty}
% \caption{\label{fig:target-intervals-models-penalty} The PeakSegFPOP
%   algorithm was used to compute optimal models for six of the largest
%   problems (panels from left to right) for a variety of penalty
%   parameters (x-axis). Y-axes show empirical measurements of cost
%   function pieces (intervals, top panel), timings (minutes, middle
%   panel), and disk usage (gigabytes, bottom panel).}
% \end{figure}

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-disk-memory-compare-speed-penalty}
\end{minipage}
\caption{\label{fig:disk-memory-compare-speed} The disk-based storage
  method is only a constant factor slower than the memory-based
  method. We benchmarked both methods on several small data sets
  ($N\leq 462,890$) for which optimal models could be computed using
  1GB of storage. \textbf{Left:} computation time is empirically
  $O(N\log N)$ for both storage methods, but the disk-based method is
  slower by a constant factor. Median line and quartile band computed
  over several penalty values for a given data set; black vertical
  line segment highlights the data set with $N=106,569$ used in the
  right panel. \textbf{Right:} computation time increases with penalty
  value $\lambda$ for both methods, with the disk-based method slower
  by a constant factor.}
\end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics{jss-figure-target-intervals-models-all}
% \caption{\label{fig:target-intervals-models-all} The PeakSegFPOP algorithm
%   was used on the large data sets in the
%   benchmark. Data sizes range from $N=10^2$ to $10^7$ weighted data to
%   segment (x-axis); disk usage (top panel) and computation time
%   (bottom panel) are log-linear $O(N \log N)$.}
% \end{figure}
 
\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models}
\end{minipage} 
\begin{minipage}{3in}
  \includegraphics{jss-figure-target-intervals-models-computation}
\end{minipage}
\caption{\label{fig:target-intervals-models-all} In our empirical
  tests, the computational requirements of the GFPOP algorithm
  were log-linear $O(N \log N)$ in the number of data points $N$ to
  segment. \textbf{Left:} we analyzed the number of intervals $I$
  (candidate changepoints) stored in the $C_t(\mu)$ cost functions,
  because the total time/space complexity is $O(NI)$. We observed
  empirically that the mean number of intervals $I=O(\log N)$ (red
  curve). Even the maximum number of intervals (blue curve) is much
  less than $N$. \textbf{Right:} storage on disk (top panel) and
  computation time (bottom panel) are empirically $O(N \log N)$. Error
  bands show median and 5\%/95\% quantiles over several data sets of a
  given size $N$; black dots and text show computational requirements
  for the most extreme data sets.}
\end{figure}

\begin{figure}[t!]
\centering
\begin{minipage}{3in}
  %\includegraphics{jss-figure-evaluations}
  \input{jss-figure-evaluations}
\end{minipage} 
\begin{minipage}{3in} 
  %\includegraphics{jss-figure-evaluations-computation}
  \input{jss-figure-evaluations-computation} 
\end{minipage} 
\vskip -0.5cm
\caption{\label{fig:evaluations} Computing a zero-error model with
  $O(\sqrt{N})$ peaks is possible in $O(N(\log N)^2)$ time using our
  proposed Optimal Partitioning Search algorithm. \textbf{Left:}
  Segment Neighborhood requires $O(\sqrt{N})$ dynamic programming
  iterations to compute a model with $O(\sqrt{N})$ peaks; our proposed
  Optimal Partitioning search algorithm requires only $O(\log N)$
  iterations. \textbf{Right:} Optimal Partitioning solves for one
  penalty in $O(N\log N)$ space/time (median line and 5\%/95\% quantile band
  over data sets and penalties); finding the zero-error model with
  $O(\sqrt{N})$ peaks takes $O(N (\log N)^2)$ time/space -- only a log
  factor more (points).}
\end{figure} 
  
\begin{figure}[t!]
\centering
%\includegraphics{jss-figure-variable-peaks}
\begin{minipage}{3in}
  \input{jss-figure-variable-peaks}
\end{minipage}
\begin{minipage}{3in} 
  \input{jss-figure-variable-peaks-zoom}
\end{minipage}
\vskip -0.5cm
\caption{\label{fig:variable-peaks} Comparison of iterations using
  Segment Neighborhood (grey lines) and Optimal Partitioning (green
  dots). GFPOP with sequential search
  (Algorithm~\ref{algo:seq-search}) was used to compute optimal models
  with different numbers of peaks $P$, for two data sets with
  $N\approx 6\times 10^5$. \textbf{Left:} the number of iterations is
  linear $O(P)$ for Segment Neighborhood (grey lines) but empirically
  $O(\log P)$ for Optimal Partitioning with sequential search (green
  dots). \textbf{Right:} Optimal Partitioning is empirically faster
  for computing models with $P>5$ peaks (10 segments); Segment
  Neighborhood is faster for smaller models.}
\end{figure}



 

% \begin{CodeChunk}
% \begin{CodeInput}
% R> m_pois <- glm(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine,
% +    family = poisson)
% \end{CodeInput}
% \end{CodeChunk}
% %
% To account for potential overdispersion we also consider a negative binomial
% GLM.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> library("MASS")
% R> m_nbin <- glm.nb(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine)
% \end{CodeInput}
% \end{CodeChunk}
% %
% In a comparison with the BIC the latter model is clearly preferred.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> BIC(m_pois, m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
%        df      BIC
% m_pois 18 2046.851
% m_nbin 19 1157.235
% \end{CodeOutput}
% \end{CodeChunk}
% %
% Hence, the full summary of that model is shown below.
% %
% \begin{CodeChunk}
% \begin{CodeInput}
% R> summary(m_nbin)
% \end{CodeInput}
% \begin{CodeOutput}
% Call:
% glm.nb(formula = Days ~ (Eth + Sex + Age + Lrn)^2, data = quine, 
%     init.theta = 1.60364105, link = log)

% Deviance Residuals: 
%     Min       1Q   Median       3Q      Max  
% -3.0857  -0.8306  -0.2620   0.4282   2.0898  

% Coefficients: (1 not defined because of singularities)
%             Estimate Std. Error z value Pr(>|z|)    
% (Intercept)  3.00155    0.33709   8.904  < 2e-16 ***
% EthN        -0.24591    0.39135  -0.628  0.52977    
% SexM        -0.77181    0.38021  -2.030  0.04236 *  
% AgeF1       -0.02546    0.41615  -0.061  0.95121    
% AgeF2       -0.54884    0.54393  -1.009  0.31296    
% AgeF3       -0.25735    0.40558  -0.635  0.52574    
% LrnSL        0.38919    0.48421   0.804  0.42153    
% EthN:SexM    0.36240    0.29430   1.231  0.21818    
% EthN:AgeF1  -0.70000    0.43646  -1.604  0.10876    
% EthN:AgeF2  -1.23283    0.42962  -2.870  0.00411 ** 
% EthN:AgeF3   0.04721    0.44883   0.105  0.91622    
% EthN:LrnSL   0.06847    0.34040   0.201  0.84059    
% SexM:AgeF1   0.02257    0.47360   0.048  0.96198    
% SexM:AgeF2   1.55330    0.51325   3.026  0.00247 ** 
% SexM:AgeF3   1.25227    0.45539   2.750  0.00596 ** 
% SexM:LrnSL   0.07187    0.40805   0.176  0.86019    
% AgeF1:LrnSL -0.43101    0.47948  -0.899  0.36870    
% AgeF2:LrnSL  0.52074    0.48567   1.072  0.28363    
% AgeF3:LrnSL       NA         NA      NA       NA    
% ---
% Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

% (Dispersion parameter for Negative Binomial(1.6036) family taken to be 1)

%     Null deviance: 235.23  on 145  degrees of freedom
% Residual deviance: 167.53  on 128  degrees of freedom
% AIC: 1100.5

% Number of Fisher Scoring iterations: 1


%               Theta:  1.604 
%           Std. Err.:  0.214 

%  2 x log-likelihood:  -1062.546 
% \end{CodeOutput}
% \end{CodeChunk}



%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and discussion} \label{sec:summary} Our main
application of the proposed algorithms is in a set of genomic data
sets ranging from $N=10^3$ to $N=10^7$. We use the labels in the data
sets to show that there are $O(\sqrt N)$ peaks/segments. The fastest
Segment Neighborhood algorithm would thus be $O(SN\log N)$



%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The pseudocode for the algorithm which implements the dynamic
programming updates (\ref{eq:generalDP}) is stated below.

\begin{algorithm}
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf z\in\RR^n$, penalty constant $\lambda\geq 0$.
\STATE Output: vectors of optimal segment means $U\in\RR^{n}$ and ends $T\in\{1,\dots,n\}^{n}$
\STATE Compute min $\underline z$ and max $\overline z$ of $\mathbf z$.
\label{line:op-min-max}
\STATE
$\underline C_1\gets
 \text{OnePiece}(z_1, \underline z, \overline z)$
\STATE for data points $t$ from 2 to $n$: // dynamic programming
\label{line:for-dp-t}
\begin{ALC@g}
  \STATE $\overline M\gets \lambda + \text{MinLess}(t-1, \underline C_{t-1})$ //cost of non-decreasing change
  \label{line:op-MinLess}
  \STATE $\overline C_{t}\gets \text{MinOfTwo}(\overline M, \overline C_{t-1})+\text{OnePiece}(z_t, \underline z, \overline z)$
  \label{line:op-MinOfTwo}
  \STATE $\underline M\gets \text{MinMore}(t-1, \overline C_{t-1})$ //cost of non-increasing change
  \label{line:op-MinMore}
  \STATE $\underline C_{t}\gets \text{MinOfTwo}(\underline M, \underline C_{t-1})+\text{OnePiece}(z_t, \underline z, \overline z)$
  \label{line:op-MinOfTwo-under}
\end{ALC@g}
\STATE $u^*,t',u'\gets \text{ArgMin}(\underline C_{n})$ // begin decoding
\label{line:op-ArgMin}
\STATE $i\gets 1;\, U_{i}\gets u^*;\, T_{i}\gets t'$
\label{line:op-store-i}
\STATE while $t' > 0$:
\begin{ALC@g}
  \STATE if $u' < \infty$: $u^*\gets u'$
  \STATE if $i$ is odd: $\text{cost}\gets \overline C_{t'}$ else $\underline C_{t'}$
  \STATE $t',u'\gets\text{FindMean}(u^*, \text{cost})$
  \STATE $i\gets i+1;\, U_{i}\gets u^*;\, T_{i}\gets t'$
\label{line:op-i+1}
\end{ALC@g}
\caption{\label{algo:GFPOP}Generalized Functional Pruning Optimal
  Partitioning (GFPOP) for changepoint model with up-down constraints
  between adjacent segment means.}
\end{algorithmic}
\end{algorithm}


% \begin{leftbar} 
% If necessary or useful, information about certain computational details
% such as version numbers, operating systems, or compilers could be included
% in an unnumbered section. Also, auxiliary packages (say, for visualizations,
% maps, tables, \dots) that are not cited in the main text can be credited here.
% \end{leftbar}

% The results in this paper were obtained using
% \proglang{R}~3.4.1 with the
% \pkg{MASS}~7.3.47 package. \proglang{R} itself
% and all packages used are available from the Comprehensive
% \proglang{R} Archive Network (CRAN) at
% \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

% \begin{leftbar}
% All acknowledgments (note the AE spelling) should be collected in this
% unnumbered section before the references. It may contain the usual information
% about funding and feedback from colleagues/reviewers/etc. Furthermore,
% information such as relative contributions of the authors may be added here
% (if any).
% \end{leftbar}


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{jss-refs}
 

% %% -- Appendix (if any) --------------------------------------------------------
% %% - After the bibliography with page break.
% %% - With proper section titles and _not_ just "Appendix".

% \newpage

% \begin{appendix}

% \section{More technical details} \label{app:technical}

% \begin{leftbar}
% Appendices can be included after the bibliography (with a page break). Each
% section within the appendix should have a proper section title (rather than
% just \emph{Appendix}).

% For more technical style details, please check out JSS's style FAQ at
% \url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
% which includes the following topics:
% \begin{itemize}
%   \item Title vs.\ sentence case.
%   \item Graphics formatting.
%   \item Naming conventions.
%   \item Turning JSS manuscripts into \proglang{R} package vignettes.
%   \item Trouble shooting.
%   \item Many other potentially helpful details\dots
% \end{itemize}
% \end{leftbar}


% \section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

% \begin{leftbar}
% References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
% references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
% \verb|\citealp| etc.\ (and never hard-coded). This commands yield different
% formats of author-year citations and allow to include additional details (e.g.,
% pages, chapters, \dots) in brackets. In case you are not familiar with these
% commands see the JSS style FAQ for details.

% Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
% when acquiring the entries automatically from mixed online sources. However,
% it is important that informations are complete and presented in a consistent
% style to avoid confusions. JSS requires the following format.
% \begin{itemize}
%   \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
%     be used in the references.
%   \item Titles should be in title case.
%   \item Journal titles should not be abbreviated and in title case.
%   \item DOIs should be included where available.
%   \item Software should be properly cited as well. For \proglang{R} packages
%     \code{citation("pkgname")} typically provides a good starting point.
% \end{itemize}
% \end{leftbar}

% \end{appendix}

% %% -----------------------------------------------------------------------------


\end{document}
