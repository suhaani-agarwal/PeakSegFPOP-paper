%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2016} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}

\usepackage{tikz}
\usepackage{amssymb,amsmath}
\usepackage{natbib}
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\Changes}{Changes}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Linear time dynamic programming algorithms for constrained optimal change-point detection}

\begin{document} 

\twocolumn[
\icmltitle{Linear time dynamic programming algorithms for constrained optimal change-point detection}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2016
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract}
  Change-point detection is a central problem in time series and
  genomic data, in which constraints on the change-point directions
  are often desired to obtain an interpretable model. We describe a
  new family of dynamic programming algorithms for solving constrained
  optimal change-point detection problems. We begin by showing how a
  recently proposed functional pruning technique can be used to solve
  the isotonic regression problem, then show how this technique can be
  extended to general affine constraints on adjacent segment means. We
  show that our algorithm achieves state-of-the-art speed and accuracy
  in a benchmark of several genomic data sets. A free/open-source C++
  implementation of our algorithms will be made available online after
  publication.
\end{abstract}

\section{Introduction}

\begin{table*}
  \centering
  \begin{tabular}{r|c|c}
    & No pruning & Functional pruning \\
    \hline
    Unconstrained & Dynamic Programming Algorithm (DPA) & Pruned DPA (PDPA) \\
    & Optimal solution, $O(N^2)$ time & Optimal solution, $O(N\log N)$ time\\
    % & \citet{segment-neighborhood} & \\
    % & \citet{optimal-partitioning} & \\
    & \citet{segment-neighborhood, optimal-partitioning}     & \citet{pruned-dp, johnson} \\
    \hline
    Up-down constrained & Constrained DPA (CDPA) & Generalized Pruned DPA (GPDPA) \\
    & Sub-optimal solution, $O(N^2)$ time & Optimal solution, $O(N\log N)$ time\\
    & \citet{HOCKING-PeakSeg} & \textbf{This paper} \\
    \hline
  \end{tabular}
  \caption{Our contribution is a new log-linear time 
    algorithm which uses a functional pruning technique 
    to compute the optimal change-points in $N$ data points,
    subject to constraints on the segment mean parameters.}
\label{tab:contribution}
\end{table*}
Change-point detection is a central problem in many fields. When there
are no constraints other than the number of change-points or segments,
the Pruned Dynamic Programming Algorithm (PDPA) can be used to recover
the change-points with minimum cost \citep{pruned-dp}. The Functional
Pruning Optimal Partitioning (FPOP) algorithm can be used when there
is no constraint on the number of change-points, but there is a
positive penalty constant \citep{FPOP}. 

In the unconstrained change-point detection model, there is no
constraint on the direction of changes that can be recovered. However,
in several kinds of data it is desirable to constrain the possible
change-points to obtain a more interpretable model. For example, in
genomic data it is desirable to only consider models that can be
easily interpreted in terms of peaks and background
\citep{HOCKING-PeakSeg}. This amounts to forcing an up change after each down
change, and vice versa.

The isotonic regression problem is another example of constrained
change-point detection. Typically there is no limit on the number of
changes, as long as they are all in the same direction. This problem
can be solved using the pool-adjacent-violators algorithm
\citep{mair2009isotone}. An L1 relaxed version of this problem is
nearly-isotonic regression \citep{tibshirani2011nearly}.

TODO discuss isotonic DP \citep{isotonic-dp}, functional pruning
\citep{phd-johnson}, reduced isotonic regression
\citep{hardwick2014optimal, reduced-monotonic-regression}, unimodal
segmentation \citep{haiminen2008algorithms}, histogram construction
\citep{halim2009fast}.

The real data analysis problem that motivates this work is the
detection of peaks in ChIP-seq data, which typically come from
genome-wide assays for histone modifications or transcription factor
binding sites \citep{practical}. These data are typically represented
as a vector of non-negative counts $\mathbf y\in\ZZ_+^n$ of aligned
sequence reads for $n$ bases in a genome. A peak detection algorithm
can be represented as a function $f(\mathbf y)\in\{0,1\}^n$ for binary
classification at every base position. The positive class is peaks
(genomic regions with large counts, representing protein binding or
modification) and the negative class is background noise with small
counts.


Since peaks and noise occur in long contiguous genomic regions, the
up-down constrained change-point model was proposed for peak
detection. The Constrained Dynamic Programming Algorithm (CDPA) of
\citet{HOCKING-PeakSeg} was shown to achieve state-of-the-art peak
detection accuracy, but it has two drawbacks. The main drawback is
that the quadratic $O(n^2)$ time complexity for $n$ data points makes
the CDPA too slow to run on large ChIP-seq data sets. The second
drawback is that the algorithm recovers a sub-optimal solution to the
up-down constrained segmentation problem.

\subsection{Contributions and organization}

The main contribution of this paper is a family of dynamic programming
algorithms for solving constrained segmentation problems. In
Section~2, we discuss related work into algorithms for change-point
detection and isotonic regression. Section~3 introduces one
optimization problem that our algorithm can solve: segment
neighborhood isotonic regression. We propose a new dynamic programming
algorithm in Section~4, and show how it can be extended to related
models in Section~5. We apply our algorithm to the up-down constrained
problem of detecting peaks in ChIP-seq data, and show state-of-the-art
speed and accuracy results in Section~6. We provide a brief discussion in
Section~7, and give implementation details in Section~8.

\section{Related work}
\label{sec:related}

The models we consider in this paper can be considered constrained
versions of optimal segmentation \citep{Segmentor} and isotonic
regression \citep{mair2009isotone}. The optimal segmentation model can
be computed using a dynamic programming algorithm (DPA)
\citep{bellman, segment-neighborhood, optimal-partitioning}, or a
pruned dynamic programming algorithm (pDPA) \citep{pruned-dp}. Both
algorithms are guaranteed to recover the exact solution to the
unconstrained model, but there are two important differences. The pDPA
is more complicated to implement, but is also computationally faster
than the DPA. For segmenting a sequence of $n$ data points, the pDPA
takes on average $O(n\log n)$ time whereas the DPA takes $O(n^2)$
time.

The constraints that we consider in this paper are a generalization of
the peak detection model \citep{HOCKING-PeakSeg}. Rather than
searching all possible change-points to find the most likely model
with $K$ segments, we propose to constrain the possible change-points
so that the segment means may be more easily interpreted.

\section{Isotonic regression and segment neighborhood models}

We first explain the classical isotonic regression model which has
inequality constraints on segment means, but no limit on the number of
segments \citep{mair2009isotone}. Then we explain the segment
neighborhood model \citep{segment-neighborhood}, which limits the
number of segment means, but has no inequality constraints. Finally,
we discuss a combination of the two models which we call segment
neighborhood isotonic regression (SNIR).

\subsection{Classical isotonic regression}

For a data set $\mathbf y\in\RR^n$, the classical isotonic regression
model is defined as the most likely sequence of increasing segment
means $\mathbf m = \left[
\begin{array}{ccc}
  m_1& \cdots &m_n
\end{array}
\right]
\in\RR^n$. More precisely, consider the following definition
of the set of possible mean vectors.
\begin{equation}
  \mathcal I^n = \{\mathbf m\in\RR^n \mid m_i \leq m_{i+1} \ \forall\ i<n\}
\end{equation}
Now, assume that the data are a realization of a probability
distribution with mean parameter $\mathbf m$. For example, assuming
$y_i \sim \mathcal N(m_i, \sigma^2)$ results in the following maximum
likelihood isotonic regression problem

\begin{equation}
  \label{eq:max_lik}
  \maximize_{\mathbf m\in\mathcal I^n, \sigma} \sum_{i=1}^n \Lik(y_i| m_i, \sigma^2).
\end{equation}

It is easy to show that the above concave maximization problem is equivalent
to the convex minimization problem below,
\begin{equation}
  \label{eq:isotonic}
  %\tag{\textbf{IsotonicRegression}}
  \minimize_{\mathbf m\in\mathcal I^n} \sum_{i=1}^n \ell(y_i, m_i),
\end{equation}
where the convex loss function $\ell:\RR\times \RR\rightarrow\RR$ in
this case is the square loss $\ell(y, m) = (y-m)^2$. This optimization
problem (\ref{eq:isotonic}) is referred to as isotonic regression, and
can be efficiently solved in $O(n)$ time using the
Pool-Adjacent-Violators Algorithm (PAVA) \citep{isotonic-unifying}.

One potential problem with the isotonic regression model is that it
has no limit on the number of change-points where $m_i < m_{i+1}$. In
particular, for an ever-increasing data set with $y_i < y_{i+1}$ for
all $i<n$, it is clear that the solution to the isotonic regression
problem (\ref{eq:isotonic}) is $m_i=y_i$ for all $i$, which implies
$n-1$ change-points. 

In contrast, it may be preferable to recover the $K\leq n$ most likely
segments (the $K-1$ most likely changes). For example, consider the
toy data set $\mathbf y= \left[
\begin{array}{cccccc}
  2 & 5 & 30 & 34 & 600 & 621
\end{array}
\right] \in\RR^6$. These data are strictly increasing, so the isotonic
regression (\ref{eq:isotonic}) solution is the trivial model
$m_i=y_i$. However, these data contain only two abrupt changes. To
recover these changes, we could instead use the segment
neighborhood model, which we discuss in the next section.

\subsection{Segment neighborhood}

The segment neighboorhood model of \citet{segment-neighborhood} uses
the same cost function as isotonic regression, but a different
constraint set. Let $K\leq n$ be the number of segments, and define
the set of segment means with $K-1$ change-points as
\begin{equation}
  \label{eq:Sk}
  \mathcal S_K^n = \left\{
  \mathbf m\in\RR^n
  \mid
  \sum_{i=1}^{n-1} I(m_i \neq m_{i+1}) = K-1
  \right\}.
\end{equation}
Using this constraint set with the same objective function results in
the following segment neighborhood problem:
\begin{equation}
  \label{eq:optimal_segment_neighborhood}
  \minimize_{\mathbf m\in\mathcal S_K^n} \sum_{i=1}^n \ell(y_i, m_i).
\end{equation}
The segment neighborhood problem can be efficiently solved in
$O(K n \log n)$ time using the pruned dynamic programming algorithm of
\citet{pruned-dp}.

Unlike isotonic regression, the segment neighborhood model
does not constrain the direction of the changes. Thus, for some
data sets $\mathbf y$, the segment neighborhood model may
recover a change down ($m_i > m_{i+1}$). For applications where
isotonic regression is used, it would be desirable to compute a model
with $K$ non-decreasing segment means. This results in the segment neighborhood
isotonic regression problem, which we introduce in the next section.

\subsection{Segment neighborhood isotonic regression (SNIR)}

The idea of fitting a non-decreasing function with a limited number of
change-points has been previously described as ``reduced isotonic
regression'' \citep{reduced-monotonic-regression,
  hardwick2014optimal}. In this paper we refer to the optimization
problem as Segment Neighborhood Isotonic Regression (SNIR) in order to
emphasize the similarity to the segment neighborhood problem
(\ref{eq:optimal_segment_neighborhood}). Both problems impose a
constraint of at most $K$ segment means ($K-1$
change-points). However, the SNIR problem imposes an additional
constraint that the segment mean must never decrease.

\begin{definition}[SNIR constraints]
  \label{def:I}
  Let $(\mathbf m, \mathbf c)\in\mathcal I_K^n$ be the set of all segment means
  $\mathbf m\in\RR^n$ and change-point indicators
  $\mathbf c\in\{0,1\}^{n-1}$ such that the following constraints are
  verified. The total number of non-zero change-point indicators is $K-1$:
  \begin{equation}
    \label{eq:isotonic_segments}
    \sum_{i=1}^{n-1} I(c_i = 1) = K-1.
  \end{equation}
  Every zero-valued change-point indicator has an equal segment mean
  after:
  \begin{equation}
    \label{eq:isotonic_0}
    c_i = 0 \Rightarrow m_i = m_{i+1}.
  \end{equation}
  Every one-valued change-point indicator may have a greater segment
  mean after:
  \begin{equation}
    \label{eq:isotonic_1}
    c_i = 1 \Rightarrow m_i \leq m_{i+1}.
  \end{equation}
\end{definition}

For a given data set $\mathbf y\in\RR^n$, loss function $\ell$, and
number of segments $K$, we define the segment neighborhood isotonic regression
problem as
\begin{equation}
  \label{eq:SNIR}
  \minimize_{(\mathbf m, \mathbf c)\in\mathcal I_K^n} \sum_{i=1}^n \ell(y_i, m_i).
\end{equation}

In the next section, we propose to compute the segment neighborhood isotonic regression solution
using an efficient dynamic programming algorithm.

%%%% update rules
%\newcommand{\FCC}{\widetilde{C}}
\newcommand{\FCC}{C}
\newcommand{\M}{\mathcal{M}}
\section{Dynamic programming algorithm for segment neighborhood isotonic regression}

In this section we propose a new dynamic programming algorithm that
can be used to compute the solution to the segment neighborhood isotonic
regression problem (\ref{eq:SNIR}). First, we show
that it is equivalent to solve a simpler problem with fewer
optimization variables. Then, we show how to recursively compute the
solution using dynamic programming.

\subsection{Equivalent problem with fewer optimization 
variables}

First, consider the following definition of a smaller optimization
space with only $K$ segment means and $K-1$ change-point indices.

\begin{definition}[Smaller optimization space for SNIR]
\label{def:Ibar}
  Let $(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K$ be the set of
  all non-decreasing segment means $u_1\leq\cdots\leq u_K$ and
  increasing change-point indices $0=t_0<t_1<\cdots<t_{K-1}<t_K=n$.
\end{definition}

Each segment mean $u_k$ in this optimization problem is assigned to
data points $i\in(t_{k-1},t_k]$, resulting in the following cost
for each segment $k\in\{1, \dots, K\}$,

\begin{equation}
  \label{eq:h}
  h_{t_{k-1}, t_k}(u_k) = \sum_{i=t_{k-1}+1}^{t_k} \ell(y_i, u_k).
\end{equation}
This results in the following optimization problem,
\begin{equation}
  \label{eq:isotonic_ut}
  \minimize_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K}
  \sum_{k=1}^K
  h_{t_{k-1}, t_k}(u_k)
\end{equation}
Rather than explicitly summing over data points $i$ as in problem
(\ref{eq:isotonic}), this problem sums over segments $k$. The next lemma
proves that it is equivalent to solve this simpler optimization problem.

\begin{lemma}[Equivalence of problem with fewer  variables]
  \label{lemma:fewer-variables}
  Let $(\mathbf u, \mathbf t)$ be the solution to problem with fewer
  optimization variables (\ref{eq:isotonic_ut}), and consider the
  following mapping from the smaller space $\bar{\mathcal I}_K^n$ to
  the original larger space $\mathcal I_K^n$. For each segment
  $k\in\{1,\dots,K\}$, we assign $m_i = u_k$ for all data points
  $i\in(t_{k-1},t_k]$ on that segment. For all
  $i\in\{t_1,\dots,t_{K-1}\}$ (data points before change-points) we
  assign $c_i=1$, and $c_i=0$ for all other data points $i$. Then
  $(\mathbf m, \mathbf c)$ is the solution to problem
  (\ref{eq:isotonic}).
\end{lemma}

\begin{proof}
  It is clear that the mapping defined in
  Lemma~\ref{lemma:fewer-variables} is a bijection between
  $\bar{\mathcal I}_K^n$ and $\mathcal I_K^n$, since the constraints
  in Definitions~\ref{def:I} and~\ref{def:Ibar} are satisfied. Since
  the objective functions (\ref{eq:SNIR}) and (\ref{eq:isotonic_ut})
  are equivalent, the optimization problems are equivalent.
\end{proof}

\subsection{Dynamic programming update rules}

Optimization problem (\ref{eq:isotonic_ut}) has $K$ segment mean
variables and $K-1$ change-point index variables. In this section we
will further eliminate all variables except a single segment mean
variable using dynamic programming. First, we define the optimal cost
functions as follows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% begin new %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% first we define the quantity $\FCC_{K,n}(u)$
%% this quantity is the quantity we will update in the algorithm
%% but its definition is not independant of our algorithm

\begin{definition}[Optimal cost with last segment mean $\mu$]
\label{def:fcc}
  We define $\FCC_{K,n}(\mu)$ the optimal cost of the segmentation
  with $K$ segments, up to data point $n$, with last segment mean
  $\mu$:
%% we take the minimum with the constraint that the last mean (u_k) is mu
\begin{equation}
\FCC_{K,n}(\mu) = \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^K \ | \ u_K = \mu} \
  \left\{ \sum_{k=1}^K
  h_{t_{k-1}, t_k}(u_k) \right\}.
\end{equation}
\end{definition}

As in the PDPA of \citet{pruned-dp}, our proposed dynamic programming
algorithm uses an exact representation of the
$C_{K,n}:\RR\rightarrow\RR$ cost functions, which can be efficiently
computed in terms of the min-less operator. 

\begin{definition}[Min-less operator]
For any real-valued
function $f:\RR\rightarrow\RR$, we define the min-less operator as
$f^\leq(\mu)=\min_{x\leq \mu} f(x)$.
\end{definition}

The min-less operator is used in the following Theorem, which states
the update rules used in our proposed dynamic programming algorithm.

\begin{theorem}[Dynamic programming updates]
  The optimal cost functions $C_{K,n}$ can be recursively computed using the
  following dynamic programming update rules.
\begin{enumerate}
\item For $K=1$ we have
$\FCC_{1,1}(u)=\ell(y_1,u)$, and for the other data
  points $t>1$ we have
\begin{equation}
\FCC_{1,t}(u)=\FCC_{1,t-1}(u)+\ell(y_t,u)
\end{equation}
\item For $K>1$ and $t=K$ we have
\begin{equation}
  \FCC_{K,t}(u)=\ell(y_K, u)+\FCC_{K-1,K-1}^\leq(u)
\end{equation}
\item In all other cases we have
  \begin{equation}
  \FCC_{K,t}(u)=\ell(y_t,u)+
  \min\{
  \FCC_{K-1,t-1}^\leq(u),\,
  \FCC_{K,t-1}(u)
  \}.
  \end{equation}
\end{enumerate}
\end{theorem}

%% we now prove the lemma
%% Case 1 and 2 are true almost by definition 
%% (there is only one possible segmentation in 1) and 
%% (there is only possible segmentation in K of K points)
\begin{proof}
  Case 1 and 2 follow from the definition of $\FCC_{K,t}(u)$, and the
  supplementary materials contain a proof for case 3.

% We now
%   focus on case 3.  First notice that by definition of
%   $\FCC_{K,t+1}(u)$ we must have
%   $\FCC_{K,t+1}(u) \leq \FCC_{K,t}(u) + \ell(y_t,u)$ and also
%   $\FCC_{K,t+1}(u) \leq \FCC_{K-1,t}(u) + \ell(y_t,u)$ (TODO: should
%   there be a $C^\leq$ here?). Thus we have
%   $\FCC_{K,t+1}(u) \leq \min \{ \FCC_{K,t}(u) + \FCC_{K-1,t}(u) \} +
%   \ell(y_{t+1},u)$.

% Now let us assume,
% $$\FCC_{K,t+1}(u) < \min \{ \FCC_{K,t}(u) + \FCC_{K-1,t}(u) \} + \ell(y_{t+1},u).$$
% We will show that this leads to a contradiction.

% We consider the optimal segmentation $(\mathbf u, \mathbf t)\in\bar{\mathcal I}_{t+1}^K$ achieving the optimal $\FCC_{K,t+1}(u)$.
% We consider two possible cases:
% \begin{description}
% \item[Scenario 1: $t_K < t$.]
% Define $\mathbf t'$ such that for all $i < K$, $t'_i = t_i$ and $t'_K = t$.
% We have $(\mathbf u, \mathbf t')\in\bar{\mathcal I}_{t}^K$.
% We can thus decompose $\FCC_{K,t+1}(u)$ as
% $$\FCC_{K,t+1}(u) = \sum_{k=1}^K
%   h_{t'_{k-1}, t'_k}(u_k) + \ell(y_{t+1},u).$$ 
% By assumption we would recover $\sum_{k=1}^K h_{t'_{k-1}, t'_k}(u_k) < \FCC_{K,t}(u)$ 
% which is a contradiction
% by definition of $\FCC_{K,t}(u)$. 
% \item[Scenario 2: $t_K=t$.]
% Define $\mathbf t'$ such that for all $i < K-1$ $t'_i = t_i$ and $t'_{K-1} = t$ as well as
% $\mathbf u'$ such that for all $i \leq K-1$ $u'_i = u_i$.
% We have $(\mathbf u', \mathbf t')\in\bar{\mathcal I}_{t}^{K-1}$.
% We can then decompose $\FCC_{K,t+1}(u)$ as
% $$\FCC_{K,t+1}(u) = \sum_{k=1}^K
%   h_{t'_{k-1}, t'_k}(u'_k) + \ell(y_{t+1},u).$$ 
% By assumption we would recover $\sum_{k=1}^{K-1} h_{t'_{k-1}, t'_k}(u'_k) < \FCC_{K-1,t}(u)$ which is a contradiction
% by definition of $\FCC_{K-1,t}(u)$. 
% \end{description}
\end{proof}

\begin{figure*}
  \centering
  \input{figure-compare-unconstrained}
  \input{figure-compare-cost}
  \vskip -0.5cm
  \caption{Comparison of previous unconstrained algorithm (grey) with
    new algorithm that constrains changes to be non-decreasing (red),
    for the toy data set $\mathbf y= [ 2, 1, 0, 4 ] \in\RR^4$ and the
    square loss. \textbf{Left:} rather than computing the
    unconstrained minimum (constant grey function), the new algorithm
    computes the min-less operator (red), resulting in a larger cost
    when the segment mean is less than the first data point
    ($u\leq 2$). \textbf{Right:} adding the cost of the second data
    point $(u-1)^2$ and minimizing yields equal means
    $u_1=u_2=1.5$ for the constrained model and decreasing 
    means $u_1=2,\, u_2=1$ for the unconstrained model.}
  \label{fig:compare-unconstrained}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% end new %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{definition}[Dynamic programming recursion]
% \label{def:fcc}
%   We refer to $\FCC_{k,n}(u)$ as the optimal cost of the segmentation
%   with $k$ segments, up to data point $n$, with last segment mean
%   $u$. For the first segment $k=1$, we define
%   $\FCC_{1,1}(u)=\ell(y_1,u)$ for the first data point, and
%   $\FCC_{1,t}(u)=\FCC_{1,t-1}(u)+\ell(y_t,u)$ for the other data
%   points $t>1$. For $k>1$ segments, we define
%   $\FCC_{k,k}(u)=\ell(y_k, u)+\FCC_{k-1,k-1}^\leq(u)$ for the $k$-th
%   data point and for $t>k$ data points,
%   \begin{equation}
% \nonumber
%   \FCC_{k,t}(u)=\ell(y_t,u)+
%   \min\{
%   \FCC_{k-1,t-1}^\leq(u),\,
%   \FCC_{k,t-1}(u)
%   \}.
%   \end{equation}
% \end{definition}
% Note that in the original pruned dynamic programming algorithm for
% solving the segment neighborhood problem \citep{pruned-dp}, the
% min-less cost functions $\FCC_{k-1,t-1}^\leq:\RR\rightarrow\RR$ are
% replaced by cost constants $C_{k-1,t-1}\in\RR$
% (Figure~\ref{fig:compare-unconstrained}). The main novelty of our
% proposed algorithm is the computation of the min-less functions in
% closed form.

% Now, consider the following lemma, which shows that the dynamic
% programming minimization over two cost functions is equivalent to the
% minimization over all possible change-points.
% \begin{lemma}[Dynamic programming minimizes with respect to all possible change-points]
% \label{lemma:t_change_points}
%   For the cost up to any data point $t> K$, the recursive dynamic
%   programming cost $\FCC_{K,t}(u)$ is equivalent to the minimum cost
%   over all possible change points
%   $\min_{\tau\in[K-1,t)}\FCC_{K-1,\tau}^\leq(u)+h_{\tau,t}(u)$.
% \end{lemma}

% \begin{proof}
%   We proceed by induction on data points $t$. First, we show that the
%   equivalence holds for $t=K+1$ data points. By definition, we have
%   \begin{eqnarray}
%     \FCC_{K,K+1}(u)
%     &=&\label{eq:proof_fcc1}\ell(y_{K+1},u)+\min\{\FCC_{K-1,K}^\leq(u),\,\FCC_{K,K}(u)\}\\
%     &=&\min\label{eq:proof_fcc2}
%         \begin{cases}
%           \FCC_{K-1,K}^\leq(u)+\ell(y_{K+1},u)\\
%           \FCC_{K-1,K-1}^\leq(u)+\ell(y_{K+1},u)+\ell(y_K,u)
%         \end{cases}\\
%     &=&\min\label{eq:proof_h}
%         \begin{cases}
%           \FCC_{K-1,K}^\leq(u)+h_{K,K+1}(u)\\
%           \FCC_{K-1,K-1}^\leq(u)+h_{K-1,K+1}(u)
%         \end{cases}\\
%     \label{eq:proof_tau}
%     &=&\min_{\tau\in[K-1,K+1)} \FCC_{K-1,\tau}^\leq(u)+h_{\tau,K+1}(u)
%   \end{eqnarray}
%   Equations (\ref{eq:proof_fcc1}-\ref{eq:proof_fcc2}) result by
%   expanding $\FCC_{K,K+1}$ and $\FCC_{K,K}$ using
%   Definition~\ref{def:fcc}. Equation (\ref{eq:proof_h}) follows from
%   the definition of $h_{K,K+1}$ and $h_{K-1,K+1}$ in
%   (\ref{eq:h}). Finally, equation (\ref{eq:proof_tau}) results from
%   introducing the change-point optimization variable $\tau$. Thus, we
%   have proved that the equivalence holds for $t=K+1$ data points.

%   Now, we
%   assume that the equivalence holds for $t$ data points, and prove it to be true
%   for $t+1$ data points.
%   \begin{eqnarray}
%     \FCC_{K,t+1}(u)\label{eq:proof_fcct1}
%     &=&\ell(y_{t+1},u)+\min\{\FCC_{K-1,t}^\leq(u),\,\FCC_{K,t}(u)\}\\
%     &=&\min\label{eq:proof_induction_h}
%         \begin{cases}
%           \FCC_{K-1,t}^\leq(u)+h_{t,t+1}(u)\\
%           \min_{\tau\in[K-1,t)} \FCC^\leq_{K-1,\tau}(u)+h_{\tau,t+1}(u)
%         \end{cases}\\
%     &=&\min_{\tau\in[K-1,t+1)} \FCC^\leq_{K-1,\tau}(u)+h_{\tau,t+1}(u)
%         \label{eq:proof_tau2}
%   \end{eqnarray}
%   Equation (\ref{eq:proof_fcct1}) results by expanding $\FCC_{K,t+1}$
%   using Definition~\ref{def:fcc}. Equation
%   (\ref{eq:proof_induction_h}) follows from the definition of
%   $h_{t,t+1}$ and the induction assumption. Finally, equation
%   (\ref{eq:proof_tau2}) results from re-writing the top $h_{t,t+1}$ term using the
%   change-point optimization variable $\tau$.  This concludes the proof
%   by induction.
% \end{proof}

% Having proved Lemma~\ref{lemma:t_change_points}, we now use it to
% prove the following theorem about the optimality of the dynamic
% programming solution.
% \begin{theorem}[Dynamic programming recovers the segment neighborhood isotonic regression solution]
%   For a data set $\mathbf y\in\RR^n$, and any number of segments $K\leq n$,
%   the optimal dynamic programming cost $\min_u \FCC_{K,n}(u)$ is
%   equivalent to the minimum value of the segment neighborhood isotonic
%   regression problem (\ref{eq:isotonic_ut}).
% \end{theorem}
% \begin{proof}
%   We proceed by induction on segments $K$. First, consider the case of $K=2$ segments:
% \begin{eqnarray}
%   \label{eq:isotonic_ut_2}
%   \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^2}
%   \sum_{k=1}^2
%   h_{t_{k-1}, t_k}(u_k)
%   &= &
%   \min_{t_1, u_2}
%   h_{t_1,n}(u_2)
%   +\min_{u_1\leq u_2}
%   h_{0,t_1}(u_1)\\
%   &=&
%       \label{eq:min-less-2}
%   \min_{t_1, u_2}
%   h_{t_1,n}(u_2)
%   +
%   h^\leq_{0,t_1}(u_2)\\
%   &=&
%       \label{eq:u2}
%   \min_{u_2} \FCC_{2, n}(u_2).
% \end{eqnarray}
% The first equality (\ref{eq:isotonic_ut_2}) follows from
% expanding the optimization variables and the sum. The second equality
% (\ref{eq:min-less-2}) follows from the definition of the min-less
% operator (\ref{eq:min-less-def}). The final equality (\ref{eq:u2})
% follows from Lemma~\ref{lemma:t_change_points}. Thus, the dynamic
% programming recursion solves the segment neighborhood isotonic regression
% problem for $K=2$ segments.

% To complete the proof by induction, we assume that the equality holds for
% $K$ segments, and prove that it holds for $K+1$ segments.
% \begin{eqnarray}
%   \label{eq:proof_separate_tau}
%   \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_n^{K+1}}
%   \sum_{k=1}^{K+1}
%   h_{t_{k-1}, t_k}(u_k)
%   &= & \label{eq:proof_hkexpand}\min_\tau
%   \min_{(\mathbf u, \mathbf t)\in\bar{\mathcal I}_\tau^K}
%        \left[
%        \sum_{k=1}^K
%        h_{t_{k-1},t_k}(u_k)
%        \right]
%        +\min_{u_{K+1}\geq u_K}
%        h_{\tau, n} (u_{K+1})\\
% &=& \min_\tau\min_{u_K} \FCC_{K,\tau}(u_K)\label{eq:proof_Ckt_induction}
%     +\min_{u_{K+1}\geq u_K} h_{\tau,n}(u_{K+1})\\
% % &=& \min_\tau \min_{u_{K+1}} h_{\tau,n}(u_{K+1})
% % +\min_{u_K\leq u_{K+1}} \FCC_{K,\tau}(u_K)\\
% &=& \min_\tau \min_{u_{K+1}} h_{\tau,n}(u_{K+1})\label{eq:proof_min_less_def_2}
% +\FCC_{K,\tau}^\leq(u_{K+1})\\
% &=& \min_{u_{K+1}} \FCC_{K+1,n}(u_{K+1}).\label{eq:proof_remove_tau}
% \end{eqnarray}
% Equation (\ref{eq:proof_hkexpand}) follows by removing the $k=K+1$
% term from the sum, and (\ref{eq:proof_Ckt_induction}) follows from the
% induction assumption. Equation (\ref{eq:proof_min_less_def_2}) follows
% from the definition of the min-less operator (\ref{eq:min-less-def}),
% and the final equality (\ref{eq:proof_remove_tau}) follows from
% Lemma~\ref{lemma:t_change_points}. This concludes the proof by induction.
% \end{proof}

\subsection{Algorithm for solving SNIR}
\label{sec:decoding}
In the previous sections we have only discussed computation of the
optimal cost, but in this section we discuss how to store and compute
the optimal segment mean and change-point parameters. The dynamic
programming algorithm requires storage of real-valued cost functions
$\FCC_{k,t}(u)$ for all possible values of the last segment mean
parameter $u$. We propose to store each cost function as a piecewise
function on intervals, as in the original pruned dynamic programming
algorithm \citep{pruned-dp}.

We therefore propose the following data structures and sub-routines for
the computation:
\begin{itemize}
\item FunctionPiece: a data structure which represents one piece of a
  cost function. It has coefficients which depend on the convex loss
  function (for the square loss it has three coefficients), and it
  always has elements for min/max mean values
  $[\underline u, \overline u]$, and previous segment endpoint $t'$
  and mean $u'$.
\item FunctionPieceList: an ordered list of FunctionPiece objects,
  which exactly stores a cost function $\FCC_{k,t}(u)$ for all values
  of last segment mean $u$.
\item $\text{OnePiece}(y, \underline u, \overline u)$: initialize a
  FunctionPieceList with just one FunctionPiece $\ell(y, u)$ defined
  on $[\underline u, \overline u]$.
\item $\text{MinLess}(t, f)$: an algorithm that inputs a change-point
  and a FunctionPieceList, and outputs the corresponding min-less
  operator $f^\leq$ (another FunctionPieceList), with the previous
  change-point set to $t'=t$ for each of its pieces. This algorithm
  also needs to store the previous mean value $u'$ for each of the
  function pieces. The supplementary materials
  (Section~\ref{sec:implementation-details}) contains
  pseudocode for this algorithm.
\item $\text{MinOfTwo} (f_1, f_2)$: an algorithm that inputs two
  FunctionPieceList objects, and outputs another FunctionPieceList
  object which is their minimum. The supplementary materials
  (Section~\ref{sec:implementation-details}) contains
  pseudocode for this algorithm.
\item $\text{ArgMin}(f)$: an algorithm that inputs a FunctionPieceList
  and outputs three values: the optimal mean $u^*=\argmin_u f(u)$, the
  previous segment end $t'$ and mean $u'$.
\item $\text{FindMean}(u, f)$ an algorithm that inputs a mean value
  and a FunctionPieceList. It finds the FunctionPiece in $f$ with mean
  $u\in[\underline u, \overline u]$ contained in its interval, then
  outputs the previous segment end $t'$ and mean $u'$ stored in that
  FunctionPiece.
\end{itemize}
The above data structures and sub-routines are used in the following
pseudo-code, which describes an algorithm for solving the SNIR
problem.
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Input: data set $\mathbf y\in\RR^n$, segments $K\in\{2,\dots, n\}$.
\STATE Output: matrices of optimal segment means\\ $U\in\RR^{K\times K}$ 
and ends $T\in\{1,\dots,n\}^{K\times K}$
\STATE Compute min $\underline y$ and max $\overline y$ of $\mathbf y$.
\label{line:min-max}
\STATE $\FCC_{1,1}\gets \text{OnePiece}(y_1, \underline y, \overline y)$
\label{line:init-1}
\STATE for data points $t$ from 2 to $n$:
\begin{ALC@g}
  \STATE $\FCC_{1,t}\gets \text{OnePiece}(y_t, \underline y, \overline y) + \FCC_{1,t-1}$
\label{line:init-t}
\end{ALC@g}
\STATE for $k$ from 2 to $K$: for $t$ from $k$ to $n$: // DP
\label{line:for-k-t}
\begin{ALC@g}
  \STATE $\text{min\_prev}\gets \text{MinLess}(t-1, \FCC_{k-1,t-1})$ 
  \label{line:MinLess}
  % \STATE if $t=k$:
  % \begin{ALC@g}
  %   \STATE $\text{min\_new}\gets\text{min\_prev}$ // there is only one
  %   possible change-point, before $t$
  % \end{ALC@g}
  % \STATE else:
  % \begin{ALC@g}
  %   \STATE $\text{min\_new}\gets\text{MinOfTwo}(\text{min\_prev}, \FCC_{k, t-1})$
  % \end{ALC@g}
    \STATE $\text{min\_new}\gets\text{min\_prev}$ if $t=k$, 
else $\text{MinOfTwo}(\text{min\_prev}, \FCC_{k, t-1})$
  \label{line:MinOfTwo}
  \STATE $\FCC_{k,t}\gets \text{min\_new} + \text{OnePiece}(y_t, \underline y, \overline y)$
  \label{line:AddNew}
\end{ALC@g}
\STATE for segments $k$ from 1 to $K$: // decoding
\label{line:for-k-decoding}
\begin{ALC@g}
  \STATE $u^*,t',u'\gets \text{ArgMin}(\FCC_{k,n})$
  \label{line:ArgMin}
  \STATE $U_{k,k}\gets u^*;\, T_{k,k}\gets t'$ 
  \label{line:decode-kk}
  \STATE for segment $s$ from $k-1$ to $1$: 
  \label{line:for-s-decoding}
  \begin{ALC@g}
    \STATE if $u' < \infty$: $u^*\gets u'$ // equality constraint active
    \label{line:equality-constraint-active}
    \STATE $t',u'\gets\text{FindMean}(u^*, \FCC_{s,t'})$
    \label{line:FindMean}
    \STATE $U_{k,s}\gets u^*;\, T_{k,s}\gets t'$ 
    \label{line:decode-ks}
  \end{ALC@g}
\end{ALC@g}
\caption{\label{algo:SNIR} SNIR solver.}
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:SNIR} begins by computing the min/max on
line~\ref{line:min-max}.  The main storage of the algorithm is
$\FCC_{k,t}$, which should be initialized as a $K\times n$ array of
empty FunctionPieceList objects. The computation of $\FCC_{1,t}$ for
all $t$ occurs on lines~\ref{line:init-1}--\ref{line:init-t}. 
The dynamic programming updates occur in the for loops on
lines~\ref{line:for-k-t}--\ref{line:AddNew}. Line~\ref{line:MinLess}
uses the MinLess sub-routine to compute the temporary
FunctionPieceList min\_prev (which represents the function
$\FCC_{k-1,t-1}^\leq$). Line~\ref{line:MinOfTwo} sets the temporary
FunctionPieceList min\_new to the cost of the only possible
change-point if $t=k$; otherwise, it uses the MinOfTwo sub-routine to
compute the cost of the best change-point for every possible mean
value. Line~\ref{line:AddNew} adds the cost of data point $t$, and
stores the resulting FunctionPieceList in $\FCC_{k,t}$.

The decoding of the optimal segment mean $U$ (a $K\times K$ array of
real numbers) and end $T$ (a $K\times K$ array of integers) variables
occurs in the for loops on
lines~\ref{line:for-k-decoding}--\ref{line:decode-ks}. For a given
model size $k$, the decoding begins on line~\ref{line:ArgMin} by using
the ArgMin sub-routine to solve $u^* = \argmin_u \FCC_{k,n}(u)$ (the
optimal values for the previous segment end $t'$ and mean $u'$ are
also returned). Now we know that $u^*$ is the optimal mean of the last
($k$-th) segment, which occurs from data point $t'+1$ to $n$. These
values are stored in $U_{k,k}$ and $T_{k,k}$
(line~\ref{line:decode-kk}). And we already know that the optimal mean
of segment $k-1$ is $u'$.  Note that the $u'=\infty$ flag means that
the equality constraint is active
(line~\ref{line:equality-constraint-active}). The decoding of the
other segments $s<k$ proceeds using the FindMean sub-routine
(line~\ref{line:FindMean}). It takes the cost $\FCC_{s,t'}$ of the
best model in $s$ segments up to data point $t'$, finds the
FunctionPiece that stores the cost of $u^*$, and returns the new
optimal values of the previous segment end $t'$ and mean $u'$. The
mean of segment $s$ is stored in $U_{k,s}$ and the end of segment
$s-1$ is stored in $T_{k,s}$ (line~\ref{line:decode-ks}).

The time complexity of Algorithm~\ref{algo:SNIR} is $O(K n I)$ where
$I$ is the complexity of the MinLess and MinOfTwo sub-routines, which
is linear in the number of intervals (FunctionPiece objects) that are
used to represent the cost functions. There are data sets for which
the number of intervals $I=O(n)$, implying a worst-case time complexity of $O(K n^2)$
\citep{pruned-dp-new}. However, the average number of intervals in real
data sets is empirically $I=O(\log n)$, as we will show in
Section~\ref{sec:results_time}. Thus the average time complexity of
Algorithm~\ref{algo:SNIR} is $O(K n \log n)$.

\subsection{Example and comparison with unconstrained algorithm}

To clarify the discussion, consider the 
toy data set $\mathbf y= \left[
\begin{array}{cccccc}
  2 & 1 & 0 & 4
\end{array}
\right] \in\RR^4$ and the square loss $\ell(y,m)=(y-m)^2$. The first
step of the algorithm is to compute the minimum and the maximum of the
data (0,4). Then the algorithm computes the optimal cost in $k=1$
segment up to data point $t=1$:
\begin{equation}
  \FCC_{1,1}(u) = (2-u)^2=4 - 4u + u^2.
\end{equation}
It is clear that this function can be stored for all values of $u$ via
the three real-valued coefficients ($\text{constant}=4$,
$\text{linear}=-4$, $\text{quadratic}=1$). To compute the optimal cost
in $K=2$ segments, we first need to compute the min-less operator of
this function (red curve on left of Figure~\ref{fig:compare-unconstrained}):
\begin{equation}
  \FCC_{1,1}^\leq(u) =
%\min_{u'\leq u}\FCC_{1,1}(u')=
  \begin{cases}
    4 - 4u + u^2 &\text{ if }u\in[0,2],\, u'=u\\
    0 + 0u + 0u^2 & \text{ if }u\in[2,4],\,  u'=2
  \end{cases}
\end{equation}
It is clear that this min-less function can be stored as a list of
coefficients and associated intervals of $u$ values.  
In addition, to
facilitate recovery of the optimal parameters, we store the previous
segment mean $u'$ and endpoint. Note that $u'=u$ means that the
equality constraint is active ($u_1=u_2$). The optimal cost in $K=2$
segments up to data point $t=2$ is then
\begin{equation}
  \FCC_{2,2}(u) = 
%\FCC_{1,1}^\leq(u)+(1-u)^2 = 
  \begin{cases}
    5 - 6u + 2u^2 &\text{ if }u\in[0,2],\,  u'=u\\
    1 - 2u + 1u^2 &\text{ if }u\in[2,4],\,  u'=2
  \end{cases}
\end{equation}
Note that the minimum of this function is achieved at $u=1.5$ which
occurs in the first of the two function pieces (red curve on right of
Figure~\ref{fig:compare-unconstrained}), with an equality constraint
active. This implies the optimal model up to data point $t=2$ with
$k=2$ up-down segments actually has no changes ($u_1=u_2=1.5$). In contrast, the minimum of the cost computed by the unconstrained algorithm is at $u_2=1$ (grey curve on right of
Figure~\ref{fig:compare-unconstrained}), resulting in a change down.

\subsection{The PeakSeg up-down constraint}
\label{sec:PeakSeg}

The PeakSeg model described by \citet{HOCKING-PeakSeg} is the most
likely segmentation where the first change is up, all up changes are
followed by down changes, and all down changes are followed by up
changes. More precisely, we define the constraint set as follows.
\begin{definition}[PeakSeg constraints]
  \label{def:U}
  Let $(\mathbf m, \mathbf c)\in\mathcal U_K^n$ be the set of all segment means
  $\mathbf m\in\RR^n$ and change-point indicators
  $\mathbf c\in\{-1, 0,1\}^{n-1}$ such that the following constraints are
  verified. The total number of non-zero change-point indicators is $K-1$:
  \begin{equation}
    \label{eq:U_segments}
    \sum_{i=1}^{n-1} I(c_i \neq 0) = K-1.
  \end{equation}
  Every zero-valued change-point indicator has an equal segment mean
  after:
  \begin{equation}
    \label{eq:U_0}
    c_i = 0 \Rightarrow m_i = m_{i+1}.
  \end{equation}
  Every positive change-point indicator may have a greater segment
  mean after:
  \begin{equation}
    \label{eq:U_1}
    c_i = 1 \Rightarrow m_i \leq m_{i+1}.
  \end{equation}
  Every negative change-point indicator may have a smaller segment
  mean after:
  \begin{equation}
    \label{eq:U-1}
    c_i = -1 \Rightarrow m_i \geq m_{i+1}.
  \end{equation}
  The first change is an up change, and every up change is followed by a
  down change, and vice versa. Mathematically, the cumulative sum of
  change-point variables up to $t$ is either zero or one, for all $t<n$:
  \begin{equation}
    \label{eq:U-cum}
    \sum_{i=1}^t c_i \in \{0, 1\}.
  \end{equation}
\end{definition}
The PeakSeg optimization problem uses this constraint set with the usual loss function. 
\begin{equation}
\label{eq:min_PeakSeg}
    \minimize_{
        (\mathbf m, \mathbf c)\in\mathcal U^n_K
      } \ 
\sum_{t=1}^n \ell(y_t, m_t).
\end{equation}

A variant of our proposed algorithm can be used to solve the PeakSeg
problem. The initialization $k=1$ is the same as in the SNIR solver
(Definition~\ref{def:fcc}). The dynamic programming updates for even
$k\in\{2, 4, \dots\}$ are also the same. However, the updates for odd
$k\in\{3, 5, \dots\}$ are
\begin{equation}
  \FCC_{k,t}(u) = \ell(y_t, u) + \min\{
  \FCC_{k-1,t-1}^\geq(u),\, \FCC_{k,t-1}(u)
  \},
\end{equation}
where the min-more operator is defined for any function $f:\RR\rightarrow\RR$ as
\begin{equation}
  \label{eq:min-more-def}
  f^\geq(u) = \min_{x\geq u} f(x).
\end{equation}
To implement this update rule, the only modification to
Algorithm~\ref{algo:SNIR} is at line~\ref{line:MinLess}. Instead of
always using the MinLess sub-routine, the constrained version uses the
MinMore sub-routine for odd segments $k$. We implemented this algorithm
using the Poisson loss $\ell(y, u) = m - y\log m$, since our
application in Section~\ref{sec:results-chip-seq} is on count data
$y\in\ZZ_+ = \{0, 1, 2, \dots\}$. Our free/open-source implementation
is available as the PeakSegPDPA function in the R package coseg
(\url{https://github.com/tdhock/coseg}). More details about how we
implemented this algorithm can be found in the supplementary materials
(Section~\ref{sec:peakseg_details}).

\subsection{General affine inequality constraints
  between adjacent segment means}

% A segmentation $m$ is described as a set of contiguous segments $\{s_1, ... s_{|m|} \}$, where $|m|$ is the number of segments of $m$
% We consider the set of all segmentation up to $n$: $\M_n$ 
% or the set of all possible segmentation in $K$ segments: $\M^K_n$.
% We define $r_m$ as the last segment of $m$.

% We aim at optimizing over all possible segmentations $m$ in $\M^K_n$ or $\M_n$
%  the quantity
% $\sum_{r \in m} \sum_{i \in s_{r}} \ell(y_i, \mu_{r})$ subject to
% the following $K-1$ linear constraints. 

% \begin{eqnarray*}
% a_{1,1}.\mu_1 \ + & a_{1,2}.\mu_2  & \geq  b_1 \\
% \cdots \ +&  \cdots & \geq \cdots \\
% a_{k,k}.\mu_{k} + & a_{k,k+1}.\mu_{k+1}  & \geq  b_{k} \\
% \cdots \ +&  \cdots & \geq \cdots  \\
% a_{K-1,K-1}.\mu_{K-1} \ +& a_{K-1,K}.\mu_K & \geq  b_{K-1},
% \end{eqnarray*}
% with all $a_{k,k+1} \neq 0$, $a_{k,k} \in \mathbb{R}$ and
% $b_{k} \in \bar{\mathbb{R}}.$ In other words we aim at recovering the
% best segmentation with successive mean parameters that obey the
% constraints.

% Some examples:
% \begin{enumerate}
% \item If we take all $a_{k,k+1} =1$, $a_{k,k}=0$ and $b_{k} = - \infty$ we recover the standard segmentation in the mean problem.
% \item If we take all $a_{k,k+1} =1$, $a_{k,k}=-1$ and $b_{k} = 0$ we
%   recover the isotonic regression problem (segment means always
%   increasing).
% \item For the PeakSeg model we take all $b_{k} = 0$. For odd $k$ we
%   take $a_{k,k+1} =1$, $a_{k,k}=-1$ and for even $k$ we take
%   $a_{k,k+1} =-1$, $a_{k,k}=1$.
% \end{enumerate}

%\subsection{Functional cost representation}
% To optimize this quantity we will consider the following functional quantity:

% \begin{equation}
% \FCC^k_t(\mu) =  \underset{m \in \M^K_n, \mu_r |  r \neq r_m}{\min} 
% 		\{ 
% 		   \underset{r \in m, r \neq r_m}{\sum} 
% 		   \underset{i \in r, i \leq t  }{\sum} \ell(y_i, \mu_{r}) 
% 		+ 
% 		   \underset{i \in r_m, i \leq t}{\sum} \ell(y_i, \mu)
% 		\}  
% \end{equation}



% \begin{eqnarray*}
% \text{subject to} \\
% a_{1,1}. \mu_1 \ + & a_{1,2}. \mu_2  & \geq  b_1 \\
% \cdots \ + & \cdots & \geq \cdots \\
% a_{k-1,k-1}. \mu_{k-1} \ + &a_{k-1,k}. \mu_{k}  & \geq  b_{k-1} \\
% \end{eqnarray*}

% $\FCC^k_t(\mu)$ is the best possible cost achievable in $k$ segment up to point $t$ with a $k$-th
% segment mean of $\mu$.

% %\subsection{Update rule}
% We can then consider the following update rule

% \begin{equation}
% \FCC^{k+1}_{t+1}(\mu) = \min \{ \FCC^{k+1}_{t}(\mu)  , \underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}  \} + \ell(y_{t+1}, \mu)
% \label{update}
% \end{equation}

% This update rule states that the best segmentation up to $t+1$ in $k+1$ segment with a last mean element of $\mu$ either has its $k$-th changepoint:
% \begin{itemize}
% \item before $t$ and in that case we should take the best possible segmentation up to $t$ in $k+1$
% segments with a last mean of $\mu$ and then add  $\ell(y_{t+1}, \mu)$, that is:
% $$\FCC^{k+1}_{t}(\mu) + \ell(y_{t+1}, \mu),$$

% \item at $t$ and in that case we should take the best possible segmentation up to $t$ in $k$ segments
% such that the last mean $\mu_k=\mu'$ validates the $k-th$ constraint with $\mu_{k+1}=\mu$ and then add  $\ell(y_{t+1}, \mu)$, that is:
%  $$\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \} + \ell(y_{t+1}, \mu).$$
% \end{itemize}


% %\subsection{Constraint}
% Assuming we have a piecewise description of $\FCC^{k}_{t}(\mu')$ on $I$ ordered intervals of $\mathbb{R}$
% then it is straightforward to recover the function:
% $\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}.$

% The update rule is a priori valid for more complex constraints, typically quadratic constraints, yet recovering
% $\underset{\mu' | a_{k,k}. \mu' + a_{k,k}. \mu  \geq  b_{k}}{\min} \{ \FCC^{k}_{t}(\mu') \}$ from $\FCC^{k}_{t}(\mu')$ would possibly be much more difficult.

In this section we briefly discuss how to apply our proposed dynamic
programming algorithm to optimization problems with general affine
inequality constraints between adjacent segment means.

\begin{definition}[General affine inequality constraints]
\label{def:affine-inequality-constraints}
  Let $a_k,b_k,c_k\in\RR$ for $k\in\{1,\dots,K-1\}$ be arbitrary
  coefficients that define affine functions
  $g_k(u_k, u_{k+1})=a_k u_k + b_k u_{k+1} + c_k$. Then we define
  $(\mathbf u, \mathbf t)\in\mathcal M^n_K$ as the set of all
  increasing change-point indices $0=t_0<t_1<\cdots<t_{K-1}<t_K=n$ and
  segment means $\mathbf u\in\RR^K$ that satisfy
  $g_k(u_k, u_{k+1}) \leq 0$ for all $k\in\{1,\dots, K-1\}$.
\end{definition}
The general optimization problem uses this constraint set with the
standard objective function:
\begin{equation}
  \label{eq:min_general_affine_inequality}
    \minimize_{
        (\mathbf u, \mathbf t)\in\mathcal M^n_K
      } \ 
\sum_{k=1}^K h_{t_{k-1}, t_k}(u_k).
\end{equation}

Some examples:
\begin{enumerate}
\item If we take all $a_k,b_k,c_k=0$ then the constraints are
  trivially satisfied, segment means are not constrained, and we
  recover the segment neighborhood problem
  (\ref{eq:optimal_segment_neighborhood}).
\item If we take all $a_{k} =1$, $b_{k}=-1$ and $c_{k} = 0$ we
  recover the segment neighborhood isotonic regression problem (\ref{eq:SNIR}).
\item To recover the PeakSeg problem (\ref{eq:min_PeakSeg}),
  we take all $c_{k} = 0$. For odd $k\in\{1,3,\dots\}$ we take
  $a_{k} =1$, $b_{k}=-1$ and for even $k\in\{2,4,\dots\}$ we take
  $a_{k} =-1$, $b_{k}=1$.
\end{enumerate}

In the general case, we need to compute the analog of the
min-less/more operator, which we call the constrained minimization
operator. For any cost function $f:\RR\rightarrow\RR$ and constraint
function $g:\RR\times\RR\rightarrow\RR$, we define the constrained
minimization operator $f^g:\RR\rightarrow\RR$ as
\begin{equation}
  \label{eq:constrained-min-operator}
  f^g(u_{k+1}) = \min_{u_k : g(u_k, u_{k+1})\leq 0} f(u_k).
\end{equation}

The constrained minimization operator is used in the following general
dynamic programming update rule which can be used to compute the solution to (\ref{eq:min_general_affine_inequality})
\begin{equation}
  \label{eq:general_dp}
  \FCC_{k,t}(u) = \ell(y_t,u) + \min\{
  \FCC_{k,t-1}(u),\,
  \FCC_{k-1,t-1}^{g_{k-1}}(u)
  \}.
\end{equation}
We note that this update rule is valid for constraint functions $g$
more general than the affine functions described in
Definition~\ref{def:affine-inequality-constraints} (e.g. quadratic
constraint functions). However, the closed-form computation of the
constrained minimization operator (\ref{eq:constrained-min-operator})
would possibly be much more difficult for these more general constraint functions. 

\section{Results on peak detection in ChIP-seq data}
\label{sec:results-chip-seq}

In this section, we show that the dynamic programming algorithm that
we proposed in Section~\ref{sec:PeakSeg} can be used to overcome this
speed drawback, while maintaining state-of-the-art accuracy. We have
implemented the algorithm in C++ code with an interface via the
PeakSegPDPA function in the coseg R package
(\url{https://github.com/tdhock/coseg}). For comparison, we study the
empirical properties of the following algorithms:

\begin{description}
\item[Segmentor3IsBack::Segmentor] is an implementation of a
  functional pruning algorithm for computing the solution to the
  Segment Neighborhood (\ref{eq:optimal_segment_neighborhood}) problem
  \citep{Segmentor}. Its average time complexity is $O(n \log n)$,
  and the solution may or may not obey the up-down constraints on
  segment means. If it does not, then the model is not directly
  interpretable in terms of peaks (segments after up changes) and
  background (segments after down changes), so we discard the model.
\item[PeakSegDP::cDPA] implements a heuristic algorithm with $O(n^2)$
  time complexity which attempts to solve the up-down
  constrained problem \citep{HOCKING-PeakSeg}. Models computed by this
  algorithm are guaranteed to satisfy the up-down constraint, but may
  not be the optimal solution to the up-down constrained problem
  (\ref{eq:min_PeakSeg}).
\item[coseg::PeakSegPDPA] is our proposed solver for the PeakSeg
  problem, described in Section~\ref{sec:PeakSeg}. It recovers the
  optimal solution to the up-down constrained problem
  (\ref{eq:min_PeakSeg}).  Since Definition~\ref{def:U} contains
  non-strict inequality constraints, the optimal solution may include
  adjacent segments with equal mean values. In that case, the model is
  not directly interpretable in terms of peaks and background, so we
  discard the model. We expected the speed of the algorithm to be
  consistent with the $O(n\log n)$ time complexity of other functional
  pruning algorithms such as Segmentor.
\item[MACS] is a heuristic algorithm with unknown time complexity from
  the bioinformatics literature \citep{MACS}. We consider it as a
  baseline, since it has been shown to achieve state-of-the-art peak
  detection accuracy for sharp H3K4me3 histone mark data
  \citep{HOCKING-PeakSeg}.
\item[HMCanBroad] is a another heuristic algorithm with unknown time
  complexity \citep{HMCan}. We consider it as a baseline, since it has
  been shown to achieve state-of-the-art peak detection accuracy for
  broad H3K36me3 histone mark data \citep{HOCKING-PeakSeg}.
\end{description}

We ran each algorithm on the McGill ChIP-seq benchmark data sets
\citep{HOCKING2016-chipseq}. We begin by comparing the speed,
feasibility, and optimality of the three optimization-based
implementations (Segmentor, PeakSegDP, coseg).

\subsection{Empirical time complexity in ChIP-seq data}
\label{sec:results_time}

We considered 2752 segmentation problems, consisting of chromosome
subsets in samples with labels that indicate presence or absence of
peaks. For each segmentation problem we attempted to compute models
with 0, ..., 9 peaks ($k\in\{1,3,\dots,19\}$ segments).

As in the original pruned dynamic programming algorithm, the time
complexity of our proposed algorithm depends on the number of
intervals (candidate change-points) stored in the $\FCC_{k,t}$ cost
functions. \citet{pruned-dp} showed that the number of intervals is at
most $O(n)$ in the worst case, but on average $O(\log n)$. In the left
of Figure~\ref{fig:timings}, we show that the empirical number of
intervals for our algorithm in the ChIP-seq data sets is $O(\log
n)$. For the largest data set ($n=263169$), the algorithm only stored
median=16 and maximum=43 intervals. The most intervals stored was 253
for one data set with $n=7776$. These results suggest that our new
algorithm only stores on average $O(\log n)$ intervals (possible
change-points), as in the original pruned dynamic programming
algorithm.

We used the \verb|system.time| function in R to record the timings of
each optimization-based algorithm (Segmentor, PeakSegDP, coseg). The
plot on the right in Figure~\ref{fig:timings} shows that the amount of
computation time increases with the number of data points. As
expected, our proposed coseg::PeakSegPDPA algorithm shows $O(n\log n)$
asymptotic timings similar to Segmentor. It is clear that the $O(n^2)$
PeakSegDP algorithm is slower than the other two algorithms,
especially for larger data sets. For the largest data set
($n=263169$), PeakSegDP took over two hours, but coseg took only about
two minutes. Overall, these results suggest that our algorithm enjoys
$O(n\log n)$ time complexity in ChIP-seq data, which makes it possible
to use for very large data sets.

\begin{figure*}[b!]
  \centering
  \parbox{0.49\textwidth}{
    %\includegraphics[width=0.45\textwidth]{figure-PDPA-intervals-all}
    \input{figure-PDPA-intervals-small}
  }
  \parbox{0.49\textwidth}{
    %\includegraphics[width=0.45\textwidth]{figure-PDPA-timings}
    \input{figure-PDPA-timings-small}
  }
  \vskip -0.5cm
  \caption{Timing results on 2752 segmentation problems from the
    McGill histone mark ChIP-seq benchmark data. For each problem we
    ran the PeakSegPDPA with a maximum of $K=19$ segments.
    \textbf{Left}: number of intervals stored in $\FCC_{k,t}$ cost
    functions (median, inter-quartile range, and maximum over all data
    points $t$ and segments $k$), \textbf{Right}: timings in seconds
    (median line and min/max band).}
  \label{fig:timings}
\end{figure*}

\subsection{Feasibility and optimality in ChIP-seq data}

For each of 2752 segmentation problems, we attempt to compute models
with 0, ..., 9 peaks, so there are a total of 27520 possible models
for each optimization-based algorithm (Segmentor, PeakSegDP,
coseg). However, none of the algorithms is theoretically guaranteed to
return a model which is feasible for the up-down constraint (PeakSegDP
either recovers an up-down model or no model at all; when coseg and
Segmentor recovered models that did not obey the PeakSeg up-down
constraints, we discarded those infeasible models). In this section,
we compare the algorithms in terms of how frequently they recover
models which are feasible and optimal.

We show the number of models which are feasible for the PeakSeg
up-down constraint in Table~\ref{tab:min-train-error}. The PeakSegDP
package computed the most feasible models (27469/27520=99.8\%),
followed by the coseg package (21278/27520=77.3\%), and the Segmentor
package computed the fewest (8106/27520=29.4\%). In terms of
optimality, the cDPA (PeakSegDP R pkg) computes a sub-optimal model
for 7246/27520 = 26.3\% of models. For 1032/7246 of these, the PeakSeg
solution exists and is recovered by our new algo (coseg R pkg) but not
the unconstrained algo (Segmentor R pkg). These results suggest that
in ChIP-seq data sets, the new coseg algorithm is more accurate than
PeakSegDP, in terms of the Poisson likelihood. Furthermore, these
results suggest that coseg is more useful than Segmentor, since there
are many cases for which Segmentor does not recover a model that
verifies the up-down constraint on the segment means.
% Numbers come from figure-PDPA-cDPA-compare.R

% \subsection{Minimum train error in ChIP-seq data}

% We quantified the minimum train error for each optimal segmentation
% algorithm for each of the 2752 problems, by selecting the number of
% peaks $p\in\{0, ..., 9\}$ which had the minimum number of incorrect
% labels (total error = false positives + false negatives). As suggested
% by \citet{HOCKING2016-chipseq}, the baseline MACS algorithm was
% trained by varying the qvalue parameter between 0 and 0.8, and the
% baseline HMCanBroad algorithm was trained by varying the
% finalThreshold parameter between $10^{-10}$ and $10^5$.

% The minimum train error for each algorithm is shown in
% Table~\ref{tab:min-train-error}. The algorithm with the smallest
% minimum train error was PeakSegDP (677/12826=5.3\%), followed by coseg
% (789/12826=6.2\%). The other algorithms had much larger minimum train
% error rates (10.1\%--21.7\%). These results suggest that the new coseg
% algorithm can find segmentation models which are nearly as accurate as
% the previous state-of-the-art PeakSegDP method.


\subsection{Test error in ChIP-seq data}

To compute the accuracy of the algorithms in the benchmark data sets,
we quantified false positive and false negative peak detection error
rates using the labels that indicate presence or absence of peaks in
specific samples and genomic regions
\citep{HOCKING2016-chipseq}. Briefly, a false negative occurs when no
peak is predicted in a region with a positive label, and a false
positive occurs when a peak is predicted in a region with a negative
label.

Finally, we performed 4-fold cross-validation to estimate the test
error of each algorithm. For each of the 7 data sets, we randomly
assigned labeled data to one of four folds. For each fold, we treat it
as a test set, and train a model using all other folds. For the
optimal segmentation models (coseg, PeakSegDP, Segmentor), we select
the number of segments using an oracle penalty
\citep{cleynen2013segmentation}. For a given data set
$\mathbf y\in\ZZ_+^n$, let $\mathbf m^s$ for
$s\in\mathcal S\subseteq \{1, 3,\dots, 19\}$ be the segment mean
vectors that obey the PeakSeg up-down constraint. The oracle.1 model
selection criterion described by \citet{HOCKING-PeakSeg} is

\begin{equation}
  \label{eq:oracle}
  s^*(\lambda) = \argmin_{s\in\mathcal S}
  \lambda s\left(1 + 4\sqrt{1.1 + \log(n/s)}\right)^2
  +\sum_{i=1}^n \ell(y_i, m_i^s)
\end{equation}

We compute ROC curves for the optimization-based algorithms (coseg,
PeakSegDP, Segmentor) by varying the
$\lambda\in\{10^{-2}, \dots,10^4\}$ penalty parameter. We compute ROC
curves for the baseline MACS and HMCanBroad algorithms by varying a
single significance threshold parameter (same as for computing minimum
train error in the previous section).

The previous algorithm with state-of-the-art accuracy on this
benchmark was PeakSegDP, and we observed that the new coseg algorithm
achieves comparable test AUC (Figure~\ref{fig:test-error-dots}). In
contrast, the unconstrained Segmentor algorithm often had much lower
test AUC, because of lower true positive rates. Although the baseline
HMCanBroad algorithm showed state-of-the-art test AUC in 2/3 broad
H3K36me3 data sets, it was very inaccurate for sharp H3K4me3 data
sets. The baseline MACS algorithm had test AUC values much lower than
the optimization-based algorithms in all 7 data sets. Overall, these
data indicate that the proposed coseg algorithm achieves
state-of-the-art accuracy in these ChIP-seq data sets.

\begin{figure*}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-test-error-dots}
  \vskip -0.5cm
  \caption{Four-fold cross-validation was used to estimate prediction
    accuracy of each algorithm (one panel for each of 7 ChIP-seq data sets). 
Each black circle shows the test AUC in one of four
    cross-validation folds, the shaded grey circle is the mean, and
    the vertical line shows the maximum mean in each data set. It is
    clear that the new algorithm implemented in the coseg R package is
    just as accurate as the slower PeakSegDP heuristic, and both are
    more accurate than the other baseline methods. 
% Interactive version
%     available at
%     \url{http://bl.ocks.org/tdhock/raw/886575874144c3b172ce6b7d7d770b9f/}
  }
  \label{fig:test-error-dots}
\end{figure*}

% \begin{table}[b!]
%   \centering
%   \input{table-min-train-error}
%   \caption{Comparison of algorithms in the ChIP-seq data sets,
%     in terms of minimum train error and number of feasible models. 
%     For each of the 2752 separate segmentation problems, 
%     each algorithm was run with several parameter values (see text for details), 
%     and we selected the parameter with the minimum number of incorrect labels
%     (errors = fp + fn). 
%     The new algorithm implemented in the coseg R package 
%     commits fewer false positives than the slower PeakSegDP heuristic, 
%     and fewer errors than the other baseline methods.
%     The new algorithm computed models that are feasible for the PeakSeg up-down constraint
%     more frequently than the unconstrained Segmentor algo,
%     but less frequently than the PeakSegDP algo.}
%   \label{tab:min-train-error}
% \end{table}

\section{Discussion and conclusions}

\bibliographystyle{icml2016}
\bibliography{refs-abbrev}

\end{document} 

