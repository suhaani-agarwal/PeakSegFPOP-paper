% -*- compile-command: "make HOCKING-PeakSeg-functional-pruning-slides.pdf" -*-
\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{A log-linear time algorithm for constrained 
changepoint detection
}

\author{
  Toby Dylan Hocking\\
  toby.hocking@mail.mcgill.ca\\
  joint work with Guillem Rigaill, Paul Fearnhead, 
  Guillaume Bourque}

\date{13 Nov 2017}

\maketitle

\section{Problem: optimizing ChIP-seq peak detection}

\begin{frame}
  \frametitle{Chromatin immunoprecipitation sequencing (ChIP-seq)}
  Analysis of DNA-protein interactions.

  \includegraphics[width=\textwidth]{Chromatin_immunoprecipitation_sequencing_wide.png}

  Source: ``ChIP-sequencing,'' Wikipedia.
\end{frame}

\begin{frame}
  \frametitle{Problem: find peaks in each of several samples}
  \includegraphics[width=\textwidth]{screenshot-ucsc-edited}

  \begin{itemize}
  \item Grey profiles are noisy aligned read count signals -- \\peaks
    are genomic locations with protein binding sites.
  \item Black bars are peaks called by MACS2 (Zhang et al, 2008) -- many
    false positives! (black bars where there is only noise)
  \item From a machine learning perspective, this is binary
    classification (positive=peaks, negative=noise).
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Previous work in genomic peak detection}
%   \begin{itemize}
%   \item Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
%   \item SICER, Zang et al, 2009.
%   \item HOMER, Heinz et al, 2010.
%   \item CCAT, Xu et al, 2010.
%   \item RSEG, Song et al, 2011.
%   \item Triform, Kornacker et al, 2012.
%   \item Histone modifications in cancer (HMCan), Ashoor et al, 2013.
%   \item PeakSeg, Hocking, Rigaill, Bourque, ICML 2015.
%   %\item PeakSegJoint Hocking and Bourque, arXiv:1506.01286.
%   \item ... dozens of others.
%   \end{itemize}
%   Two big questions: how to choose the best...
%   \begin{itemize}
%   \item ...algorithm? (testing)
%   \item \alert<1>{...parameters? (training)}
%   \end{itemize}
% \end{frame}

% \begin{frame}[fragile]
%   \frametitle{How to choose parameters of unsupervised peak
%     detectors?}
% \scriptsize
% 19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
% \begin{verbatim}
%   [-g GSIZE]
%   [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
%   [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
%   [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
%   [--down-sample] [--seed SEED] [--nolambda]
%   [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
%   [--shift-control] [--half-ext] [--broad]
%   [--broad-cutoff BROADCUTOFF] [--call-summits]
% \end{verbatim}
% 10 parameters for Histone modifications in cancer (HMCan),
% Ashoor et al, 2013.
% \begin{verbatim}
% minLength 145
% medLength 150
% maxLength 155
% smallBinLength 50
% largeBinLength 100000
% pvalueThreshold 0.01
% mergeDistance 200
% iterationThreshold 5
% finalThreshold 0
% maxIter 20
% \end{verbatim}
% \end{frame}
 
\begin{frame}
  \frametitle{Which macs parameter is best for these data?}
  \includegraphics[width=1\textwidth]{figure-macs-problem.png}
\end{frame}

\begin{frame}
  \frametitle{Compute likelihood/loss of piecewise constant model}
  \includegraphics[width=1\textwidth]{figure-macs-problem-7-5.png}
  % $\PoissonLoss(\mathbf z, \mathbf m) = \sum_{i=1}^n m_i - z_i \log(m_i)$
  % for count data $\mathbf z\in\ZZ_+^n$ 
  % and segment mean model $\mathbf m\in\RR^n$.
\end{frame}

\begin{frame}
  \frametitle{Idea: choose the parameter with a lower loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-15.png}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}

  Simple model with only one parameter (number of peaks).

  %Choose the number of peaks via standard penalties (AIC, BIC,
  %  ...)\\or learned penalties based on visual labels (more on this later).
\end{frame}

% \begin{frame}
%   \frametitle{Maximum likelihood Poisson segmentation models}
%   \includegraphics[width=1\textwidth]{figure-Segmentor-PeakSeg}

%   \begin{itemize}
%   \item Previous work: unconstrained maximum likelihood mean\\
%     for $s$ segments ($s-1$ changes), Cleynen et al 2014.
%   \item Hocking et al, ICML 2015: PeakSeg constraint enforces up, down, up,
%     down changes (and not up, up, down). 
%   \item Odd-numbered segments are background noise,\\
%     even-numbered segments are peaks.
%   \item Constrained Dynamic Programming Algorithm, $O(N^2)$ time for $N$ data points.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{But quadratic time is not fast enough for genomic data!}
%   \includegraphics[width=\textwidth]{figure-PDPA-timings-dp}
%   \begin{itemize}
%   \item Genomic data is large, $N \geq 10^6$.
%   \item Split into subsets? What if we split a peak in half?
%   \item Need linear time algorithm for analyzing whole data set.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Statistical model is Poisson with change constraints}
%   \begin{itemize}
%   \item We have $N$ count data $z_1, \dots, z_N\in\ZZ_+$.
%   \item Fix the number of segments $S\in\{1, 2, \dots, N\}$.
%   \item PeakSeg Model: $z_t \sim \text{Poisson}(m_t)$ such that $m_t$
%     has $S-1$ up-down changes.
%   \item Want to find means $m_t$ which maximize the Poisson likelihood:
%     $P(Z = z_t|m_t) = m_t^{z_t} e^{-m_t} / (z_t!)$.
%   \item Equivalent to finding means $m_t$ which minimize the Poisson
%     loss: $\ell(m_t, z_t) = m_t - z_t\log m_t$.
%   \item Naive computation is $O(N^S)$, since there are $O(N^{S-1})$ possible
%     positions for $S-1$ change-points, and it takes $O(N)$ operations to
%     compute the mean and loss for each.
%   % \item Comparison to Hidden Markov Model:
%   %   \begin{description}
%   %   \item[Likelihood] Same emission terms, no transition terms.
%   %   \item[Constraint] Number of changes rather than values.
%   %   \end{description}
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{Maximum likelihood changepoint detection with up-down constraints on adjacent segment means (PeakSeg)}
H {\it et al.}, {\it ICML} 2015. 
    
\only<1>{\input{figure-PeakSeg}}      
\only<2>{\input{figure-PeakSeg-unconstrained}}
\only<3>{\input{figure-PeakSeg-constrained}}
\vskip -1.5cm
\begin{align*}
    \minimize_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} &\ \ 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  x_i) 
  \label{PeakSegPDPA}
\\
      \text{subject to \hskip 0.75cm} &\ \ \alert<3>{u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\},}
  \nonumber\\
  &\ \ \alert<3>{u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\}.}
  \nonumber 
\end{align*}
\vskip -0.4cm
\begin{itemize}  
\item Simple: 1 parameter = number of segments $S\in\{1,3,\dots\}$.
\item Hard optimization problem, naively $O(n^S)$ time.
\item \alert<2>{Previous unconstrained model: not always up-down changes.}
\item \alert<3>{Interpretable: $P=(S-1)/2$ peaks (segments 2, 4, ...).}
\item New $O(Kn^2)$ time approximate algorithm based on classic
  dynamic programming (R package PeakSegDP).
\end{itemize}
\end{frame} 

\section{New functional pruning algorithm}


\begin{frame}
  \frametitle{Relation to previous work}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & \alert<1>{Dynamic Prog. Algo.} & \alert<2>{Pruned DPA} \\
     & \alert<1>{exact $O(SN^2)$} & \alert<2>{exact $O(SN\log N)$}\\
    R pkgs: & \alert<1>{changepoint} & \alert<2>{cghseg, Segmentor}\\
    \hline
    up-down constrained & \alert<3>{Constrained DPA} & \alert<4>{\textbf{GPDPA}} \\
     & \alert<3>{inexact $O(SN^2)$} & \alert<4>{exact $O(SN\log N)$}\\
    R pkgs: & \alert<3>{PeakSegDP} & \alert<4>{PeakSegOptimal}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item \alert<1>{Auger and Lawrence 1989, Jackson et al 2005}.
  \item \alert<2>{Rigaill 2010, Johnson 2013, Cleynen et al 2014}.
  \item \alert<3>{Hocking, Rigaill, Bourque 2015}.
  \item \alert<4>{\textbf{Contribution:} Generalized Pruned Dynamic
        Programming Algorithm (GPDPA) that \textbf{exactly} computes the
      \textbf{constrained} model for $N$ data points and up to $S$ segments in
       $O(SN\log N)$ time}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dynamic programming and functional pruning}
  \textbf{Classical dynamic programming} (Auger and Lawrence 1989)
  computes the matrix of optimal loss values in $S$ segments up to $N$
  data points, $O(S N^2)$
$$
\begin{array}{ccc}
  \mathcal L_{1,1} & \cdots &   \mathcal L_{1,N}\\
  \vdots &  & \vdots\\
  \mathcal L_{S,1} & \cdots & \mathcal L_{S,N}\\
\end{array}
$$
\textbf{Dynamic programming with functional pruning} (Rigaill 2010,
Johnson 2013) computes a matrix of loss \textbf{functions}, the
optimal loss up to $N$ data points if segment $S$ has mean $\mu_S$,
$O(S N\log N)$
$$
\begin{array}{ccc}
   L_{1,1}(\mu_1) & \cdots & L_{1,N}(\mu_1)\\
  \vdots &  & \vdots\\
   L_{S,1}(\mu_S) & \cdots & L_{S,N}(\mu_S),\\
\end{array}
$$
\textbf{Contribution of this work}: a new algorithm that applies the
functional pruning technique to the up-down constrained model.
\end{frame}

\begin{frame}
  \frametitle{First segment, first data point}
  \begin{itemize}
  \item For data $z_1, \dots, z_N\in\ZZ_+$ let
  \begin{equation*}
    \gamma_t(\mu) = \ell(\mu, z_t) = \mu - z_t \log \mu
  \end{equation*}
  be the Poisson loss for each $t\in\{1, \dots, N\}$.
\item For example $z = 2, 1, 0, 4$.
\item Then $\gamma_1(\mu)=L_{1,1}(\mu)= \alert{1}\mu - \alert{2}\log \mu + \alert{0}$.
\item Need to store 3 coefficients (\alert{linear}, \alert{log}, \alert{constant}).
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-1data}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{First segment, other data points}
  \begin{itemize}
\item
  The loss of the first segment up to data point $t$ is
  \begin{equation*}
    \label{eq:C1b}
    L_{1,t}(\mu) = \sum_{i=1}^t \gamma_i(\mu).
  \end{equation*}
\item For example $z = 2, 1, 0, 4$.
% \item $L_{1,2}(\mu) = (2\mu - 3\log\mu + 0)/2 = 1\mu - 1.5\log\mu + 0$.
% \item $L_{1,3}(\mu) = (3\mu - 13\log\mu + 0)/3 = 1\mu - 4.333\log\mu + 0$.
\item $L_{1,2}(\mu) = 2\mu - 3\log\mu + 0$.
\item $L_{1,3}(\mu) = 3\mu - 12\log\mu + 0$.
\item ...
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-2data}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-3data}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 2}
  \begin{itemize}
  \item The cost in 2 segments up to data point 2 is
\begin{eqnarray*}
  L_{2,2}(\mu_2) 
  &=&  \gamma_2(\mu_2)+\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)\\
  &=& \gamma_2(\mu_2)+L_{1,1}^{\leq}(\mu_2)
\end{eqnarray*}
\item Min-less operator $L^\leq(\mu) = \min_{x\leq\mu} L(x)$ can be
  computed and stored exactly for every possible $\mu$!
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Comparison with unconstrained Pruned DPA}
  \begin{itemize}
  \item For our constrained algorithm, the first segment mean must be
    less than the second, and the first segment cost is a function:
    \begin{equation*}
      L_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \underbrace{\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)}_{L^\leq_{1,1}(\mu_2)}.
    \end{equation*}
  \item For the unconstrained algorithm, it is \alert<1>{constant}:
    \begin{equation*}
      \widehat{L}_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \alert<1>{\underbrace{\min_{\mu_1} L_{1,1}(\mu_1)}_{\mathcal L_{1,1}}}.
    \end{equation*}
  \item For example $z = 2, 1, 0, 4$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-mincompare-2segments-2data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Storage as a piecewise function on intervals}
  \begin{itemize}
  \item For example $z = 2, 1, 0, 4$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
  \item Storage: coefficients, intervals, previous segment mean,
    $L_{2,2}(\mu) = \gamma_2(\mu) +$
    \begin{equation*}
      \begin{cases}
        %L_{1,1}(\mu) = 
        1\mu - 2\log \mu + 0 & \text{ if } \mu\in[0, 2],\, \mu'=\mu\\
        %\mathcal L_{1,1} = 
        0\mu -0\log\mu + 0.6137 & \text{ if } \mu\in[2, 4],\, \mu'=2.
      \end{cases}
    \end{equation*}
  \end{itemize}
\end{frame}

 
\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 3}
  \begin{itemize}
  \item For data point 3 we need to consider two change-points:
    \begin{equation*}
      L_{2,3}(\mu) =  \gamma_3(\mu) + \min
      \begin{cases}
        L_{1,2}^{\leq}(\mu), & \text{ change up after data point 2},\\
        L_{2,2}(\mu), & \text{ change up after data point 1}. 
      \end{cases}
    \end{equation*}
  \item For $z = 2, 1, 0, 4$ the min operation prunes a
    change after data point 1.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-3data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Second segment, up to data point t}
  \begin{itemize}
  \item The updates continue for every data point $t\in\{3, ..., N\}$
    \begin{eqnarray*}
      L_{2,t}(\mu) &=&  \gamma_t(\mu) + \min
      \begin{cases}
        L_{1,t-1}^{\leq}(\mu), & \text{change up after $t-1$,}\\
        L_{2,t-1}(\mu), & \text{change up before $t-1$.}
      \end{cases}
% \\
%       &=& \min_{\tau\in\{1,2,\dots,t-1\}} c_{2,t,\tau}(\mu)
%           = \min_{\tau\in T_{2,t}} c_{2,t,\tau}(\mu)
    \end{eqnarray*}
  \item For example for $z = 2, 1, 0, 4$, at data point $t=4$
    we only need to consider changes after 2 and 3 (1 has been
    pruned).
% $$
% L_{2,4}(\mu) &=& \min_{\tau\in\{1,2,3\}} c_{2,t,\tau} = 
% \min_{\tau\in\{2,3\}} c_{2,t,\tau} 
% $$
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-4data}
    \end{center}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Generalized pruned dynamic programming algorithm (GPDPA)}
  Dynamic programming update rule: the constrained cost of a
  mean $\mu$ for the segment $s$, up to data point $t$.
  \begin{itemize}
  \item For $s=2, 4, \dots$
    \begin{equation*}
      L_{s,t}(\mu) = \gamma_t(\mu) + \min
      \begin{cases}
        L_{s,t-1}(\mu),\\
        L_{s-1,t-1}^{\alert{\leq}}(\mu),
      \end{cases}
    \end{equation*}
  \item For $s=3, 5, \dots$
    \begin{equation*}
      L_{s,t}(\mu) = \gamma_t(\mu) + \min
      \begin{cases}
        L_{s,t-1}(\mu),\\
        L_{s-1,t-1}^{\alert{\geq}}(\mu),
      \end{cases}
    \end{equation*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{More complex $\min\{\}$ computation}
  Time/space complexity linear in number of intervals (candidate changepoints).
  \begin{minipage}[t]{\linewidth}
  \hskip -1cm
  \input{figure-2-min-envelope} 
  \end{minipage}
\end{frame}


% \section{Results on heartbeat audio time series}

% \begin{frame}
%   \frametitle{Heartbeat mp3}
%   \includegraphics[width=\textwidth]{figure-heartbeat-data-only}

%   \scriptsize
%   \url{https://github.com/tdhock/heartbeat/blob/master/heartbeat.mp3}
% \end{frame}

% \begin{frame}
%   \frametitle{Unconstrained 
% model does not satisfy up-down constraint}
%   \includegraphics[width=\textwidth]{figure-heartbeat-unconstrained}
% \end{frame}

% \begin{frame}
%   \frametitle{Up-down
% model interpretable in terms of heartbeats}
%   \includegraphics[width=\textwidth]{figure-heartbeat-PeakSeg}
% \end{frame}

% \begin{frame}
%   \frametitle{Can measure heartbeat duration}
%   \includegraphics[width=\textwidth]{figure-heartbeat}
% \end{frame}

\section{Results on ChIP-seq benchmark}

\begin{frame}
  \frametitle{New labeling method for peak detection in ChIP-seq data}

  H {\it et al.}, {\it Bioinformatics} 2017: choose peak model/parameters
  which minimize the number of incorrectly predicted labels.

  \includegraphics[width=\textwidth]{figure-PeakError.pdf}
  \begin{itemize}
  \item \textbf{peakStart}: exactly one peak start (0=FN, more=FP).
  \item \textbf{peakEnd}: exactly one peak end (0=FN, more=FP).
  \item \textbf{noPeaks}: no overlapping peaks (otherwise FP).
  \item \textbf{peaks}: at least one overlapping peak (otherwise FN).
  \item R package PeakError.
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Benchmark data sets, algorithms}

%  \url{http://cbio.ensmp.fr/~thocking/chip-seq-chunk-db/}
%   \begin{itemize}
%   \item Hocking \emph{et al} Bioinformatics (2016). Optimizing
%     ChIP-seq peak detectors using visual labels and supervised machine
%     learning.
%   \item 37 labeled H3K4me3 samples (sharp peak pattern).
%   \item 29 labeled H3K36me3 samples (broad peak pattern).
%   \item 12,826 labeled regions with and without peaks.
%   \item 2,752 separate segmentation problems.
%   \end{itemize}

%   Algorithms for $S$ optimal segmentations in $N$ data points:
%   \begin{center}
%   \begin{tabular}{ccccc}
%     algorithm & constraint & exact? & complexity \\
%     \hline
%     CDPA & $\mu_1 < \mu_2 > \mu_3 \dots$ & no & $O(S N^2)$\\
%     \textbf{GPDPA} &
%  $\mu_1 \leq \mu_2 \geq \mu_3 \dots$ & yes & $O(S N\log N)$ \\
%     PDPA & none & yes & $O(S N\log N)$
%   \end{tabular}

%   \vskip 0.5cm

%   PDPA loss $\leq$ GPDPA loss $\leq$ CDPA loss.
%   \end{center}
% \end{frame}
 
\begin{frame}
  \frametitle{Test AUC on 7 benchmark data sets}
  \includegraphics[width=\textwidth]{figure-test-error-dots}
  \begin{itemize}
  \item 4-fold cross-validation: train on 3/4 of labels, test on 1/4.
  \item All models trained by learning a scalar significance
    threshold / penalty parameter, which is varied to compute ROC/AUC.
  \item MACS is highly inaccurate in all data sets.
  \item HMCanBroad is accurate for broad but not sharp pattern.
  \item Unconstrained PDPA algorithm not as accurate as up-down
    constrained algorithms (CDPA, \textbf{proposed GPDPA}).
  \end{itemize}
  \scriptsize
\url{http://bl.ocks.org/tdhock/raw/886575874144c3b172ce6b7d7d770b9f/}
\end{frame}

\begin{frame}
  \frametitle{Functional pruning algorithms faster for larger data sets}
  %\includegraphics[width=1\textwidth]{figure-PDPA-timings.pdf}
  \input{figure-PDPA-timings-wide-labels}

  Total time to compute 10 models (0, ..., 9 peaks) for all problems:
  \begin{itemize}
  \item CDPA: 156 hours, inexact.
  \item GPDPA: 6 hours, exact.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Contribution: new fast and exact constrained algorithm}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Dynamic Programming & Pruned DP \\
     & exact $O(S N^2)$ & exact $O(SN\log N)$\\
    R pkgs: & changepoint & cghseg, Segmentor\\
    \hline
    up-down constrained & constrained DP & \textbf{GPDPA} \\
     & inexact $O(SN^2)$ & exact $O(SN\log N)$\\
    R pkgs: & PeakSegDP & \textbf{PeakSegOptimal}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item New GPDPA that computes optimal changepoints 
    using the Poisson loss and the up-down constraint.
  \item State-of-the-art peak detection accuracy in ChIP-seq data.
  \item Empirically $O(S N \log N)$ time and memory for $N$ data
    points and $S$ segments.
  \item C++ code in PeakSegOptimal R package, 
    \url{https://cran.r-project.org/package=PeakSegOptimal}
  \end{itemize}
  Future work: other constraints and loss functions.
\end{frame}

\section{Other projects and future work}

\begin{frame}
  \frametitle{New labeling and training methods for breakpoint detection}

  H {\it et al.}, {\it BMC Bioinformatics} 2013.
  \begin{itemize}
  \item Create positive/breakpoint and negative/normal labels.
  \item Quantify/optimize error rate (number of incorrect labels).
  \item Benchmark data set of 3418 chromosomes,
    labeled via visual inspection (R package neuroblastoma).
  \item Results: most accurate model is maximum Gaussian likelihood 
    changepoint detection (cross-validation experiments).
  \end{itemize}

  \includegraphics[width=\textwidth]{neuroblastoma-ok-relapse-supervised}

\end{frame} 

% \begin{frame}
%   \frametitle{Fast FPOP algorithm for computing the maximum likelihood
%     changepoint model}

%   Maidstone, Hocking, Rigaill, Fearnhead, {\it Stat. and
%     Comp.} 2017.

%   \begin{itemize}
%   \item Naively exponential $O(p^K)$ time to compute best model with
%     $K$ segments ($K-1$ changes) for a sequence of $p$ data.
%   \item Previous work: other algorithms for computing the model
%     (\algo{PELT} and \algo{pDPA}).
%   \item Contribution: new log-linear $O(p\log p)$ \algo{FPOP}
%     algorithm for computing the same model (R package fpop).
%   \end{itemize}
%   \begin{center}
%     \includegraphics[width=0.5\textwidth]{figure-systemtime-arrays-bins}
%   \end{center}
% \end{frame}

\begin{frame}
  \frametitle{Supervised interactive DNA copy number analysis}
  H {\it et al.}, {\it Bioinformatics} 2014.

  \begin{itemize}
  \item Previous work: non-interactive command line programs
    -- collaborators can not correct obvious
    errors.
  \item Interactive system: when you edit the labels, the system
    learns and updates the model. (SegAnnDB python module)
  \item Result below: only a few labels required to learn a highly accurate
    breakpoint detection model.
  \end{itemize}
  \begin{minipage}{0.5\linewidth}
    \includegraphics[width=\textwidth]{SegAnnDB-test-error-decreases}
  \end{minipage}
  \begin{minipage}[0.5\linewidth]{0.48\linewidth}
SegAnnDB demo:\\interactively label chr1/chrX
  \url{http://bioviz.rocq.inria.fr/profile/GSM313887/}
  \end{minipage}
\end{frame}

\begin{frame}
  \frametitle{Cancer biology applications}
  Aichi cancer center, Nagoya, Japan.
  \begin{itemize}
  \item Suguro {\it et al.}, {\it Cancer Sci} 2014, Clonal
    heterogeneity of lymphoid malignancies correlates with poor
    prognosis.
  \item Shimada {\it et al.}, {\it Leukemia} 2016, Development and
    analysis of patient-derived xenograft mouse models in
    intravascular large B-cell lymphoma.
  \end{itemize}
  Institut Curie, Paris, France.
  \begin{itemize}
  \item Chicard {\it et al.}, {\it Clinical Cancer Research} 2016,
    Genomic copy number profiling using circulating free tumor DNA
    highlights heterogeneity in neuroblastoma.
  \item Ongoing work, characterizing alterations in neuroblastoma.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Other research projects}
  \begin{tabular}{ll}
Unsupervised convex & \multirow{4}{*}{
\includegraphics[height=0.2\textheight]{screenshot-clusterpath}
}\\
hierarchical clustering.\\
H {\it et al.}, {\it ICML} 2011\\
%91 citations. 
R package clusterpath. \\
\hline
Predicting the  & \multirow{4}{*}{
\includegraphics[height=0.25\textheight]{Screenshot-max-margin}
}\\
number of changepoints.\\
H {\it et al.}, {\it ICML} 2013. \\
R package penaltyLearning.\\
useR2017 tutorial.\\
\hline
Support vector machines & \multirow{4}{*}{ 
    \includegraphics[height=0.2\textheight]{screenshot-ranksvmcompare}
}\\
for ranking and comparison. \\
H {\it et al.}, arXiv:1401.8008. \\
R package rankSVMcompare.\\
\hline
New interactive keywords& \multirow{5}{*}{
\includegraphics[height=0.2\textheight]{figure-design}
}\\
for the grammar of graphics,\\
under review at \emph{Journal}\\
\emph{of Computational and Graphical}\\
\emph{Statistics}, R package animint.\\
%useR2016 tutorial.\\
  \end{tabular}
  % \begin{minipage}{0.3\linewidth}
  %   Clusterpath for convex hierarchical clustering (H {\it et
  %     al.}, {\it ICML} 2011, 91 citations). R package clusterpath.
  %   \includegraphics[width=\linewidth]{screenshot-clusterpath}
  % \end{minipage}
  % \begin{minipage}{0.3\linewidth}
  %   Support vector machines for ranking and comparison. H {\it et
  %     al.}, arXiv:1401.8008. R package rankSVMcompare.
  %   \includegraphics[width=\linewidth]{screenshot-ranksvmcompare}
  % \end{minipage}
  % \begin{minipage}{0.3\linewidth}
  %   New interactive keywords for the grammar of graphics, under review
  %   at \emph{Journal of Computational and Graphical Statistics}, R
  %   package animint.
  % \end{minipage}

  % Collaborations at the McGill Genome Center:
  % \begin{itemize}
  % \item Predicting which people will respond to the flu vaccine based
  %   on SNP data. (with Maiko Narahara)
  % \item Predicting which people have asthma based on SNP
  %   data. (with Audrey Grant)
  % \item Predicting which genetic variants are pathogenic based on
  %   biochemistry, evolutionary conservation, etc. (with
  %   Najmeh Alirezaie)
  % % \item Predicting genomic regions with peaks based on ChIP-seq
  % %   data. (with Guillaume Bourque)
  % \end{itemize}
\end{frame}
 
\begin{frame}
  \frametitle{Future projects for my research lab}
  \begin{itemize}
  %\item Collaborations with experts from other scientific domains.
  % \item Supervising graduate students who want to work on algorithms
  %   for solving important big data analysis problems.
  \item Multi-task learning for predicting the number of changepoints
    (multiple scientists provide breakpoint or peak labels).
  % \item Survival models for predicting the number of changepoints: R
  %   package iregnet.
  \item Fast algorithms for other constrained changepoint models
    (neuro spike train data, segmented regression, etc).
  \item Deep learning for nonparametric changepoint detection.
  \item GenomicLearner platform for collaborations: interactive
    labeling and machine learning analysis.
  \end{itemize}
\end{frame}


% \begin{frame}[fragile]
%   \frametitle{How to choose parameters of unsupervised peak
%     detectors?}
% \scriptsize
% 19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
% \begin{verbatim}
%   [-g GSIZE]
%   [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
%   [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
%   [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
%   [--down-sample] [--seed SEED] [--nolambda]
%   [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
%   [--shift-control] [--half-ext] [--broad]
%   [--broad-cutoff BROADCUTOFF] [--call-summits]
% \end{verbatim}
% 10 parameters for Histone modifications in cancer (HMCan),
% Ashoor et al, 2013.
% \begin{verbatim}
% minLength 145
% medLength 150
% maxLength 155
% smallBinLength 50
% largeBinLength 100000
% pvalueThreshold 0.01
% mergeDistance 200
% iterationThreshold 5
% finalThreshold 0
% maxIter 20
% \end{verbatim}
% \end{frame}

% \begin{frame}
%   \frametitle{PeakSeg: search for the peaks with lowest loss}
%   \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}
  
%   Simple model with only one parameter (number of peaks).\\
%   But computationally difficult!
% \end{frame}

\begin{frame}
  \frametitle{Thanks for your attention!}

  Questions? toby.hocking@mail.mcgill.ca
  \begin{itemize}
  \item A log-linear time algorithm for constrained changepoint
    detection, by Toby Dylan Hocking, Guillem Rigaill, Paul Fearnhead,
    Guillaume Bourque \url{https://arxiv.org/abs/1703.03352}
  \item 
  PeakSegOptimal R package, \\
\url{https://cran.r-project.org/package=PeakSegOptimal}
  \item source code for these slides:
  \url{https://github.com/tdhock/PeakSegFPOP-paper}
  \end{itemize}
\end{frame}

\section*{Results on comparing several samples}

\begin{frame}
  \frametitle{Target interval computation time}
  \includegraphics[width=\textwidth]{figure-target-interval-time}
\end{frame}

\begin{frame}
  \frametitle{More peaks in sharp H3K4me3 than in broad H3K36me3}
  \includegraphics[width=0.7\textwidth]{figure-min-err-peaks-compare}
\end{frame}

\begin{frame}
  \frametitle{More peaks in sharp H3K4me3 than in broad H3K36me3}
  \includegraphics[width=0.7\textwidth]{figure-peak-size-model}
\end{frame}

\begin{frame}
  \frametitle{Max-margin penalty learning algortihm}
  \includegraphics[width=\textwidth]{figure-large-margin}
 
\scriptsize \url{http://bl.ocks.org/tdhock/raw/9311ca39d643d127e04a088814c81ee1/}

\end{frame}

% \begin{frame}
%   \frametitle{Segmenting whole chromosomes?}
%   \includegraphics[width=\textwidth]{screenshot-gap-peaks}
%   \begin{itemize}
%   \item 365 regions with no gaps in hg19.
%   \item 272 regions with no gaps on chr1-22, X, Y.
%   \item Smallest: 31,833 bases (chr6:157,609,467-157,641,300).
%   \item Largest: 115,591,997 bases (chr4:75,452,279-191,044,276).
%   \end{itemize}
% \end{frame}


% \begin{frame}
%   \frametitle{Reasonable time to segment biggest region on chr1}
%   \includegraphics[width=\textwidth]{figure-cosegData-timings.pdf}
%   \begin{itemize}
%   \item R package in memory: $O(N \log N)$ time.
%   \item Command line program on disk: $O(N \log N)$ time.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Memory requirements reasonable for on-disk version}
%   \includegraphics[width=\textwidth]{figure-cosegData-timings-memory-disk.pdf}
%   \begin{itemize}
%   \item R package in memory: $O(N \log N)$ memory.
%   \item Command line program on disk: $O(\log N)$ memory.
%   \end{itemize}
% \end{frame}

% \begin{frame}
%   \frametitle{Disk usage reasonable}
%   \includegraphics[width=\textwidth]{figure-cosegData-timings-disk.pdf}
%   \begin{itemize}
%   \item R package in memory: no disk usage.
%   \item Command line program: $O(N \log N)$ disk space (temporary).
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{Two annotators provide consistent labels, but different
    precision}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/screenshot-several-annotators}

  \begin{itemize}
  \item TDH peakStart/peakEnd more precise than AM peaks.
  \item AM noPeaks more precise than TDH no label.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-TDH-experiments.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-TDH-experiments.pdf}
\end{frame}


\begin{frame}
  \frametitle{PeakSeg with 2 peaks more likely than default macs}
  \includegraphics[width=1\textwidth]{figure-macs-problem-1-30103.png}
\end{frame}

\begin{frame}
  \frametitle{Unconstrained model can have any changes}
  \includegraphics[width=1\textwidth]{Seg_SansC.png}
  \vskip -1cm
  \begin{itemize}
  \item The model above does NOT verify the PeakSeg up-down constraint
    (second change is up, should be down).
  \item Not directly interpretable as background and peaks.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg constraint forces up then down changes}
  \includegraphics[width=1\textwidth]{Seg_AvecC1.png}
  \vskip -1cm
  \begin{itemize}
  \item Novelty of PeakSeg model with respect to previous optimal
    segmentation models: the up-down constraint.
  \item Interpretable as background (S1, S3, ...)\\
    and peaks (S2, S4, ...).
  \end{itemize}
\end{frame}

\section*{Interpretation of equal segment means}

\begin{frame}
  \frametitle{What does our solution say about the PeakSeg solution?}
  \begin{itemize}
  \item Our algo exactly solves a problem with \textbf{non-strict
      inequality} constraints.
  \item For example, $N=3$ data points and $S=3$ segments,
    \begin{equation*}
      \min_{m_1\leq m_2\geq m_3}
      \sum_{t=1}^N m_t - z_t\log m_t.
    \end{equation*}
  \item But the PeakSeg problem has \textbf{strict inequality}
    constraints:
    \begin{equation*}
      \min_{m_1<m_2 >m_3}
      \sum_{t=1}^N m_t - z_t\log m_t.
    \end{equation*}
  \end{itemize}
  When our algo returns equal values for adjacent segment means,
  \begin{itemize}
  \item Our solution is not feasible for the PeakSeg problem, and
  \item The PeakSeg solution is undefined.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Example: 13, 14, 10, 1}
  What do you think is the best model with 1 peak?\\
  (3 segments, 1 change up, 1 change down)
  
  \vskip 1cm

  Guessing game:

  \url{https://github.com/tdhock/PeakSegFPOP-paper/blob/master/figure-min-undefined.R} 
\end{frame}

\begin{frame}
  \frametitle{PeakSegDP returns this highly suboptimal model}
  \includegraphics[width=\textwidth]{figure-min-undefined-suboptimal}
\end{frame}

\begin{frame}
  \frametitle{A better model}
  \includegraphics[width=\textwidth]{figure-min-undefined-1}
\end{frame}

\begin{frame}
  \frametitle{Even better}
  \includegraphics[width=\textwidth]{figure-min-undefined-2}
\end{frame}

\begin{frame}
  \frametitle{Still better}
  \includegraphics[width=\textwidth]{figure-min-undefined-3}
\end{frame}

\begin{frame}
  \frametitle{We can keep going forever}
  \includegraphics[width=\textwidth]{figure-min-undefined-4}
\end{frame}

\begin{frame}
  \frametitle{Best model is not feasible}
  \includegraphics[width=\textwidth]{figure-min-undefined}
\end{frame}
 
\section*{Segmentation problem definitions}

\begin{frame}
  \frametitle{Unconstrained maximum likelihood segmentation}
  \begin{itemize}
  \item We have a sequence of $n$ count data $\mathbf y\in\ZZ_+^n$ to
    segment. 
  \item Choose the number of segments $S\in\{1, 2, \dots, n\}$.
  \end{itemize}
\begin{align*}
  \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) 
\\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = S, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}
\nonumber
\end{align*}
\begin{itemize}
\item The Poisson loss function is  $\ell( m,  y)= m - y \log m$.
\item Every $t$ such that $c_t \neq 0$ is a
change-point.
\end{itemize}

\end{frame}

% We refer to (\ref{unconstrained}) as the ``unconstrained'' model
% since $\mathbf{\hat m}^K(\mathbf y)$ is the most likely segmentation
% of all possible models with $K$ piecewise constant segments ($K-1$
% change-points). 
% Although (\ref{unconstrained}) is a non-convex optimization problem,
% the sequence of segmentations
% $\mathbf{\hat m}^1(\mathbf y), \dots, \mathbf{\hat m}^{K}(\mathbf y)$
% can be computed in $O(K n^2)$ time using the standard dynamic
% programming algorithm \citep{bellman}, or in $O(K n \log n)$ time
% using dynamic programming with functional pruning \citep{pruned-dp,
%   johnson, Segmentor}.

\begin{frame}
  \frametitle{The PeakSeg constrained maximum likelihood model}
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) \\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = S, \\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}\\
&\ \ \forall t\in\{1, \dots, n-1\},\, \alert{P_t(\mathbf c) \in\{0, 1\}}.
\end{align*}
The only difference with the unconstrained problem is that\\
\alert{we
have added the constraint $P_t(\mathbf c)=\sum_{i=1}^t c_i \in\{0, 1\}$}.
% Another way to interpret the constrained
% \ref{PeakSeg} problem is that the sequence of changes in the segment
% means $\mathbf m$ must begin with a positive change and then
% alternate: up, down, up, down, ... (and not up, up, down). Thus the
% even-numbered segments may be interpreted as peaks $P_t(\mathbf c)=1$,
% and the odd-numbered segments may be interpreted as background
% $P_t(\mathbf c)=0$.
\end{frame}

\begin{frame}
  \frametitle{The problem solved by the PeakSegPDPA}
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) 
  \label{PeakSegPDPA}
\\
    \text{subject to} &\ \  1+ \sum_{t=1}^{n-1} I(c_t \neq 0) = S, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} \alert{\geq} m_{t+1} \text{ (change down or no change)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} \alert{\leq} m_{t+1} \text{ (change up or no change)}
\nonumber\\
&\ \ \forall t\in\{1, \dots, n-1\},\, P_t(\mathbf c) \in\{0, 1\}.
\nonumber
\end{align*}
\begin{itemize}
\item The only difference with the \textbf{PeakSeg} problem is that\\
  \alert{we have changed the strict inequality constraints to non-strict inequality
constraints}. 
\item This model has \textbf{at most} $S$ distinct
  segment means (some may be equal due to the non-strict equality
  constraints).
\end{itemize}
\end{frame}


% \section*{Review of constrained dynamic programming algorithm}

% \input{figure-dp-first}

% \input{figure-dp-short}

% \input{figure-dp}

% \begin{frame}
%   \frametitle{Dynamic programming is faster than grid search for $s>
%     2$ segments}

%   Computation time in number of data points $N$:

%   \vskip 1cm

%   \begin{tabular}{ccc}
%     segments $s$ & grid search & dynamic programming \\
%     \hline
%     1 & $O(N)$ & $O(N)$ \\
%     2 & $O(N^2)$ & $O(N^2)$ \\
%     3 & $O(N^3)$ & $O(N^2)$ \\
%     4 & $O(N^4)$ & $O(N^2)$ \\
%     $\vdots$ &     $\vdots$ &     $\vdots$ 
%   \end{tabular}

%   \vskip 1cm

%   For example $N = 5735$ data points to segment.\\
%   $N^2 = 32890225$\\
%   $N^3 = 188625440375$\\
%   $\vdots$
% \end{frame}

% \input{figure-dp-third}

% \begin{frame}
%   \frametitle{5 errors for coseg/Segmentor, only 1 error for PeakSegDP}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1}
% \end{frame}

% \begin{frame}
%   \frametitle{5 errors for coseg/Segmentor, only 1 error for PeakSegDP}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-best.png}
% \end{frame}

% \begin{frame}
%   \frametitle{Constrained optimization worse than macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem4}
% \end{frame}

% \begin{frame}
%   \frametitle{Constrained optimization worse than macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem4-best}
% \end{frame}

% \begin{frame}
%   \frametitle{Constrained optimization worse than macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem4-best-zoom}
% \end{frame}


% \begin{frame}
%   \frametitle{Functional pruning complete example}
% %\includegraphics[width=\textwidth]{screenshot-PDPA-demo}
%     \input{figure-2-min-envelope}

%   \url{https://github.com/tdhock/PeakSegFPOP-paper}
%   %\url{http://bl.ocks.org/tdhock/raw/8c5dd0af533e24a893e7c5232f9bc94c/}
% \end{frame}

% \begin{frame}
%   \frametitle{New coseg algorithm more accurate than unconstrained
%     maximum likelihood Poisson model (Segmentor)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-Segmentor}
% \end{frame}

% \begin{frame}
%   \frametitle{New coseg algorithm mostly agrees with slower inexact DP}
%   \includegraphics[width=\textwidth]{figure-min-train-error-PeakSegDP}
% \end{frame}

% \begin{frame}
%   \frametitle{New coseg algorithm sometimes disagrees with macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-macs}
% \end{frame}

% \begin{frame}
%   \frametitle{Constrained optimization better than macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5}
% \end{frame}

% \begin{frame}
%   \frametitle{0 errors for coseg/PeakSegDP, 6 errors for Segmentor}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2}
% \end{frame}

% \begin{frame}
%   \frametitle{8 false negative labels for models with 0 peaks}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-0peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Models with 1 peak are better (6 FN)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-1peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Models with 2 peaks are better still (4 FN)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-2peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Models with 3 peaks are the same (4 FN)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-3peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Models with 4 peaks are better (2 FN)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-4peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Models with 5 peaks have no incorrect labels}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-5peaks}
% \end{frame}


% \begin{frame}
%   \frametitle{Models with 6 peaks are worse (1 false positive)}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-6peaks}
% \end{frame}

% \begin{frame}
%   \frametitle{Constrained optimization better than macs}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem5-best}
% \end{frame}


\begin{frame}
  \frametitle{Implementation details}
  Implemented using C++ Standard Template Library (linked list),
  \url{https://cran.r-project.org/package=PeakSegOptimal}
\begin{description}
\item[Weights] for data sequences that contain repeats:
  5,1,1,1,0,0,5,5 is encoded as $n=4$ counts $y_t$ 5,1,0,5 with
  corresponding weights $w_t$ 1,3,2,2.
\item[Mean cost rather than total cost] more complicated update rule,
  but more numerically stable.
\item[Intervals in log(mean) space] For example rather than storing
  $\mu\in[0,1]$ we store $x=\log\mu\in[-\infty, 0]$ -- Poisson model,
  negative mean impossible.
\item[Newton root finding] For the larger root we solve
  $a\log\mu + b\mu + c = 0$ (linear as $\mu\rightarrow\infty$) and for
  the smaller root we solve $a x + be^x + c = 0$ ($x=\log \mu$, linear
  as $x\rightarrow -\infty$ and $\mu\rightarrow 0$). 
\end{description}
  
\end{frame}

\end{document}
