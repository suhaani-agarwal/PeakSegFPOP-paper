% -*- compile-command: "make HOCKING-PeakSeg-functional-pruning-slides.pdf" -*-
\documentclass{beamer}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{A linear time algorithm for constrained 
optimal segmentation of ChIP-seq data}

\author{
  Toby Dylan Hocking\\
  toby.hocking@mail.mcgill.ca\\
  joint work with Guillem Rigaill, Paul Fearnhead, 
  Guillaume Bourque}

\date{29 Sep 2016}

\maketitle

\section{Problem: optimizing ChIP-seq peak detection}

\begin{frame}
  \frametitle{Chromatin immunoprecipitation sequencing (ChIP-seq)}
  Analysis of DNA-protein interactions.

  \includegraphics[width=\textwidth]{Chromatin_immunoprecipitation_sequencing_wide.png}

  Source: ``ChIP-sequencing,'' Wikipedia.
\end{frame}

\begin{frame}
  \frametitle{Problem: find peaks in each of several samples}
  \includegraphics[width=\textwidth]{screenshot-ucsc-edited}

  Grey profiles are normalized aligned read count signals.

  Black bars are ``peaks'' called by MACS2 (Zhang et al, 2008):
  \begin{itemize}
  \item \alert<1>{many false positives}.
  \item overlapping peaks have different start/end positions.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Previous work in genomic peak detection}
  \begin{itemize}
  \item Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
  \item SICER, Zang et al, 2009.
  \item HOMER, Heinz et al, 2010.
  \item CCAT, Xu et al, 2010.
  \item RSEG, Song et al, 2011.
  \item Triform, Kornacker et al, 2012.
  \item Histone modifications in cancer (HMCan), Ashoor et al, 2013.
  \item PeakSeg, Hocking, Rigaill, Bourque, ICML 2015.
  \item PeakSegJoint Hocking and Bourque, arXiv:1506.01286.
  \item ... dozens of others.
  \end{itemize}
  Two big questions: how to choose the best...
  \begin{itemize}
  \item ...algorithm? (testing)
  \item \alert<1>{...parameters? (training)}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{How to choose parameters of unsupervised peak
    detectors?}
\scriptsize
19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
\begin{verbatim}
  [-g GSIZE]
  [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
  [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
  [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
  [--down-sample] [--seed SEED] [--nolambda]
  [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
  [--shift-control] [--half-ext] [--broad]
  [--broad-cutoff BROADCUTOFF] [--call-summits]
\end{verbatim}
10 parameters for Histone modifications in cancer (HMCan),
Ashoor et al, 2013.
\begin{verbatim}
minLength 145
medLength 150
maxLength 155
smallBinLength 50
largeBinLength 100000
pvalueThreshold 0.01
mergeDistance 200
iterationThreshold 5
finalThreshold 0
maxIter 20
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Which macs parameter is best for these data?}
  \includegraphics[width=1\textwidth]{figure-macs-problem.png}
\end{frame}

\begin{frame}
  \frametitle{Compute likelihood/loss of piecewise constant model}
  \includegraphics[width=1\textwidth]{figure-macs-problem-7-5.png}
  % $\PoissonLoss(\mathbf z, \mathbf m) = \sum_{i=1}^n m_i - z_i \log(m_i)$
  % for count data $\mathbf z\in\ZZ_+^n$ 
  % and segment mean model $\mathbf m\in\RR^n$.
\end{frame}

\begin{frame}
  \frametitle{Idea: choose the parameter with a lower loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-15.png}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}

  Choose the number of peaks via standard penalties (AIC, BIC,
    ...)\\or learned penalties based on visual labels (more on this later).
\end{frame}

\begin{frame}
  \frametitle{Maximum likelihood Poisson segmentation models}
  \includegraphics[width=1\textwidth]{figure-Segmentor-PeakSeg}

  \begin{itemize}
  \item Previous work: unconstrained maximum likelihood mean\\
    for $s$ segments ($s-1$ changes).
  \item Hocking et al, ICML 2015: PeakSeg constraint enforces up, down, up,
    down changes (and not up, up, down). 
  \item Odd-numbered segments are background noise,\\
    even-numbered segments are peaks.
  \item Constrained Dynamic Programming Algorithm, $O(N^2)$ time for $N$ data points.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{But quadratic time is not fast enough for genomic data!}
  \includegraphics[width=\textwidth]{figure-PDPA-timings-dp}
  \begin{itemize}
  \item Genomic data is large, $N \geq 10^6$.
  \item Split into subsets? What if we split a peak in half?
  \item Need linear time algorithm for analyzing whole data set.
  \end{itemize}
\end{frame}

\section{New linear time algorithm using functional pruning}

\begin{frame}
  \frametitle{Relation to previous work}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & \alert<1>{Dynamic Programming} & \alert<2>{Pruned DP} \\
     & \alert<1>{exact $O(N^2)$} & \alert<2>{exact $O(N\log N)$}\\
    R pkgs: & \alert<1>{changepoint} & \alert<2>{cghseg, Segmentor}\\
    \hline
    up-down constrained & \alert<3>{constrained DP} & \alert<4>{\textbf{this work}} \\
     & \alert<3>{inexact $O(N^2)$} & \alert<4>{exact $O(N\log N)$}\\
    R pkgs: & \alert<3>{PeakSegDP} & \alert<4>{coseg}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item \alert<1>{Auger and Lawrence 1989, Jackson et al 2005}.
  \item \alert<2>{Rigaill 2010, Johnson 2013, Cleynen et al 2014}.
  \item \alert<3>{Hocking, Rigaill, Bourque 2015}.
  \item \alert<4>{\textbf{Contribution}: new algorithm that
      \textbf{exactly} computes the \textbf{constrained} optimal
      segmentation for $N$ data points in linear $O(N\log N)$ time}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Statistical model is Poisson with change constraints}
  \begin{itemize}
  \item We have $N$ count data $z_1, \dots, z_N\in\ZZ_+$.
  \item Fix the number of segments $S\in\{1, 2, \dots, N\}$.
  \item PeakSeg Model: $z_t \sim \text{Poisson}(m_t)$ such that $m_t$
    has $S-1$ up-down changes.
  \item Want to find means $m_t$ which maximize the Poisson likelihood:
    $P(Z = z_t|m_t) = m_t^{z_t} e^{-m_t} / (z_t!)$.
  \item Equivalent to finding means $m_t$ which minimize the Poisson
    loss: $\ell(m_t, z_t) = m_t - z_t\log m_t$.
  \item Naive computation is $O(N^S)$, since there are $O(N^{S-1})$ possible
    positions for $S-1$ change-points, and it takes $O(N)$ operations to
    compute the mean and loss for each.
  \item Comparison to Hidden Markov Model:
    \begin{description}
    \item[Likelihood] Same emission terms, no transition terms.
    \item[Constraint] Number of changes rather than values.
    \end{description}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynamic programming and functional pruning}
  \textbf{Classical dynamic programming} (Auger and Lawrence 1989) computes the matrix of optimal loss
  values in $S$ segments up to $N$ data points, $O(S N^2)$
$$
\begin{array}{ccc}
  \mathcal L_{1,1} & \cdots &   \mathcal L_{1,N}\\
  \vdots &  & \vdots\\
  \mathcal L_{S,1} & \cdots & \mathcal L_{S,N}\\
\end{array}
$$
\textbf{Dynamic programming with functional pruning} (Rigaill
2010) computes a matrix of loss \textbf{functions}, the
optimal loss up to $N$ data points if segment $S$ has mean $\mu_S$,
$O(S N\log N)$
$$
\begin{array}{ccc}
   L_{1,1}(\mu_1) & \cdots & L_{1,N}(\mu_1)\\
  \vdots &  & \vdots\\
   L_{S,1}(\mu_S) & \cdots & L_{S,N}(\mu_S),\\
\end{array}
$$
\textbf{Contribution of this work}: a new algorithm that applies the
functional pruning technique to the up-down constrained model.
\end{frame}

\begin{frame}
  \frametitle{First segment, first data point}
  \begin{itemize}
  \item For data $z_1, \dots, z_N\in\ZZ_+$ let
  \begin{equation*}
    \gamma_t(\mu) = \ell(\mu, z_t) = \mu - z_t \log \mu
  \end{equation*}
  be the Poisson loss for each $t\in\{1, \dots, N\}$.
\item For example $z = 2, 1, 9, 5, 10, 3$.
\item Then $\gamma_1(\mu)=L_{1,1}(\mu)= \alert{1}\mu - \alert{2}\log \mu + \alert{0}$.
\item Need to store 3 coefficients (\alert{linear}, \alert{log}, \alert{constant}).
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-1data}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{First segment, other data points}
  \begin{itemize}
\item
  The loss of the first segment up to data point $t$ is
  \begin{equation*}
    \label{eq:C1b}
    L_{1,t}(\mu) = \sum_{i=1}^t \gamma_i(\mu).
  \end{equation*}
\item For example $z = 2, 1, 9, 5, 10, 3$.
% \item $L_{1,2}(\mu) = (2\mu - 3\log\mu + 0)/2 = 1\mu - 1.5\log\mu + 0$.
% \item $L_{1,3}(\mu) = (3\mu - 13\log\mu + 0)/3 = 1\mu - 4.333\log\mu + 0$.
\item $L_{1,2}(\mu) = 2\mu - 3\log\mu + 0$.
\item $L_{1,3}(\mu) = 3\mu - 12\log\mu + 0$.
\item ...
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-2data}
    \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-cost-1segments-3data}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 2}
  \begin{itemize}
  \item The mean cost in 2 segments up to data point 2 is
\begin{eqnarray*}
  L_{2,2}(\mu_2) 
  &=&  \gamma_2(\mu_2)+\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)\\
  &=& \gamma_2(\mu_2)+L_{1,1}^{\leq}(\mu_2)
\end{eqnarray*}
\item Min-less operator is $L^\leq(\mu) = \min_{x\leq\mu} L(x),$
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Comparison with unconstrained Pruned DPA}
  \begin{itemize}
  \item For our constrained algorithm, the first segment mean must be
    less than the second, and the first segment cost is a function:
    \begin{equation*}
      L_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \underbrace{\min_{\mu_1 \leq \mu_2} L_{1,1}(\mu_1)}_{L^\leq_{1,1}(\mu_2)}.
    \end{equation*}
  \item For the unconstrained algorithm, it is \alert<1>{constant}:
    \begin{equation*}
      \widehat{L}_{2,2}(\mu_2) = \gamma_2(\mu_2)+
      \alert<1>{\underbrace{\min_{\mu_1} L_{1,1}(\mu_1)}_{\mathcal L_{1,1}}}.
    \end{equation*}
  \item For example $z = 2, 1, 9, 5, 10, 3$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-mincompare-2segments-2data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Storage as a piecewise function on intervals}
  \begin{itemize}
  \item For example $z = 2, 1, 9, 5, 10, 3$.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minlessmore-2segments-2data}
    \end{center}
  \item Storage: 
    \begin{equation*}
      L_{2,2}(\mu) = \gamma_2(\mu) + 
      \begin{cases}
        L_{1,1}(\mu) = 1\mu - 2\log \mu + 0 & \text{ if } \mu\in[1,2],\\
        \mathcal L_{1,1} = 0\mu -0\log\mu + 0.6137 & \text{ if } \mu\in[2,10].
      \end{cases}
    \end{equation*}
  \end{itemize}
\end{frame}

 
\begin{frame}[fragile]
  \frametitle{Second segment, up to data point 3}
  \begin{itemize}
  \item For data point 3 we need to consider two change-points:
    \begin{equation*}
      L_{2,3}(\mu) =  \gamma_3(\mu) + \min
      \begin{cases}
        L_{1,2}^{\leq}(\mu), & \text{ change up after data point 2},\\
        L_{2,2}(\mu), & \text{ change up after data point 1}. 
      \end{cases}
    \end{equation*}
  \item For $z = 2, 1, 9, 5, 10, 3$ the min operation prunes a
    change after data point 1.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-3data}
    \end{center}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Second segment, up to data point t}
  \begin{itemize}
  \item The updates continue for every data point $t\in\{3, ..., N\}$
    \begin{equation*}
      L_{2,t}(\mu) =  \gamma_t(\mu) + \min
      \begin{cases}
        L_{1,t-1}^{\leq}(\mu), & \text{change up after $t-1$,}\\
        L_{2,t-1}(\mu), & \text{change up before $t-1$.}
      \end{cases}
    \end{equation*}
  \item For example for $z = 2, 1, 9, 5, 10, 3$, at data point $t=4$
    we only need to consider changes after 2 and 3 (1 has been
    pruned).
    \begin{center}
      \includegraphics[width=0.5\textwidth]{figure-PeakSegPDPA-demo-minenv-2segments-4data}
    \end{center}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{General functional pruning equation}
  \begin{itemize}
  \item The constrained cost of a mean $\mu$ for the segment $s$,
    up to data point $t$:
    \begin{equation*}
      L_{s,t}(\mu) = \gamma_t(\mu) + \min
      \begin{cases}
        L_{s,t-1}(\mu),\\
        L_{s-1,t-1}^{*}(\mu),
      \end{cases}
    \end{equation*}
  \item Time complexity of min and min-less/more * is linear in the
    number of intervals, empirically sub-linear $O(\log N)$.
    \includegraphics[width=0.5\textwidth]{figure-PDPA-intervals-all}
  \item Total time complexity: $O(S N\log N)$.
  \end{itemize}
\end{frame}

\section{Results on benchmark data sets}

\begin{frame}
  \frametitle{Benchmark data sets, algorithms}

 \url{http://cbio.ensmp.fr/~thocking/chip-seq-chunk-db/}
  \begin{itemize}
  \item 4 postdocs and PhD students (AM, TDH, PGP, XJ).
  \item 8 cell types.
  \item 37 labeled H3K4me3 samples (sharp peak pattern).
  \item 29 labeled H3K36me3 samples (broad peak pattern).
  \item 12,826 labeled regions with and without peaks.
  \item 2,752 separate segmentation problems.
  \end{itemize}

  Algorithms for segmenting $N$ data points:
  \begin{center}
  \begin{tabular}{ccccc}
    package & constraint & exact? & complexity \\
    \hline
    coseg & $\mu_1 \leq \mu_2 \geq \mu_3 \dots$ & yes & $O(N\log N)$ \\
    PeakSegDP & $\mu_1 < \mu_2 > \mu_3 \dots$ & no & $O(N^2)$\\
    Segmentor & none & yes & $O(N\log N)$
  \end{tabular}

  \vskip 0.5cm

  Segmentor loss $\leq$ coseg loss $\leq$ PeakSegDP loss.
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Linear time algorithms faster for larger data sets}
  \includegraphics[width=1\textwidth]{figure-PDPA-timings.pdf}

  Total time to compute 10 models (0, ..., 9 peaks) for all data sets:
  \begin{itemize}
  \item PeakSegDP: 156 hours, inexact.
  \item coseg: 6 hours, exact.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{8 false negative labels for models with 0 peaks}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-0peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 1 peak are better (6 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-1peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 2 peaks are better still (4 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-2peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 3 peaks are the same (4 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-3peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 4 peaks are better (2 FN)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-4peaks}
\end{frame}

\begin{frame}
  \frametitle{Models with 5 peaks have no incorrect labels}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-5peaks}
\end{frame}


\begin{frame}
  \frametitle{Models with 6 peaks are worse (1 false positive)}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-6peaks}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization better than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5-best}
\end{frame}

\begin{frame}
  \frametitle{0 errors for coseg/PeakSegDP, 6 errors for Segmentor}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem2-best.png}
\end{frame}


% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-0peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-1peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-2peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-3peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem2-4peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-0peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-1peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-2peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-3peaks.png}
% \end{frame}

% \begin{frame}
%   \includegraphics[width=\textwidth]{figure-min-train-error-problem1-4peaks.png}
% \end{frame}


\begin{frame}
  \frametitle{Test AUC on 7 benchmark data sets}
  \includegraphics[width=\textwidth]{figure-test-error-dots}
  \begin{itemize}
  \item 4-fold cross-validation: train on 3/4 of labels, test on 1/4.
  \item HMCanBroad is accurate for broad H3K36me3 data but not sharp H3K4me3 data.
  \item MACS is accurate for sharp H3K4me3 but not broad H3K36me3 data.
  \item Proposed algorithm in coseg R package yields state-of-the-art
    accuracy in all benchmark data sets.
  \end{itemize}
  \scriptsize
\url{http://bl.ocks.org/tdhock/raw/886575874144c3b172ce6b7d7d770b9f/}
\end{frame}

\section{Conclusions and future work}

\begin{frame}[fragile]
  \frametitle{How to choose parameters of unsupervised peak
    detectors?}
\scriptsize
19 parameters for Model-based analysis of ChIP-Seq (MACS), Zhang et al, 2008.
\begin{verbatim}
  [-g GSIZE]
  [-s TSIZE] [--bw BW] [-m MFOLD MFOLD] [--fix-bimodal]
  [--nomodel] [--extsize EXTSIZE | --shiftsize SHIFTSIZE]
  [-q QVALUE | -p PVALUE | -F FOLDENRICHMENT] [--to-large]
  [--down-sample] [--seed SEED] [--nolambda]
  [--slocal SMALLLOCAL] [--llocal LARGELOCAL]
  [--shift-control] [--half-ext] [--broad]
  [--broad-cutoff BROADCUTOFF] [--call-summits]
\end{verbatim}
10 parameters for Histone modifications in cancer (HMCan),
Ashoor et al, 2013.
\begin{verbatim}
minLength 145
medLength 150
maxLength 155
smallBinLength 50
largeBinLength 100000
pvalueThreshold 0.01
mergeDistance 200
iterationThreshold 5
finalThreshold 0
maxIter 20
\end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{Which macs parameter is best for these data?}
  \includegraphics[width=1\textwidth]{figure-macs-problem.png}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg: search for the peaks with lowest loss}
  \includegraphics[width=1\textwidth]{figure-macs-problem-PeakSeg.png}
  
  Simple model with only one parameter to train (maxPeaks).
\end{frame}

\begin{frame}
  \frametitle{Conclusions}
  \begin{tabular}{r|c|c}
    & no pruning & functional pruning \\
    \hline
    unconstrained & Dynamic Programming & Pruned DP \\
     & exact $O(N^2)$ & exact $O(N\log N)$\\
    R pkgs: & changepoint & cghseg, Segmentor\\
    \hline
    up-down constrained & constrained DP & \textbf{this work} \\
     & inexact $O(N^2)$ & exact $O(N\log N)$\\
    R pkgs: & PeakSegDP & \textbf{coseg}\\
    \hline
  \end{tabular}
  \begin{itemize}
  \item New algorithm that \textbf{exactly} computes the
    \textbf{constrained} optimal change-points/peaks for $N$ data points.
  \item C++ code in coseg R package, $O(N \log N)$ memory
    \url{https://github.com/tdhock/coseg}
  \item PeakSegFPOP program for big $N > 10^6$ data,
    $O(\log N)$ memory and $O(N\log N)$ disk space.
  \item TODO: regularized isotonic regression solver.
  \item TODO: supervised peak calling for ENCODE, Roadmap, ...
  \item TODO: interactive web app for creating labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Thanks for your attention!}

  Questions? toby.hocking@mail.mcgill.ca
  \begin{itemize}
  \item 
  \textbf{coseg} R package, \\
  \url{https://github.com/tdhock/coseg}
  \item 
    \textbf{PeakSegFPOP} command line program, 
  \url{https://github.com/tdhock/PeakSegFPOP}
  \item source code for these slides:
  \url{https://github.com/tdhock/PeakSegFPOP-paper}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Segmenting whole chromosomes?}
  \includegraphics[width=\textwidth]{screenshot-gap-peaks}
  \begin{itemize}
  \item 365 regions with no gaps in hg19.
  \item 272 regions with no gaps on chr1-22, X, Y.
  \item Smallest: 31,833 bases (chr6:157,609,467-157,641,300).
  \item Largest: 115,591,997 bases (chr4:75,452,279-191,044,276).
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Reasonable time to segment biggest region on chr1}
  \includegraphics[width=\textwidth]{figure-cosegData-timings.pdf}
  \begin{itemize}
  \item R package in memory: $O(N \log N)$ time.
  \item Command line program on disk: $O(N \log N)$ time.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Memory requirements reasonable for on-disk version}
  \includegraphics[width=\textwidth]{figure-cosegData-timings-memory-disk.pdf}
  \begin{itemize}
  \item R package in memory: $O(N \log N)$ memory.
  \item Command line program on disk: $O(\log N)$ memory.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Disk usage reasonable}
  \includegraphics[width=\textwidth]{figure-cosegData-timings-disk.pdf}
  \begin{itemize}
  \item R package in memory: no disk usage.
  \item Command line program: $O(N \log N)$ disk space (temporary).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Previous work in computer vision: look and add labels
    to...}
  \begin{tabular}{ccc}
    Photos & Cell images & Copy number profiles \\
    \includegraphics[width=1.3in]{../chip-seq-paper/faces} &
    \includegraphics[width=1.3in]{../chip-seq-paper/cellprofiler} &
    \includegraphics[width=1.5in]{../chip-seq-paper/regions-axes}\\
    Labels: names & phenotypes & alterations \\ \\
    CVPR 2013 & CellProfiler & SegAnnDB \\
    246 papers & 873 citations & Hocking et al, 2014. \\
     &
  \end{tabular}
  Sources: \url{http://en.wikipedia.org/wiki/Face_detection}\\
  Jones et al PNAS 2009. Scoring diverse cellular morphologies in
  image-based screens with iterative feedback and machine learning.
\end{frame}

\begin{frame}
  \frametitle{Two annotators provide consistent labels, but different
    precision}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/screenshot-several-annotators}

  \begin{itemize}
  \item TDH peakStart/peakEnd more precise than AM peaks.
  \item AM noPeaks more precise than TDH no label.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one person, test on another\\
(same histone mark and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-annotators.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on some samples, test on others\\
(same histone mark and person)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-H3K4me3-types.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1\textwidth]{../chip-seq-paper/figure-splits-TDH-experiments.pdf}
\end{frame}

\begin{frame}
  \frametitle{Train on one histone mark, test on another\\
(same person and samples)}
  \includegraphics[width=1.1\textwidth]{../chip-seq-paper/figure-test-TDH-experiments.pdf}
\end{frame}


\begin{frame}
  \frametitle{PeakSeg with 2 peaks more likely than default macs}
  \includegraphics[width=1\textwidth]{figure-macs-problem-1-30103.png}
\end{frame}

\begin{frame}
  \frametitle{Unconstrained model can have any changes}
  \includegraphics[width=1\textwidth]{Seg_SansC.png}
  \vskip -1cm
  \begin{itemize}
  \item The model above does NOT verify the PeakSeg up-down constraint
    (second change is up, should be down).
  \item Not directly interpretable as background and peaks.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PeakSeg constraint forces up then down changes}
  \includegraphics[width=1\textwidth]{Seg_AvecC1.png}
  \vskip -1cm
  \begin{itemize}
  \item Novelty of PeakSeg model with respect to previous optimal
    segmentation models: the up-down constraint.
  \item Interpretable as background (S1, S3, ...)\\
    and peaks (S2, S4, ...).
  \end{itemize}
\end{frame}

\section*{Interpretation of equal segment means}

\begin{frame}
  \frametitle{What does our solution say about the PeakSeg solution?}
  \begin{itemize}
  \item Our algo exactly solves a problem with \textbf{non-strict
      inequality} constraints.
  \item For example, $N=3$ data points and $S=3$ segments,
    \begin{equation*}
      \min_{m_1\leq m_2\geq m_3}
      \sum_{t=1}^N m_t - z_t\log m_t.
    \end{equation*}
  \item But the PeakSeg problem has \textbf{strict inequality}
    constraints:
    \begin{equation*}
      \min_{m_1<m_2 >m_3}
      \sum_{t=1}^N m_t - z_t\log m_t.
    \end{equation*}
  \end{itemize}
  When our algo returns equal values for adjacent segment means,
  \begin{itemize}
  \item Our solution is not feasible for the PeakSeg problem, and
  \item The PeakSeg solution is undefined.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Example: 13, 14, 10, 1}
  What do you think is the best model with 1 peak?\\
  (3 segments, 1 change up, 1 change down)
  
  \vskip 1cm

  Guessing game:

  \url{https://github.com/tdhock/PeakSegFPOP-paper/blob/master/figure-min-undefined.R} 
\end{frame}

\begin{frame}
  \frametitle{PeakSegDP returns this highly suboptimal model}
  \includegraphics[width=\textwidth]{figure-min-undefined-suboptimal}
\end{frame}

\begin{frame}
  \frametitle{A better model}
  \includegraphics[width=\textwidth]{figure-min-undefined-1}
\end{frame}

\begin{frame}
  \frametitle{Even better}
  \includegraphics[width=\textwidth]{figure-min-undefined-2}
\end{frame}

\begin{frame}
  \frametitle{Still better}
  \includegraphics[width=\textwidth]{figure-min-undefined-3}
\end{frame}

\begin{frame}
  \frametitle{We can keep going forever}
  \includegraphics[width=\textwidth]{figure-min-undefined-4}
\end{frame}

\begin{frame}
  \frametitle{Best model is not feasible}
  \includegraphics[width=\textwidth]{figure-min-undefined}
\end{frame}

\section*{Segmentation problem definitions}

\begin{frame}
  \frametitle{Unconstrained maximum likelihood segmentation}
  \begin{itemize}
  \item We have a sequence of $n$ count data $\mathbf y\in\ZZ_+^n$ to
    segment. 
  \item Choose the number of segments $S\in\{1, 2, \dots, n\}$.
  \end{itemize}
\begin{align*}
  \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) 
\\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = S, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}
\nonumber
\end{align*}
\begin{itemize}
\item The Poisson loss function is  $\ell( m,  y)= m - y \log m$.
\item Every $t$ such that $c_t \neq 0$ is a
change-point.
\end{itemize}

\end{frame}

% We refer to (\ref{unconstrained}) as the ``unconstrained'' model
% since $\mathbf{\hat m}^K(\mathbf y)$ is the most likely segmentation
% of all possible models with $K$ piecewise constant segments ($K-1$
% change-points). 
% Although (\ref{unconstrained}) is a non-convex optimization problem,
% the sequence of segmentations
% $\mathbf{\hat m}^1(\mathbf y), \dots, \mathbf{\hat m}^{K}(\mathbf y)$
% can be computed in $O(K n^2)$ time using the standard dynamic
% programming algorithm \citep{bellman}, or in $O(K n \log n)$ time
% using dynamic programming with functional pruning \citep{pruned-dp,
%   johnson, Segmentor}.

\begin{frame}
  \frametitle{The PeakSeg constrained maximum likelihood model}
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) \\
    \text{subject to} &\ \  1+\sum_{t=1}^{n-1} I(c_t \neq 0) = S, \\
& \ \ c_t = -1 \Rightarrow m_{t} > m_{t+1} \text{ (change down)}\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}\\
& \ \ c_t = 1 \Rightarrow m_{t} < m_{t+1} \text{ (change up)}\\
&\ \ \forall t\in\{1, \dots, n-1\},\, \alert{P_t(\mathbf c) \in\{0, 1\}}.
\end{align*}
The only difference with the unconstrained problem is that\\
\alert{we
have added the constraint $P_t(\mathbf c)=\sum_{i=1}^t c_i \in\{0, 1\}$}.
% Another way to interpret the constrained
% \ref{PeakSeg} problem is that the sequence of changes in the segment
% means $\mathbf m$ must begin with a positive change and then
% alternate: up, down, up, down, ... (and not up, up, down). Thus the
% even-numbered segments may be interpreted as peaks $P_t(\mathbf c)=1$,
% and the odd-numbered segments may be interpreted as background
% $P_t(\mathbf c)=0$.
\end{frame}

\begin{frame}
  \frametitle{The problem solved by the PeakSegPDPA}
\begin{align*}
    \minimize_{\substack{
  \mathbf m\in\RR^{n}
\\
  \mathbf c\in\{-1,0,1\}^{n-1}
  }} &\ \ 
    \sum_{t=1}^n \ell( m_t,  z_t) 
  \label{PeakSegPDPA}
\\
    \text{subject to} &\ \  1+ \sum_{t=1}^{n-1} I(c_t \neq 0) = S, 
\nonumber\\
& \ \ c_t = -1 \Rightarrow m_{t} \alert{\geq} m_{t+1} \text{ (change down or no change)}
\nonumber\\
& \ \ c_t = 0 \Rightarrow m_{t} = m_{t+1}  \text{ (no change)}
\nonumber\\
& \ \ c_t = 1 \Rightarrow m_{t} \alert{\leq} m_{t+1} \text{ (change up or no change)}
\nonumber\\
&\ \ \forall t\in\{1, \dots, n-1\},\, P_t(\mathbf c) \in\{0, 1\}.
\nonumber
\end{align*}
\begin{itemize}
\item The only difference with the \textbf{PeakSeg} problem is that\\
  \alert{we have changed the strict inequality constraints to non-strict inequality
constraints}. 
\item This model has \textbf{at most} $S$ distinct
  segment means (some may be equal due to the non-strict equality
  constraints).
\end{itemize}
\end{frame}


\section*{Review of constrained dynamic programming algorithm}

\input{figure-dp-first}

\input{figure-dp-short}

\input{figure-dp}

\begin{frame}
  \frametitle{Dynamic programming is faster than grid search for $s>
    2$ segments}

  Computation time in number of data points $N$:

  \vskip 1cm

  \begin{tabular}{ccc}
    segments $s$ & grid search & dynamic programming \\
    \hline
    1 & $O(N)$ & $O(N)$ \\
    2 & $O(N^2)$ & $O(N^2)$ \\
    3 & $O(N^3)$ & $O(N^2)$ \\
    4 & $O(N^4)$ & $O(N^2)$ \\
    $\vdots$ &     $\vdots$ &     $\vdots$ 
  \end{tabular}

  \vskip 1cm

  For example $N = 5735$ data points to segment.\\
  $N^2 = 32890225$\\
  $N^3 = 188625440375$\\
  $\vdots$
\end{frame}

\input{figure-dp-third}

\begin{frame}
  \frametitle{5 errors for coseg/Segmentor, only 1 error for PeakSegDP}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem1}
\end{frame}

\begin{frame}
  \frametitle{5 errors for coseg/Segmentor, only 1 error for PeakSegDP}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem1-best.png}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization worse than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem4}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization worse than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem4-best}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization worse than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem4-best-zoom}
\end{frame}


\begin{frame}
  \frametitle{Functional pruning complete example}
%\includegraphics[width=\textwidth]{screenshot-PDPA-demo}
    \input{figure-2-min-envelope}

  \url{https://github.com/tdhock/PeakSegFPOP-paper}
  %\url{http://bl.ocks.org/tdhock/raw/8c5dd0af533e24a893e7c5232f9bc94c/}
\end{frame}

\begin{frame}
  \frametitle{New coseg algorithm more accurate than unconstrained
    maximum likelihood Poisson model (Segmentor)}
  \includegraphics[width=\textwidth]{figure-min-train-error-Segmentor}
\end{frame}

\begin{frame}
  \frametitle{New coseg algorithm mostly agrees with slower inexact DP}
  \includegraphics[width=\textwidth]{figure-min-train-error-PeakSegDP}
\end{frame}

\begin{frame}
  \frametitle{New coseg algorithm sometimes disagrees with macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-macs}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization better than macs}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem5}
\end{frame}

\begin{frame}
  \frametitle{0 errors for coseg/PeakSegDP, 6 errors for Segmentor}
  \includegraphics[width=\textwidth]{figure-min-train-error-problem2}
\end{frame}


\end{document}
